<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns:mda="urn:oasis:names:tc:xliff:metadata:2.0" srcLang="en-US" trgLang="ja-jp" version="2.0" xml:space="preserve" xmlns="urn:oasis:names:tc:xliff:document:2.0">
	<file id="1">
		<mda:metadata>
			<mda:metaGroup>
				<mda:meta type="tool-id">mdxliff</mda:meta>
				<mda:meta type="tool-name">mdxliff</mda:meta>
				<mda:meta type="tool-version">1.0-00ddeee</mda:meta>
				<mda:meta type="tool-company">Microsoft</mda:meta>
			</mda:metaGroup>
		<mda:metaGroup><mda:meta type="olfilehash">6a84cec5bbbfad28407eb421330cebbb73b2ce90</mda:meta><mda:meta type="olfilepath">wdg-cpub-test\ndolci2\input-and-devices\speech-recognition.md</mda:meta><mda:meta type="oltranslationpriority"></mda:meta><mda:meta type="oltranslationtype">Human Translation</mda:meta><mda:meta type="olskeletonhash">8d30fbea6aaf4ec4ed08d6f40d2d4a5e2196144f</mda:meta><mda:meta type="olxliffhash">692da5d70bf261dbb786a5506f9daf8d509ed22b</mda:meta></mda:metaGroup></mda:metadata>
		<group id="content">
			<unit id="101" translate="yes">
				<segment state="initial">
					<source>Use speech recognition to provide input, specify an action or command, and accomplish tasks.</source>
					<target>Use speech recognition to provide input, specify an action or command, and accomplish tasks.</target>
				</segment>
			</unit>
			<unit id="102" translate="yes">
				<segment state="initial">
					<source>Speech recognition</source>
					<target>Speech recognition</target>
				</segment>
			</unit>
			<unit id="103" translate="yes">
				<segment state="initial">
					<source>Speech recognition</source>
					<target>Speech recognition</target>
				</segment>
			</unit>
			<unit id="104" translate="yes">
				<segment state="initial">
					<source>\[ Updated for UWP apps on Windows 10.</source>
					<target>\[ Updated for UWP apps on Windows 10.</target>
				</segment>
			</unit>
			<unit id="105" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?linkid=619132)</data>
				</originalData>
				<segment state="initial">
					<source>For Windows 8.x articles, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">archive</pc> \]</source>
					<target>For Windows 8.x articles, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">archive</pc> \]</target>
				</segment>
			</unit>
			<unit id="106" translate="yes">
				<segment state="initial">
					<source>Use speech recognition to provide input, specify an action or command, and accomplish tasks.</source>
					<target>Use speech recognition to provide input, specify an action or command, and accomplish tasks.</target>
				</segment>
			</unit>
			<unit id="107" translate="yes">
				<segment state="initial">
					<source>Important APIs</source>
					<target>Important APIs</target>
				</segment>
			</unit>
			<unit id="108" translate="yes">
				<segment state="initial">
					<source>Windows.Media.SpeechRecognition</source>
					<target>Windows.Media.SpeechRecognition</target>
				</segment>
			</unit>
			<unit id="109" translate="yes">
				<segment state="initial">
					<source>Speech recognition is made up of a speech runtime, recognition APIs for programming the runtime, ready-to-use grammars for dictation and web search, and a default system UI that helps users discover and use speech recognition features.</source>
					<target>Speech recognition is made up of a speech runtime, recognition APIs for programming the runtime, ready-to-use grammars for dictation and web search, and a default system UI that helps users discover and use speech recognition features.</target>
				</segment>
			</unit>
			<unit id="110" translate="yes">
				<segment state="initial">
					<source>Set up the audio feed</source>
					<target>Set up the audio feed</target>
				</segment>
			</unit>
			<unit id="111" translate="yes">
				<segment state="initial">
					<source>Ensure that your device has a microphone or the equivalent.</source>
					<target>Ensure that your device has a microphone or the equivalent.</target>
				</segment>
			</unit>
			<unit id="112" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/br211430)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
					<data id="id7">[</data>
					<data id="id8">](https://msdn.microsoft.com/library/windows/apps/br211474)</data>
					<data id="id9">**</data>
					<data id="id10">**</data>
				</originalData>
				<segment state="initial">
					<source>Set the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Microphone</pc> device capability (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">DeviceCapability</pc></pc>) in the <pc dataRefEnd="id8" dataRefStart="id7" id="p4">App package manifest</pc> (<pc dataRefEnd="id10" dataRefStart="id9" id="p5">package.appxmanifest</pc> file) to get access to the microphone’s audio feed.</source>
					<target>Set the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Microphone</pc> device capability (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">DeviceCapability</pc></pc>) in the <pc dataRefEnd="id8" dataRefStart="id7" id="p4">App package manifest</pc> (<pc dataRefEnd="id10" dataRefStart="id9" id="p5">package.appxmanifest</pc> file) to get access to the microphone’s audio feed.</target>
				</segment>
			</unit>
			<unit id="113" translate="yes">
				<segment state="initial">
					<source>This allows the app to record audio from connected microphones.</source>
					<target>This allows the app to record audio from connected microphones.</target>
				</segment>
			</unit>
			<unit id="114" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/mt270968)</data>
				</originalData>
				<segment state="initial">
					<source>See <pc dataRefEnd="id2" dataRefStart="id1" id="p1">App capability declarations</pc>.</source>
					<target>See <pc dataRefEnd="id2" dataRefStart="id1" id="p1">App capability declarations</pc>.</target>
				</segment>
			</unit>
			<unit id="115" translate="yes">
				<segment state="initial">
					<source>Recognize speech input</source>
					<target>Recognize speech input</target>
				</segment>
			</unit>
			<unit id="116" translate="yes">
				<originalData>
					<data id="id1">*</data>
					<data id="id2">*</data>
				</originalData>
				<segment state="initial">
					<source>A <pc dataRefEnd="id2" dataRefStart="id1" id="p1">constraint</pc> defines the words and phrases (vocabulary) that an app recognizes in speech input.</source>
					<target>A <pc dataRefEnd="id2" dataRefStart="id1" id="p1">constraint</pc> defines the words and phrases (vocabulary) that an app recognizes in speech input.</target>
				</segment>
			</unit>
			<unit id="117" translate="yes">
				<segment state="initial">
					<source>Constraints are at the core of speech recognition and give your app great over the accuracy of speech recognition.</source>
					<target>Constraints are at the core of speech recognition and give your app great over the accuracy of speech recognition.</target>
				</segment>
			</unit>
			<unit id="118" translate="yes">
				<segment state="initial">
					<source>You can use various types of constraints when performing speech recognition:</source>
					<target>You can use various types of constraints when performing speech recognition:</target>
				</segment>
			</unit>
			<unit id="119" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn631446)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Predefined grammars</pc> (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognitionTopicConstraint</pc></pc>).</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Predefined grammars</pc> (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognitionTopicConstraint</pc></pc>).</target>
				</segment>
			</unit>
			<unit id="120" translate="yes">
				<segment state="initial">
					<source>Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar.</source>
					<target>Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar.</target>
				</segment>
			</unit>
			<unit id="121" translate="yes">
				<segment state="initial">
					<source>When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device.</source>
					<target>When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device.</target>
				</segment>
			</unit>
			<unit id="122" translate="yes">
				<segment state="initial">
					<source>The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases.</source>
					<target>The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases.</target>
				</segment>
			</unit>
			<unit id="123" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn653226)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>The predefined dictation grammar is used if you don't specify any constraints for your <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizer</pc></pc> object.</source>
					<target>The predefined dictation grammar is used if you don't specify any constraints for your <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizer</pc></pc> object.</target>
				</segment>
			</unit>
			<unit id="124" translate="yes">
				<segment state="initial">
					<source>Free-text dictation is useful when you don't want to limit the kinds of things a user can say.</source>
					<target>Free-text dictation is useful when you don't want to limit the kinds of things a user can say.</target>
				</segment>
			</unit>
			<unit id="125" translate="yes">
				<segment state="initial">
					<source>Typical uses include creating notes or dictating the content for a message.</source>
					<target>Typical uses include creating notes or dictating the content for a message.</target>
				</segment>
			</unit>
			<unit id="126" translate="yes">
				<segment state="initial">
					<source>The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say.</source>
					<target>The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say.</target>
				</segment>
			</unit>
			<unit id="127" translate="yes">
				<segment state="initial">
					<source>However, it is optimized to recognize terms that people typically use when searching the web.</source>
					<target>However, it is optimized to recognize terms that people typically use when searching the web.</target>
				</segment>
			</unit>
			<unit id="128" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Note</pc>  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Note</pc>  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.</target>
				</segment>
			</unit>
			<unit id="129" translate="yes">
				<segment state="initial">
					<source>These predefined grammars can be used to recognize up to 10 seconds of speech input and require no authoring effort on your part.</source>
					<target>These predefined grammars can be used to recognize up to 10 seconds of speech input and require no authoring effort on your part.</target>
				</segment>
			</unit>
			<unit id="130" translate="yes">
				<segment state="initial">
					<source>However, they do require a connection to a network.</source>
					<target>However, they do require a connection to a network.</target>
				</segment>
			</unit>
			<unit id="131" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">&amp;gt;</data>
					<data id="id4">&amp;gt;</data>
				</originalData>
				<segment state="initial">
					<source>To use web-service constraints, speech input and dictation support must be enabled in <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Settings</pc> by turning on the "Get to know me" option in the Settings -<ph dataRef="id3" id="ph1" /> Privacy -<ph dataRef="id4" id="ph2" /> Speech, inking, and typing page.</source>
					<target>To use web-service constraints, speech input and dictation support must be enabled in <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Settings</pc> by turning on the "Get to know me" option in the Settings -<ph dataRef="id3" id="ph1" /> Privacy -<ph dataRef="id4" id="ph2" /> Speech, inking, and typing page.</target>
				</segment>
			</unit>
			<unit id="132" translate="yes">
				<originalData>
					<data id="id1">&amp;gt;</data>
					<data id="id2">&amp;gt;</data>
				</originalData>
				<segment state="initial">
					<source>Here, we show how to test whether speech input is enabled and open the Settings -<ph dataRef="id1" id="ph1" /> Privacy -<ph dataRef="id2" id="ph2" /> Speech, inking, and typing page, if not.</source>
					<target>Here, we show how to test whether speech input is enabled and open the Settings -<ph dataRef="id1" id="ph1" /> Privacy -<ph dataRef="id2" id="ph2" /> Speech, inking, and typing page, if not.</target>
				</segment>
			</unit>
			<unit id="133" translate="yes">
				<segment state="initial">
					<source>First, we initialize a global variable (HResultPrivacyStatementDeclined) to the HResult value of 0x80045509.</source>
					<target>First, we initialize a global variable (HResultPrivacyStatementDeclined) to the HResult value of 0x80045509.</target>
				</segment>
			</unit>
			<unit id="134" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn532194)</data>
				</originalData>
				<segment state="initial">
					<source>See <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Exception handling for in C# or Visual Basic</pc>.</source>
					<target>See <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Exception handling for in C# or Visual Basic</pc>.</target>
				</segment>
			</unit>
			<unit id="135" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn631421)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Programmatic list constraints</pc> (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognitionListConstraint</pc></pc>).</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Programmatic list constraints</pc> (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognitionListConstraint</pc></pc>).</target>
				</segment>
			</unit>
			<unit id="136" translate="yes">
				<segment state="initial">
					<source>Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases.</source>
					<target>Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases.</target>
				</segment>
			</unit>
			<unit id="137" translate="yes">
				<segment state="initial">
					<source>A list constraint works well for recognizing short, distinct phrases.</source>
					<target>A list constraint works well for recognizing short, distinct phrases.</target>
				</segment>
			</unit>
			<unit id="138" translate="yes">
				<segment state="initial">
					<source>Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match.</source>
					<target>Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match.</target>
				</segment>
			</unit>
			<unit id="139" translate="yes">
				<segment state="initial">
					<source>The list can also be programmatically updated.</source>
					<target>The list can also be programmatically updated.</target>
				</segment>
			</unit>
			<unit id="140" translate="yes">
				<segment state="initial">
					<source>A list constraint consists of an array of strings that represents speech input that your app will accept for a recognition operation.</source>
					<target>A list constraint consists of an array of strings that represents speech input that your app will accept for a recognition operation.</target>
				</segment>
			</unit>
			<unit id="141" translate="yes">
				<segment state="initial">
					<source>You can create a list constraint in your app by creating a speech-recognition list-constraint object and passing an array of strings.</source>
					<target>You can create a list constraint in your app by creating a speech-recognition list-constraint object and passing an array of strings.</target>
				</segment>
			</unit>
			<unit id="142" translate="yes">
				<segment state="initial">
					<source>Then, add that object to the constraints collection of the recognizer.</source>
					<target>Then, add that object to the constraints collection of the recognizer.</target>
				</segment>
			</unit>
			<unit id="143" translate="yes">
				<segment state="initial">
					<source>Recognition is successful when the speech recognizer recognizes any one of the strings in the array.</source>
					<target>Recognition is successful when the speech recognizer recognizes any one of the strings in the array.</target>
				</segment>
			</unit>
			<unit id="144" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn631412)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">SRGS grammars</pc> (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognitionGrammarFileConstraint</pc></pc>).</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">SRGS grammars</pc> (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognitionGrammarFileConstraint</pc></pc>).</target>
				</segment>
			</unit>
			<unit id="145" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?LinkID=262302)</data>
				</originalData>
				<segment state="initial">
					<source>An Speech Recognition Grammar Specification (SRGS) grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SRGS Version 1.0</pc>.</source>
					<target>An Speech Recognition Grammar Specification (SRGS) grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SRGS Version 1.0</pc>.</target>
				</segment>
			</unit>
			<unit id="146" translate="yes">
				<segment state="initial">
					<source>An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.</source>
					<target>An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.</target>
				</segment>
			</unit>
			<unit id="147" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn653220)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Voice command constraints</pc> (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognitionVoiceCommandDefinitionConstraint</pc></pc>)</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Voice command constraints</pc> (<pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognitionVoiceCommandDefinitionConstraint</pc></pc>)</target>
				</segment>
			</unit>
			<unit id="148" translate="yes">
				<segment state="initial">
					<source>Use a Voice Command Definition (VCD) XML file to define the commands that the user can say to initiate actions when activating your app.</source>
					<target>Use a Voice Command Definition (VCD) XML file to define the commands that the user can say to initiate actions when activating your app.</target>
				</segment>
			</unit>
			<unit id="149" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](launch-a-foreground-app-with-voice-commands-in-cortana.md)</data>
				</originalData>
				<segment state="initial">
					<source>For more detail, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Launch a foreground app with voice commands in Cortana</pc>.</source>
					<target>For more detail, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Launch a foreground app with voice commands in Cortana</pc>.</target>
				</segment>
			</unit>
			<unit id="150" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Note</pc>  Which type of constraint type you use depends on the complexity of the recognition experience you want to create.</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Note</pc>  Which type of constraint type you use depends on the complexity of the recognition experience you want to create.</target>
				</segment>
			</unit>
			<unit id="151" translate="yes">
				<segment state="initial">
					<source>Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.</source>
					<target>Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.</target>
				</segment>
			</unit>
			<unit id="152" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](define-custom-recognition-constraints.md)</data>
				</originalData>
				<segment state="initial">
					<source>To get started with constraints, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Define custom recognition constraints</pc>.</source>
					<target>To get started with constraints, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Define custom recognition constraints</pc>.</target>
				</segment>
			</unit>
			<unit id="153" translate="yes">
				<segment state="initial">
					<source>The predefined Universal Windows app dictation grammar recognizes most words and short phrases in a language.</source>
					<target>The predefined Universal Windows app dictation grammar recognizes most words and short phrases in a language.</target>
				</segment>
			</unit>
			<unit id="154" translate="yes">
				<segment state="initial">
					<source>It is activated by default when a speech recognizer object is instantiated without custom constraints.</source>
					<target>It is activated by default when a speech recognizer object is instantiated without custom constraints.</target>
				</segment>
			</unit>
			<unit id="155" translate="yes">
				<segment state="initial">
					<source>In this example, we show how to:</source>
					<target>In this example, we show how to:</target>
				</segment>
			</unit>
			<unit id="156" translate="yes">
				<segment state="initial">
					<source>Create a speech recognizer.</source>
					<target>Create a speech recognizer.</target>
				</segment>
			</unit>
			<unit id="157" translate="yes">
				<segment state="initial">
					<source>Compile the default Universal Windows app constraints (no grammars have been added to the speech recognizer's grammar set).</source>
					<target>Compile the default Universal Windows app constraints (no grammars have been added to the speech recognizer's grammar set).</target>
				</segment>
			</unit>
			<unit id="158" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn653245)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Start listening for speech by using the basic recognition UI and TTS feedback provided by the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">RecognizeWithUIAsync</pc></pc> method.</source>
					<target>Start listening for speech by using the basic recognition UI and TTS feedback provided by the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">RecognizeWithUIAsync</pc></pc> method.</target>
				</segment>
			</unit>
			<unit id="159" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn653244)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">RecognizeAsync</pc></pc> method if the default UI is not required.</source>
					<target>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">RecognizeAsync</pc></pc> method if the default UI is not required.</target>
				</segment>
			</unit>
			<unit id="160" translate="yes">
				<segment state="initial">
					<source>Customize the recognition UI</source>
					<target>Customize the recognition UI</target>
				</segment>
			</unit>
			<unit id="161" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn653245)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>When your app attempts speech recognition by calling <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizer.RecognizeWithUIAsync</pc></pc>, several screens are shown in the following order.</source>
					<target>When your app attempts speech recognition by calling <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizer.RecognizeWithUIAsync</pc></pc>, several screens are shown in the following order.</target>
				</segment>
			</unit>
			<unit id="162" translate="yes">
				<segment state="initial">
					<source>If you're using a constraint based on a predefined grammar (dictation or web search):</source>
					<target>If you're using a constraint based on a predefined grammar (dictation or web search):</target>
				</segment>
			</unit>
			<unit id="163" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Listening</pc> screen.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Listening</pc> screen.</target>
				</segment>
			</unit>
			<unit id="164" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Thinking</pc> screen.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Thinking</pc> screen.</target>
				</segment>
			</unit>
			<unit id="165" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Heard you say</pc> screen or the error screen.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Heard you say</pc> screen or the error screen.</target>
				</segment>
			</unit>
			<unit id="166" translate="yes">
				<segment state="initial">
					<source>If you're using a constraint based on a list of words or phrases, or a constraint based on a SRGS grammar file:</source>
					<target>If you're using a constraint based on a list of words or phrases, or a constraint based on a SRGS grammar file:</target>
				</segment>
			</unit>
			<unit id="167" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Listening</pc> screen.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Listening</pc> screen.</target>
				</segment>
			</unit>
			<unit id="168" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Did you say</pc> screen, if what the user said could be interpreted as more than one potential result.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Did you say</pc> screen, if what the user said could be interpreted as more than one potential result.</target>
				</segment>
			</unit>
			<unit id="169" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Heard you say</pc> screen or the error screen.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Heard you say</pc> screen or the error screen.</target>
				</segment>
			</unit>
			<unit id="170" translate="yes">
				<segment state="initial">
					<source>The following image shows an example of the flow between screens for a speech recognizer that uses a constraint based on a SRGS grammar file.</source>
					<target>The following image shows an example of the flow between screens for a speech recognizer that uses a constraint based on a SRGS grammar file.</target>
				</segment>
			</unit>
			<unit id="171" translate="yes">
				<segment state="initial">
					<source>In this example, speech recognition was successful.</source>
					<target>In this example, speech recognition was successful.</target>
				</segment>
			</unit>
			<unit id="172" translate="yes">
				<segment state="initial">
					<source>initial recognition screen for a constraint based on a sgrs grammar file</source>
					<target>initial recognition screen for a constraint based on a sgrs grammar file</target>
				</segment>
			</unit>
			<unit id="173" translate="yes">
				<segment state="initial">
					<source>intermediate recognition screen for a constraint based on a sgrs grammar file</source>
					<target>intermediate recognition screen for a constraint based on a sgrs grammar file</target>
				</segment>
			</unit>
			<unit id="174" translate="yes">
				<segment state="initial">
					<source>final recognition screen for a constraint based on a sgrs grammar file</source>
					<target>final recognition screen for a constraint based on a sgrs grammar file</target>
				</segment>
			</unit>
			<unit id="175" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Listening</pc> screen can provide examples of words or phrases that the app can recognize.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Listening</pc> screen can provide examples of words or phrases that the app can recognize.</target>
				</segment>
			</unit>
			<unit id="176" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn653234)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn653254)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
					<data id="id9">**</data>
					<data id="id10">**</data>
				</originalData>
				<segment state="initial">
					<source>Here, we show how to use the properties of the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizerUIOptions</pc></pc> class (obtained by calling the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">SpeechRecognizer.UIOptions</pc></pc> property) to customize content on the <pc dataRefEnd="id10" dataRefStart="id9" id="p5">Listening</pc> screen.</source>
					<target>Here, we show how to use the properties of the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizerUIOptions</pc></pc> class (obtained by calling the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">SpeechRecognizer.UIOptions</pc></pc> property) to customize content on the <pc dataRefEnd="id10" dataRefStart="id9" id="p5">Listening</pc> screen.</target>
				</segment>
			</unit>
			<unit id="177" translate="yes">
				<segment state="initial">
					<source>Related articles</source>
					<target>Related articles</target>
				</segment>
			</unit>
			<unit id="178" translate="yes">
				<segment state="initial">
					<source>Developers</source>
					<target>Developers</target>
				</segment>
			</unit>
			<unit id="179" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](speech-interactions.md)</data>
					<data id="id3">
</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Speech interactions</pc><ph dataRef="id3" id="ph1" /><pc dataRefEnd="id5" dataRefStart="id4" id="p2">Designers</pc></source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Speech interactions</pc><ph dataRef="id3" id="ph1" /><pc dataRefEnd="id5" dataRefStart="id4" id="p2">Designers</pc></target>
				</segment>
			</unit>
			<unit id="180" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn596121)</data>
					<data id="id3">
</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Speech design guidelines</pc><ph dataRef="id3" id="ph1" /><pc dataRefEnd="id5" dataRefStart="id4" id="p2">Samples</pc></source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Speech design guidelines</pc><ph dataRef="id3" id="ph1" /><pc dataRefEnd="id5" dataRefStart="id4" id="p2">Samples</pc></target>
				</segment>
			</unit>
			<unit id="181" translate="yes">
				<segment state="initial">
					<source>Speech recognition and speech synthesis sample</source>
					<target>Speech recognition and speech synthesis sample</target>
				</segment>
			</unit>
		</group>
	</file>
</xliff>
{"nodes":[{"pos":[85,230],"content":"This article describes how to use the SceneAnalysisEffect and the FaceDetectionEffect to analyze the content of the media capture preview stream.","needQuote":true,"needEscape":true,"nodes":[{"content":"This article describes how to use the SceneAnalysisEffect and the FaceDetectionEffect to analyze the content of the media capture preview stream.","pos":[0,145]}]},{"pos":[238,270],"content":"Scene analysis for media capture","needQuote":true,"needEscape":true,"nodes":[{"content":"Scene analysis for media capture","pos":[0,32]}]},{"content":"Scene analysis for media capture","pos":[278,310]},{"content":"\\[ Updated for UWP apps on Windows 10.","pos":[312,350]},{"content":"For Windows 8.x articles, see the <bpt id=\"p1\">[</bpt>archive<ept id=\"p1\">](http://go.microsoft.com/fwlink/p/?linkid=619132)</ept> \\]","pos":[351,446],"source":" For Windows 8.x articles, see the [archive](http://go.microsoft.com/fwlink/p/?linkid=619132) \\]"},{"pos":[449,722],"content":"This article describes how to use the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SceneAnalysisEffect<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948902)</ept> and the <bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>FaceDetectionEffect<ept id=\"p4\">**</ept><ept id=\"p3\">](https://msdn.microsoft.com/library/windows/apps/dn948776)</ept> to analyze the content of the media capture preview stream.","source":"This article describes how to use the [**SceneAnalysisEffect**](https://msdn.microsoft.com/library/windows/apps/dn948902) and the [**FaceDetectionEffect**](https://msdn.microsoft.com/library/windows/apps/dn948776) to analyze the content of the media capture preview stream."},{"content":"Scene analysis effect","pos":[727,748]},{"content":"The <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SceneAnalysisEffect<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948902)</ept> analyzes the video frames in the media capture preview stream and recommends processing options to improve the capture result.","pos":[750,964],"source":"The [**SceneAnalysisEffect**](https://msdn.microsoft.com/library/windows/apps/dn948902) analyzes the video frames in the media capture preview stream and recommends processing options to improve the capture result."},{"content":"Currently, the effect supports detecting whether the capture would be improved by using High Dynamic Range (HDR) processing.","pos":[965,1089]},{"content":"If the effect recommends using HDR, you can do this in the following ways:","pos":[1091,1165]},{"content":"Use the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>AdvancedPhotoCapture<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/mt181386)</ept> class to capture photos using the Windows built-in HDR processing algorithm.","pos":[1171,1340],"source":"Use the [**AdvancedPhotoCapture**](https://msdn.microsoft.com/library/windows/apps/mt181386) class to capture photos using the Windows built-in HDR processing algorithm."},{"content":"For more information, see <bpt id=\"p1\">[</bpt>High Dynamic Range (HDR) photo capture<ept id=\"p1\">](high-dynamic-range-hdr-photo-capture.md)</ept>.","pos":[1341,1449],"source":" For more information, see [High Dynamic Range (HDR) photo capture](high-dynamic-range-hdr-photo-capture.md)."},{"content":"Use the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>HdrVideoControl<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn926680)</ept> to capture video using the Windows built-in HDR processing algorithm.","pos":[1455,1612],"source":"Use the [**HdrVideoControl**](https://msdn.microsoft.com/library/windows/apps/dn926680) to capture video using the Windows built-in HDR processing algorithm."},{"content":"For more information, see <bpt id=\"p1\">[</bpt>Capture device controls for video capture<ept id=\"p1\">](capture-device-controls-for-video-capture.md)</ept>.","pos":[1613,1729],"source":" For more information, see [Capture device controls for video capture](capture-device-controls-for-video-capture.md)."},{"content":"Use the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>VariablePhotoSequenceControl<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn640573)</ept> to capture a sequence of frames that you can then composite using a custom HDR implementation.","pos":[1735,1930],"source":"Use the [**VariablePhotoSequenceControl**](https://msdn.microsoft.com/library/windows/apps/dn640573) to capture a sequence of frames that you can then composite using a custom HDR implementation."},{"content":"For more information, see <bpt id=\"p1\">[</bpt>Variable photo sequence<ept id=\"p1\">](variable-photo-sequence.md)</ept>.","pos":[1931,2011],"source":" For more information, see [Variable photo sequence](variable-photo-sequence.md)."},{"content":"Scene analysis namespaces","pos":[2017,2042]},{"content":"To use scene analysis, your app must include the following namespaces in addition to the namespaces required for basic media capture.","pos":[2044,2177]},{"pos":[2189,2207],"content":"SceneAnalysisUsing"},{"content":"Initialize the scene analysis effect and add it to the preview stream","pos":[2292,2361]},{"content":"Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.","pos":[2363,2575]},{"content":"Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.","pos":[2576,2730]},{"pos":[2742,2768],"content":"DeclareSceneAnalysisEffect"},{"pos":[2857,3045],"content":"In your app, after you have initialized the <bpt id=\"p1\">**</bpt>MediaCapture<ept id=\"p1\">**</ept> object, create a new instance of <bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>SceneAnalysisEffectDefinition<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/dn948903)</ept>.","source":"In your app, after you have initialized the **MediaCapture** object, create a new instance of [**SceneAnalysisEffectDefinition**](https://msdn.microsoft.com/library/windows/apps/dn948903)."},{"content":"Register the effect with the capture device by calling <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>AddVideoEffectAsync<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn878035)</ept> on your <bpt id=\"p3\">**</bpt>MediaCapture<ept id=\"p3\">**</ept> object, providing the <bpt id=\"p4\">**</bpt>SceneAnalysisEffectDefinition<ept id=\"p4\">**</ept> and specifying <bpt id=\"p5\">[</bpt><bpt id=\"p6\">**</bpt>MediaStreamType.VideoPreview<ept id=\"p6\">**</ept><ept id=\"p5\">](https://msdn.microsoft.com/library/windows/apps/br226640)</ept> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.","pos":[3047,3483],"source":"Register the effect with the capture device by calling [**AddVideoEffectAsync**](https://msdn.microsoft.com/library/windows/apps/dn878035) on your **MediaCapture** object, providing the **SceneAnalysisEffectDefinition** and specifying [**MediaStreamType.VideoPreview**](https://msdn.microsoft.com/library/windows/apps/br226640) to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream."},{"content":"<bpt id=\"p1\">**</bpt>AddVideoEffectAsync<ept id=\"p1\">**</ept> returns an instance of the added effect.","pos":[3484,3548],"source":"**AddVideoEffectAsync** returns an instance of the added effect."},{"content":"Because this method can be used with multiple effect types, you must cast the returned instance to a <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SceneAnalysisEffect<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948902)</ept> object.","pos":[3549,3741],"source":" Because this method can be used with multiple effect types, you must cast the returned instance to a [**SceneAnalysisEffect**](https://msdn.microsoft.com/library/windows/apps/dn948902) object."},{"pos":[3743,3909],"content":"To receive the results of the scene analysis, you must register a handler for the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SceneAnalyzed<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948920)</ept> event.","source":"To receive the results of the scene analysis, you must register a handler for the [**SceneAnalyzed**](https://msdn.microsoft.com/library/windows/apps/dn948920) event."},{"content":"Currently, the scene analysis effect only includes the high dynamic range analyzer.","pos":[3911,3994]},{"content":"Enable HDR analysis by setting the effect's <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>HighDynamicRangeControl.Enabled<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948827)</ept> to true.","pos":[3995,4143],"source":" Enable HDR analysis by setting the effect's [**HighDynamicRangeControl.Enabled**](https://msdn.microsoft.com/library/windows/apps/dn948827) to true."},{"pos":[4155,4185],"content":"CreateSceneAnalysisEffectAsync"},{"content":"Implement the SceneAnalyzed event handler","pos":[4282,4323]},{"content":"The results of the scene analysis are returned in the <bpt id=\"p1\">**</bpt>SceneAnalyzed<ept id=\"p1\">**</ept> event handler.","pos":[4325,4411],"source":"The results of the scene analysis are returned in the **SceneAnalyzed** event handler."},{"content":"The <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SceneAnalyzedEventArgs<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948922)</ept> object passed into the handler has a <bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>SceneAnalysisEffectFrame<ept id=\"p4\">**</ept><ept id=\"p3\">](https://msdn.microsoft.com/library/windows/apps/dn948907)</ept> object which has a <bpt id=\"p5\">[</bpt><bpt id=\"p6\">**</bpt>HighDynamicRangeOutput<ept id=\"p6\">**</ept><ept id=\"p5\">](https://msdn.microsoft.com/library/windows/apps/dn948830)</ept> object.","pos":[4412,4742],"source":" The [**SceneAnalyzedEventArgs**](https://msdn.microsoft.com/library/windows/apps/dn948922) object passed into the handler has a [**SceneAnalysisEffectFrame**](https://msdn.microsoft.com/library/windows/apps/dn948907) object which has a [**HighDynamicRangeOutput**](https://msdn.microsoft.com/library/windows/apps/dn948830) object."},{"content":"The <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>Certainty<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948833)</ept> property of the high dynamic range output provides a value between 0 and 1.0 where 0 indicates that HDR processing would not help improve the capture result and 1.0 indicates that HDR processing would help.","pos":[4743,5027],"source":" The [**Certainty**](https://msdn.microsoft.com/library/windows/apps/dn948833) property of the high dynamic range output provides a value between 0 and 1.0 where 0 indicates that HDR processing would not help improve the capture result and 1.0 indicates that HDR processing would help."},{"content":"Your can decide the threshold point at which you want to use HDR or show the results to the user and let the user decide.","pos":[5028,5149]},{"pos":[5161,5174],"content":"SceneAnalyzed"},{"content":"The <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>HighDynamicRangeOutput<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948830)</ept> object passed into the handler also has a <bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>FrameControllers<ept id=\"p4\">**</ept><ept id=\"p3\">](https://msdn.microsoft.com/library/windows/apps/dn948834)</ept> property which contains suggested frame controllers for capturing a variable photo sequence for HDR processing.","pos":[5250,5575],"source":"The [**HighDynamicRangeOutput**](https://msdn.microsoft.com/library/windows/apps/dn948830) object passed into the handler also has a [**FrameControllers**](https://msdn.microsoft.com/library/windows/apps/dn948834) property which contains suggested frame controllers for capturing a variable photo sequence for HDR processing."},{"content":"For more information, see <bpt id=\"p1\">[</bpt>Variable photo sequence<ept id=\"p1\">](variable-photo-sequence.md)</ept>.","pos":[5576,5656],"source":" For more information, see [Variable photo sequence](variable-photo-sequence.md)."},{"content":"Clean up the scene analysis effect","pos":[5662,5696]},{"content":"When your app is done capturing, before disposing of the <bpt id=\"p1\">**</bpt>MediaCapture<ept id=\"p1\">**</ept> object, you should disable the scene analysis effect by setting the effect's <bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>HighDynamicRangeAnalyzer.Enabled<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/dn948827)</ept> property to false and unregister your <bpt id=\"p4\">[</bpt><bpt id=\"p5\">**</bpt>SceneAnalyzed<ept id=\"p5\">**</ept><ept id=\"p4\">](https://msdn.microsoft.com/library/windows/apps/dn948920)</ept> event handler.","pos":[5698,6076],"source":"When your app is done capturing, before disposing of the **MediaCapture** object, you should disable the scene analysis effect by setting the effect's [**HighDynamicRangeAnalyzer.Enabled**](https://msdn.microsoft.com/library/windows/apps/dn948827) property to false and unregister your [**SceneAnalyzed**](https://msdn.microsoft.com/library/windows/apps/dn948920) event handler."},{"content":"Call <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>MediaCapture.ClearEffectsAsync<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/br226592)</ept>, specifying the video preview stream since that was the stream to which the effect was added.","pos":[6077,6270],"source":" Call [**MediaCapture.ClearEffectsAsync**](https://msdn.microsoft.com/library/windows/apps/br226592), specifying the video preview stream since that was the stream to which the effect was added."},{"content":"Finally, set your member variable to null.","pos":[6271,6313]},{"pos":[6325,6356],"content":"CleanUpSceneAnalysisEffectAsync"},{"content":"Face detection effect","pos":[6453,6474]},{"content":"The <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>FaceDetectionEffect<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948776)</ept> identifies the location of faces within the media capture preview stream.","pos":[6476,6637],"source":"The [**FaceDetectionEffect**](https://msdn.microsoft.com/library/windows/apps/dn948776) identifies the location of faces within the media capture preview stream."},{"content":"The effect allows you to receive a notification whenever a face is detected in the preview stream and provides the bounding box for each detected face within the preview frame.","pos":[6638,6814]},{"content":"On supported devices, the face detection effect also provides enhanced exposure and focus on the most important face in the scene.","pos":[6815,6945]},{"content":"Face detection namespaces","pos":[6951,6976]},{"content":"To use face detection, your app must include the following namespaces in addition to the namespaces required for basic media capture.","pos":[6978,7111]},{"pos":[7123,7141],"content":"FaceDetectionUsing"},{"content":"Initialize the face detection effect and add it to the preview stream","pos":[7226,7295]},{"content":"Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.","pos":[7297,7509]},{"content":"Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.","pos":[7510,7664]},{"pos":[7676,7702],"content":"DeclareFaceDetectionEffect"},{"content":"In your app, after you have initialized the <bpt id=\"p1\">**</bpt>MediaCapture<ept id=\"p1\">**</ept> object, create a new instance of <bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>FaceDetectionEffectDefinition<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/dn948778)</ept>.","pos":[7791,7979],"source":"In your app, after you have initialized the **MediaCapture** object, create a new instance of [**FaceDetectionEffectDefinition**](https://msdn.microsoft.com/library/windows/apps/dn948778)."},{"content":"Set the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>DetectionMode<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948781)</ept> property to prioritize faster face detection or more accurate face detection.","pos":[7980,8143],"source":" Set the [**DetectionMode**](https://msdn.microsoft.com/library/windows/apps/dn948781) property to prioritize faster face detection or more accurate face detection."},{"content":"Set <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SynchronousDetectionEnabled<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948786)</ept> to specify that incoming frames are not delayed waiting for face detection to complete as this can result in a choppy preview experience.","pos":[8144,8377],"source":" Set [**SynchronousDetectionEnabled**](https://msdn.microsoft.com/library/windows/apps/dn948786) to specify that incoming frames are not delayed waiting for face detection to complete as this can result in a choppy preview experience."},{"content":"Register the effect with the capture device by calling <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>AddVideoEffectAsync<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn878035)</ept> on your <bpt id=\"p3\">**</bpt>MediaCapture<ept id=\"p3\">**</ept> object, providing the <bpt id=\"p4\">**</bpt>FaceDetectionEffectDefinition<ept id=\"p4\">**</ept> and specifying <bpt id=\"p5\">[</bpt><bpt id=\"p6\">**</bpt>MediaStreamType.VideoPreview<ept id=\"p6\">**</ept><ept id=\"p5\">](https://msdn.microsoft.com/library/windows/apps/br226640)</ept> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.","pos":[8379,8815],"source":"Register the effect with the capture device by calling [**AddVideoEffectAsync**](https://msdn.microsoft.com/library/windows/apps/dn878035) on your **MediaCapture** object, providing the **FaceDetectionEffectDefinition** and specifying [**MediaStreamType.VideoPreview**](https://msdn.microsoft.com/library/windows/apps/br226640) to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream."},{"content":"<bpt id=\"p1\">**</bpt>AddVideoEffectAsync<ept id=\"p1\">**</ept> returns an instance of the added effect.","pos":[8816,8880],"source":"**AddVideoEffectAsync** returns an instance of the added effect."},{"content":"Because this method can be used with multiple effect types, you must cast the returned instance to a <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>FaceDetectionEffect<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948776)</ept> object.","pos":[8881,9073],"source":" Because this method can be used with multiple effect types, you must cast the returned instance to a [**FaceDetectionEffect**](https://msdn.microsoft.com/library/windows/apps/dn948776) object."},{"content":"Enable or disable the effect by setting the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>FaceDetectionEffect.Enabled<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948818)</ept> property.","pos":[9075,9220],"source":"Enable or disable the effect by setting the [**FaceDetectionEffect.Enabled**](https://msdn.microsoft.com/library/windows/apps/dn948818) property."},{"content":"Adjust how often the effect analyzes frames by setting the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>FaceDetectionEffect.DesiredDetectionInterval<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948814)</ept> property.","pos":[9221,9398],"source":" Adjust how often the effect analyzes frames by setting the [**FaceDetectionEffect.DesiredDetectionInterval**](https://msdn.microsoft.com/library/windows/apps/dn948814) property."},{"content":"Both of these properties can be adjusted while media capture is ongoing.","pos":[9399,9471]},{"pos":[9483,9513],"content":"CreateFaceDetectionEffectAsync"},{"content":"Receive notifications when faces are detected","pos":[9610,9655]},{"pos":[9657,9891],"content":"If you want to perform some action when faces are detected, such as drawing a box around detected faces in the video preview, you can register for the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>FaceDetected<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948820)</ept> event.","source":"If you want to perform some action when faces are detected, such as drawing a box around detected faces in the video preview, you can register for the [**FaceDetected**](https://msdn.microsoft.com/library/windows/apps/dn948820) event."},{"pos":[9903,9931],"content":"RegisterFaceDetectionHandler"},{"content":"In the handler for the event, you can get a list of all faces detected in a frame by accessing the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>FaceDetectionEffectFrame.DetectedFaces<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn948792)</ept> property of the <bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>FaceDetectedEventArgs<ept id=\"p4\">**</ept><ept id=\"p3\">](https://msdn.microsoft.com/library/windows/apps/dn948774)</ept>.","pos":[10022,10326],"source":"In the handler for the event, you can get a list of all faces detected in a frame by accessing the [**FaceDetectionEffectFrame.DetectedFaces**](https://msdn.microsoft.com/library/windows/apps/dn948792) property of the [**FaceDetectedEventArgs**](https://msdn.microsoft.com/library/windows/apps/dn948774)."},{"content":"The <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>FaceBox<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn974126)</ept> property is a <bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>BitmapBounds<ept id=\"p4\">**</ept><ept id=\"p3\">](https://msdn.microsoft.com/library/windows/apps/br226169)</ept> structure that describes the rectangle containing the detected face in units relative to the preview stream dimensions.","pos":[10327,10613],"source":" The [**FaceBox**](https://msdn.microsoft.com/library/windows/apps/dn974126) property is a [**BitmapBounds**](https://msdn.microsoft.com/library/windows/apps/br226169) structure that describes the rectangle containing the detected face in units relative to the preview stream dimensions."},{"content":"To view sample code that transforms the preview stream coordinates into screen coordinates, see the <bpt id=\"p1\">[</bpt>face detection UWP sample<ept id=\"p1\">](http://go.microsoft.com/fwlink/?LinkId=619486)</ept>.","pos":[10614,10789],"source":" To view sample code that transforms the preview stream coordinates into screen coordinates, see the [face detection UWP sample](http://go.microsoft.com/fwlink/?LinkId=619486)."},{"pos":[10801,10813],"content":"FaceDetected"},{"content":"Clean up the face detection effect","pos":[10892,10926]},{"content":"When your app is done capturing, before disposing of the <bpt id=\"p1\">**</bpt>MediaCapture<ept id=\"p1\">**</ept> object, you should disable the face detection effect with <bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>FaceDetectionEffect.Enabled<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/dn948818)</ept> and unregister your <bpt id=\"p4\">[</bpt><bpt id=\"p5\">**</bpt>FaceDetected<ept id=\"p5\">**</ept><ept id=\"p4\">](https://msdn.microsoft.com/library/windows/apps/dn948820)</ept> event handler if you previously registered one.","pos":[10928,11296],"source":"When your app is done capturing, before disposing of the **MediaCapture** object, you should disable the face detection effect with [**FaceDetectionEffect.Enabled**](https://msdn.microsoft.com/library/windows/apps/dn948818) and unregister your [**FaceDetected**](https://msdn.microsoft.com/library/windows/apps/dn948820) event handler if you previously registered one."},{"content":"Call <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>MediaCapture.ClearEffectsAsync<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/br226592)</ept>, specifying the video preview stream since that was the stream to which the effect was added.","pos":[11297,11490],"source":" Call [**MediaCapture.ClearEffectsAsync**](https://msdn.microsoft.com/library/windows/apps/br226592), specifying the video preview stream since that was the stream to which the effect was added."},{"content":"Finally, set your member variable to null.","pos":[11491,11533]},{"pos":[11545,11576],"content":"CleanUpFaceDetectionEffectAsync"},{"content":"Check for focus and exposure support for detected faces","pos":[11674,11729]},{"content":"Not all devices have a capture device that can adjust its focus and exposure based on detected faces.","pos":[11731,11832]},{"content":"Because face detection consumes device resources, you may only want to enable face detection on devices that can use the feature to enhance capture.","pos":[11833,11981]},{"content":"To see if face-based capture optimization is available, get the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>VideoDeviceController<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/br226825)</ept> for your initialized <bpt id=\"p3\">[</bpt>MediaCapture<ept id=\"p3\">](capture-photos-and-video-with-mediacapture.md)</ept> and then get the video device controller's <bpt id=\"p4\">[</bpt><bpt id=\"p5\">**</bpt>RegionsOfInterestControl<ept id=\"p5\">**</ept><ept id=\"p4\">](https://msdn.microsoft.com/library/windows/apps/dn279064)</ept>.","pos":[11982,12347],"source":" To see if face-based capture optimization is available, get the [**VideoDeviceController**](https://msdn.microsoft.com/library/windows/apps/br226825) for your initialized [MediaCapture](capture-photos-and-video-with-mediacapture.md) and then get the video device controller's [**RegionsOfInterestControl**](https://msdn.microsoft.com/library/windows/apps/dn279064)."},{"content":"Check to see if the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>MaxRegions<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn279069)</ept> supports at least one region.","pos":[12348,12472],"source":" Check to see if the [**MaxRegions**](https://msdn.microsoft.com/library/windows/apps/dn279069) supports at least one region."},{"content":"Then check to see if either <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>AutoExposureSupported<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn279065)</ept> or <bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>AutoFocusSupported<ept id=\"p4\">**</ept><ept id=\"p3\">](https://msdn.microsoft.com/library/windows/apps/dn279066)</ept> are true.","pos":[12473,12682],"source":" Then check to see if either [**AutoExposureSupported**](https://msdn.microsoft.com/library/windows/apps/dn279065) or [**AutoFocusSupported**](https://msdn.microsoft.com/library/windows/apps/dn279066) are true."},{"content":"If these conditions are met, then the device can take advantage of face detection to enhance capture.","pos":[12683,12784]},{"pos":[12796,12828],"content":"AreFaceFocusAndExposureSupported"},{"content":"Related topics","pos":[12926,12940]},{"content":"Capture photos and video with MediaCapture","pos":[12945,12987]}],"content":"---\nauthor: drewbatgit\nms.assetid: B5D915E4-4280-422C-BA0E-D574C534410B\ndescription: This article describes how to use the SceneAnalysisEffect and the FaceDetectionEffect to analyze the content of the media capture preview stream.\ntitle: Scene analysis for media capture\n---\n\n# Scene analysis for media capture\n\n\\[ Updated for UWP apps on Windows 10. For Windows 8.x articles, see the [archive](http://go.microsoft.com/fwlink/p/?linkid=619132) \\]\n\n\nThis article describes how to use the [**SceneAnalysisEffect**](https://msdn.microsoft.com/library/windows/apps/dn948902) and the [**FaceDetectionEffect**](https://msdn.microsoft.com/library/windows/apps/dn948776) to analyze the content of the media capture preview stream.\n\n## Scene analysis effect\n\nThe [**SceneAnalysisEffect**](https://msdn.microsoft.com/library/windows/apps/dn948902) analyzes the video frames in the media capture preview stream and recommends processing options to improve the capture result. Currently, the effect supports detecting whether the capture would be improved by using High Dynamic Range (HDR) processing.\n\nIf the effect recommends using HDR, you can do this in the following ways:\n\n-   Use the [**AdvancedPhotoCapture**](https://msdn.microsoft.com/library/windows/apps/mt181386) class to capture photos using the Windows built-in HDR processing algorithm. For more information, see [High Dynamic Range (HDR) photo capture](high-dynamic-range-hdr-photo-capture.md).\n\n-   Use the [**HdrVideoControl**](https://msdn.microsoft.com/library/windows/apps/dn926680) to capture video using the Windows built-in HDR processing algorithm. For more information, see [Capture device controls for video capture](capture-device-controls-for-video-capture.md).\n\n-   Use the [**VariablePhotoSequenceControl**](https://msdn.microsoft.com/library/windows/apps/dn640573) to capture a sequence of frames that you can then composite using a custom HDR implementation. For more information, see [Variable photo sequence](variable-photo-sequence.md).\n\n### Scene analysis namespaces\n\nTo use scene analysis, your app must include the following namespaces in addition to the namespaces required for basic media capture.\n\n[!code-cs[SceneAnalysisUsing](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetSceneAnalysisUsing)]\n\n### Initialize the scene analysis effect and add it to the preview stream\n\nVideo effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect. Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.\n\n[!code-cs[DeclareSceneAnalysisEffect](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetDeclareSceneAnalysisEffect)]\n\nIn your app, after you have initialized the **MediaCapture** object, create a new instance of [**SceneAnalysisEffectDefinition**](https://msdn.microsoft.com/library/windows/apps/dn948903).\n\nRegister the effect with the capture device by calling [**AddVideoEffectAsync**](https://msdn.microsoft.com/library/windows/apps/dn878035) on your **MediaCapture** object, providing the **SceneAnalysisEffectDefinition** and specifying [**MediaStreamType.VideoPreview**](https://msdn.microsoft.com/library/windows/apps/br226640) to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream. **AddVideoEffectAsync** returns an instance of the added effect. Because this method can be used with multiple effect types, you must cast the returned instance to a [**SceneAnalysisEffect**](https://msdn.microsoft.com/library/windows/apps/dn948902) object.\n\nTo receive the results of the scene analysis, you must register a handler for the [**SceneAnalyzed**](https://msdn.microsoft.com/library/windows/apps/dn948920) event.\n\nCurrently, the scene analysis effect only includes the high dynamic range analyzer. Enable HDR analysis by setting the effect's [**HighDynamicRangeControl.Enabled**](https://msdn.microsoft.com/library/windows/apps/dn948827) to true.\n\n[!code-cs[CreateSceneAnalysisEffectAsync](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetCreateSceneAnalysisEffectAsync)]\n\n### Implement the SceneAnalyzed event handler\n\nThe results of the scene analysis are returned in the **SceneAnalyzed** event handler. The [**SceneAnalyzedEventArgs**](https://msdn.microsoft.com/library/windows/apps/dn948922) object passed into the handler has a [**SceneAnalysisEffectFrame**](https://msdn.microsoft.com/library/windows/apps/dn948907) object which has a [**HighDynamicRangeOutput**](https://msdn.microsoft.com/library/windows/apps/dn948830) object. The [**Certainty**](https://msdn.microsoft.com/library/windows/apps/dn948833) property of the high dynamic range output provides a value between 0 and 1.0 where 0 indicates that HDR processing would not help improve the capture result and 1.0 indicates that HDR processing would help. Your can decide the threshold point at which you want to use HDR or show the results to the user and let the user decide.\n\n[!code-cs[SceneAnalyzed](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetSceneAnalyzed)]\n\nThe [**HighDynamicRangeOutput**](https://msdn.microsoft.com/library/windows/apps/dn948830) object passed into the handler also has a [**FrameControllers**](https://msdn.microsoft.com/library/windows/apps/dn948834) property which contains suggested frame controllers for capturing a variable photo sequence for HDR processing. For more information, see [Variable photo sequence](variable-photo-sequence.md).\n\n### Clean up the scene analysis effect\n\nWhen your app is done capturing, before disposing of the **MediaCapture** object, you should disable the scene analysis effect by setting the effect's [**HighDynamicRangeAnalyzer.Enabled**](https://msdn.microsoft.com/library/windows/apps/dn948827) property to false and unregister your [**SceneAnalyzed**](https://msdn.microsoft.com/library/windows/apps/dn948920) event handler. Call [**MediaCapture.ClearEffectsAsync**](https://msdn.microsoft.com/library/windows/apps/br226592), specifying the video preview stream since that was the stream to which the effect was added. Finally, set your member variable to null.\n\n[!code-cs[CleanUpSceneAnalysisEffectAsync](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetCleanUpSceneAnalysisEffectAsync)]\n\n## Face detection effect\n\nThe [**FaceDetectionEffect**](https://msdn.microsoft.com/library/windows/apps/dn948776) identifies the location of faces within the media capture preview stream. The effect allows you to receive a notification whenever a face is detected in the preview stream and provides the bounding box for each detected face within the preview frame. On supported devices, the face detection effect also provides enhanced exposure and focus on the most important face in the scene.\n\n### Face detection namespaces\n\nTo use face detection, your app must include the following namespaces in addition to the namespaces required for basic media capture.\n\n[!code-cs[FaceDetectionUsing](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetFaceDetectionUsing)]\n\n### Initialize the face detection effect and add it to the preview stream\n\nVideo effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect. Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.\n\n[!code-cs[DeclareFaceDetectionEffect](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetDeclareFaceDetectionEffect)]\n\nIn your app, after you have initialized the **MediaCapture** object, create a new instance of [**FaceDetectionEffectDefinition**](https://msdn.microsoft.com/library/windows/apps/dn948778). Set the [**DetectionMode**](https://msdn.microsoft.com/library/windows/apps/dn948781) property to prioritize faster face detection or more accurate face detection. Set [**SynchronousDetectionEnabled**](https://msdn.microsoft.com/library/windows/apps/dn948786) to specify that incoming frames are not delayed waiting for face detection to complete as this can result in a choppy preview experience.\n\nRegister the effect with the capture device by calling [**AddVideoEffectAsync**](https://msdn.microsoft.com/library/windows/apps/dn878035) on your **MediaCapture** object, providing the **FaceDetectionEffectDefinition** and specifying [**MediaStreamType.VideoPreview**](https://msdn.microsoft.com/library/windows/apps/br226640) to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream. **AddVideoEffectAsync** returns an instance of the added effect. Because this method can be used with multiple effect types, you must cast the returned instance to a [**FaceDetectionEffect**](https://msdn.microsoft.com/library/windows/apps/dn948776) object.\n\nEnable or disable the effect by setting the [**FaceDetectionEffect.Enabled**](https://msdn.microsoft.com/library/windows/apps/dn948818) property. Adjust how often the effect analyzes frames by setting the [**FaceDetectionEffect.DesiredDetectionInterval**](https://msdn.microsoft.com/library/windows/apps/dn948814) property. Both of these properties can be adjusted while media capture is ongoing.\n\n[!code-cs[CreateFaceDetectionEffectAsync](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetCreateFaceDetectionEffectAsync)]\n\n### Receive notifications when faces are detected\n\nIf you want to perform some action when faces are detected, such as drawing a box around detected faces in the video preview, you can register for the [**FaceDetected**](https://msdn.microsoft.com/library/windows/apps/dn948820) event.\n\n[!code-cs[RegisterFaceDetectionHandler](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetRegisterFaceDetectionHandler)]\n\nIn the handler for the event, you can get a list of all faces detected in a frame by accessing the [**FaceDetectionEffectFrame.DetectedFaces**](https://msdn.microsoft.com/library/windows/apps/dn948792) property of the [**FaceDetectedEventArgs**](https://msdn.microsoft.com/library/windows/apps/dn948774). The [**FaceBox**](https://msdn.microsoft.com/library/windows/apps/dn974126) property is a [**BitmapBounds**](https://msdn.microsoft.com/library/windows/apps/br226169) structure that describes the rectangle containing the detected face in units relative to the preview stream dimensions. To view sample code that transforms the preview stream coordinates into screen coordinates, see the [face detection UWP sample](http://go.microsoft.com/fwlink/?LinkId=619486).\n\n[!code-cs[FaceDetected](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetFaceDetected)]\n\n### Clean up the face detection effect\n\nWhen your app is done capturing, before disposing of the **MediaCapture** object, you should disable the face detection effect with [**FaceDetectionEffect.Enabled**](https://msdn.microsoft.com/library/windows/apps/dn948818) and unregister your [**FaceDetected**](https://msdn.microsoft.com/library/windows/apps/dn948820) event handler if you previously registered one. Call [**MediaCapture.ClearEffectsAsync**](https://msdn.microsoft.com/library/windows/apps/br226592), specifying the video preview stream since that was the stream to which the effect was added. Finally, set your member variable to null.\n\n[!code-cs[CleanUpFaceDetectionEffectAsync](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetCleanUpFaceDetectionEffectAsync)]\n\n### Check for focus and exposure support for detected faces\n\nNot all devices have a capture device that can adjust its focus and exposure based on detected faces. Because face detection consumes device resources, you may only want to enable face detection on devices that can use the feature to enhance capture. To see if face-based capture optimization is available, get the [**VideoDeviceController**](https://msdn.microsoft.com/library/windows/apps/br226825) for your initialized [MediaCapture](capture-photos-and-video-with-mediacapture.md) and then get the video device controller's [**RegionsOfInterestControl**](https://msdn.microsoft.com/library/windows/apps/dn279064). Check to see if the [**MaxRegions**](https://msdn.microsoft.com/library/windows/apps/dn279069) supports at least one region. Then check to see if either [**AutoExposureSupported**](https://msdn.microsoft.com/library/windows/apps/dn279065) or [**AutoFocusSupported**](https://msdn.microsoft.com/library/windows/apps/dn279066) are true. If these conditions are met, then the device can take advantage of face detection to enhance capture.\n\n[!code-cs[AreFaceFocusAndExposureSupported](./code/BasicMediaCaptureWin10/cs/MainPage.xaml.cs#SnippetAreFaceFocusAndExposureSupported)]\n\n## Related topics\n\n* [Capture photos and video with MediaCapture](capture-photos-and-video-with-mediacapture.md)\n \n\n \n\n\n\n\n"}
{"nodes":[{"pos":[17,122],"content":"Incorporate speech into your apps using Cortana voice commands, speech recognition, and speech synthesis.","needQuote":true,"needEscape":true,"nodes":[{"content":"Incorporate speech into your apps using Cortana voice commands, speech recognition, and speech synthesis.","pos":[0,105]}]},{"pos":[130,149],"content":"Speech interactions","needQuote":true,"needEscape":true,"nodes":[{"content":"Speech interactions","pos":[0,19]}]},{"content":"Speech interactions","pos":[254,273]},{"content":"\\[ Updated for UWP apps on Windows 10.","pos":[274,312]},{"content":"For Windows 8.x articles, see the <bpt id=\"p1\">[</bpt>archive<ept id=\"p1\">](http://go.microsoft.com/fwlink/p/?linkid=619132)</ept> \\]","pos":[313,408],"source":" For Windows 8.x articles, see the [archive](http://go.microsoft.com/fwlink/p/?linkid=619132) \\]"},{"content":"Integrate speech recognition and text-to-speech (also known as TTS, or speech synthesis) directly into the user experience of your app.","pos":[410,545]},{"content":"Other speech components","pos":[550,573]},{"pos":[581,702],"content":"See the <bpt id=\"p1\">[</bpt>Cortana design guidelines<ept id=\"p1\">](cortana-interactions.md)</ept> if you are exposing app functionality in the <bpt id=\"p2\">**</bpt>Cortana<ept id=\"p2\">**</ept> UI.","source":"See the [Cortana design guidelines](cortana-interactions.md) if you are exposing app functionality in the **Cortana** UI."},{"content":"**Speech recognition:  **converts words spoken by the user into text for form input, for text dictation, to specify an action or command, and to accomplish tasks.","pos":[705,867]},{"content":"Both pre-defined grammars for free-text dictation and web search, and custom grammars authored using Speech Recognition Grammar Specification (SRGS) Version 1.0 are supported.","pos":[868,1043]},{"content":"**TTS:  **uses a speech synthesis engine (voice) to convert a text string into spoken words.","pos":[1045,1137]},{"content":"The input string can be either basic, unadorned text or more complex Speech Synthesis Markup Language (SSML).","pos":[1138,1247]},{"content":"SSML provides a standard way to control characteristics of speech output, such as pronunciation, volume, pitch, rate or speed, and emphasis.","pos":[1248,1388]},{"content":"<bpt id=\"p1\">**</bpt>Note<ept id=\"p1\">**</ept>  Using <bpt id=\"p2\">**</bpt>Cortana<ept id=\"p2\">**</ept> and customized voice commands, your app can be launched in the foreground (the app takes focus, just as if it was launched from the Start menu) or activated as a background service (<bpt id=\"p3\">**</bpt>Cortana<ept id=\"p3\">**</ept> retains focus but provides results from the app).","pos":[1390,1661],"source":"**Note**  Using **Cortana** and customized voice commands, your app can be launched in the foreground (the app takes focus, just as if it was launched from the Start menu) or activated as a background service (**Cortana** retains focus but provides results from the app)."},{"content":"Commands that require additional context or user input (such as sending a message to a specific contact) are best handled in a foreground app, while basic commands can be handled in <bpt id=\"p1\">**</bpt>Cortana<ept id=\"p1\">**</ept> through a background app.","pos":[1662,1881],"source":" Commands that require additional context or user input (such as sending a message to a specific contact) are best handled in a foreground app, while basic commands can be handled in **Cortana** through a background app."},{"content":"If you are exposing functionality as a background service through voice commands in the <bpt id=\"p1\">**</bpt>Cortana<ept id=\"p1\">**</ept> UI, see the <bpt id=\"p2\">[</bpt>Cortana design guidelines<ept id=\"p2\">](cortana-design-guidelines.md)</ept>.","pos":[1882,2052],"source":"\nIf you are exposing functionality as a background service through voice commands in the **Cortana** UI, see the [Cortana design guidelines](cortana-design-guidelines.md)."},{"content":"Designed and implemented thoughtfully, speech can be a robust and enjoyable way for people to interact with your app, complementing, or even replacing, keyboard, mouse, touch, and gestures.","pos":[2057,2246]},{"pos":[2383,2408],"content":"Speech interaction design"},{"content":"These guidelines and recommendations describe how to best integrate both speech recognition and TTS into the interaction experience of your app.","pos":[2411,2555]},{"content":"If you are considering supporting speech interactions in your app:","pos":[2557,2623]},{"content":"What actions can be taken through speech?","pos":[2629,2670]},{"content":"Can a user navigate between pages, invoke commands, or enter data as text fields, brief notes, or long messages?","pos":[2671,2783]},{"content":"Is speech input a good option for completing a task?","pos":[2788,2840]},{"content":"How does a user know when speech input is available?","pos":[2845,2897]},{"content":"Is the app always listening, or does the user need to take an action for the app to enter listening mode?","pos":[2902,3007]},{"content":"What phrases initiate an action or behavior?","pos":[3012,3056]},{"content":"Do the phrases and actions need to be enumerated on screen?","pos":[3057,3116]},{"content":"Are prompt, confirmation, and disambiguation screens or TTS required?","pos":[3121,3190]},{"content":"What is the interaction dialog between app and user?","pos":[3195,3247]},{"content":"Is a custom or constrained vocabulary required (such as medicine, science, or locale) for the context of your app?","pos":[3252,3366]},{"content":"Is network connectivity required?","pos":[3371,3404]},{"pos":[3496,3506],"content":"Text input"},{"content":"Speech for text input can range from short form (single word or phrase) to long form (continuous dictation).","pos":[3509,3617]},{"content":"Short form input must be less than 10 seconds in length, while long form input session can be up to two minutes in length.","pos":[3618,3740]},{"content":"(Long form input can be restarted without user intervention to give the impression of continuous dictation.)","pos":[3741,3849]},{"content":"You should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on.","pos":[3851,3999]},{"content":"For example, a command bar button with a microphone glyph (see <bpt id=\"p1\">[</bpt>Command bars<ept id=\"p1\">](../controls-and-patterns/app-bars.md)</ept>) can be used to show both availability and state.","pos":[4000,4165],"source":" For example, a command bar button with a microphone glyph (see [Command bars](../controls-and-patterns/app-bars.md)) can be used to show both availability and state."},{"content":"Provide ongoing recognition feedback to minimize any apparent lack of response while recognition is being performed.","pos":[4167,4283]},{"content":"Let users revise recognition text using keyboard input, disambiguation prompts, suggestions, or additional speech recognition.","pos":[4285,4411]},{"content":"Stop recognition if input is detected from a device other than speech recognition, such as touch or keyboard.","pos":[4413,4522]},{"content":"This probably indicates that the user has moved onto another task, such as correcting the recognition text or interacting with other form fields.","pos":[4523,4668]},{"content":"Specify the length of time for which no speech input indicates that recognition is over.","pos":[4670,4758]},{"content":"Do not automatically restart recognition after this period of time as it typically indicates the user has stopped engaging with your app.","pos":[4759,4896]},{"content":"Disable all continuous recognition UI and terminate the recognition session if a network connection is not available.","pos":[4898,5015]},{"content":"Continuous recogntion requires a network connection.","pos":[5016,5068]},{"pos":[5160,5170],"content":"Commanding"},{"content":"Speech input can initiate actions, invoke commands, and accomplish tasks.","pos":[5173,5246]},{"content":"If space permits, consider displaying the supported responses for the current app context, with examples of valid input.","pos":[5248,5368]},{"content":"This reduces the potential responses your app has to process and also eliminates confusion for the user.","pos":[5369,5473]},{"content":"Try to frame your questions such that they elicit as specific a response as possible.","pos":[5475,5560]},{"content":"For example, \"What do you want to do today?\"","pos":[5561,5605]},{"content":"is very open ended and would require a very large grammar definition due to how varied the responses could be.","pos":[5606,5716]},{"content":"Alternatively, \"Would you like to play a game or listen to music?\"","pos":[5717,5783]},{"content":"constrains the response to one of two valid answers with a correspondingly small grammar definition.","pos":[5784,5884]},{"content":"A small grammar is much easier to author and results in much more accurate recognition results.","pos":[5885,5980]},{"content":"Request confirmation from the user when speech recognition confidence is low.","pos":[5982,6059]},{"content":"If the user's intent is unclear, it's better to get clarification than to initiate an unintended action.","pos":[6060,6164]},{"content":"You should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on.","pos":[6166,6314]},{"content":"For example, a command bar button with a microphone glyph (see <bpt id=\"p1\">[</bpt>Guidelines for command bars<ept id=\"p1\">](../controls-and-patterns/app-bars.md)</ept>) can be used to show both availability and state.","pos":[6315,6495],"source":" For example, a command bar button with a microphone glyph (see [Guidelines for command bars](../controls-and-patterns/app-bars.md)) can be used to show both availability and state."},{"content":"If the speech recognition switch is typically out of view, consider displaying a state indicator in the content area of the app.","pos":[6497,6625]},{"content":"If recognition is initiated by the user, consider using the built-in recognition experience for consistency.","pos":[6627,6735]},{"content":"The built-in experience includes customizable screens with prompts, examples, disambiguations, confirmations, and errors.","pos":[6736,6857]},{"content":"The screens vary depending on the specified constraints:","pos":[6859,6915]},{"content":"Pre-defined grammar (dictation or web search)","pos":[6921,6966]},{"pos":[6976,7001],"content":"The <bpt id=\"p1\">**</bpt>Listening<ept id=\"p1\">**</ept> screen.","source":"The **Listening** screen."},{"pos":[7010,7034],"content":"The <bpt id=\"p1\">**</bpt>Thinking<ept id=\"p1\">**</ept> screen.","source":"The **Thinking** screen."},{"pos":[7043,7092],"content":"The <bpt id=\"p1\">**</bpt>Heard you say<ept id=\"p1\">**</ept> screen or the error screen.","source":"The **Heard you say** screen or the error screen."},{"content":"List of words or phrases, or a SRGS grammar file","pos":[7097,7145]},{"pos":[7155,7180],"content":"The <bpt id=\"p1\">**</bpt>Listening<ept id=\"p1\">**</ept> screen.","source":"The **Listening** screen."},{"pos":[7189,7294],"content":"The <bpt id=\"p1\">**</bpt>Did you say<ept id=\"p1\">**</ept> screen, if what the user said could be interpreted as more than one potential result.","source":"The **Did you say** screen, if what the user said could be interpreted as more than one potential result."},{"pos":[7303,7352],"content":"The <bpt id=\"p1\">**</bpt>Heard you say<ept id=\"p1\">**</ept> screen or the error screen.","source":"The **Heard you say** screen or the error screen."},{"pos":[7354,7390],"content":"On the <bpt id=\"p1\">**</bpt>Listening<ept id=\"p1\">**</ept> screen you can:","source":"On the **Listening** screen you can:"},{"content":"Customize the heading text.","pos":[7396,7423]},{"content":"Provide example text of what the user can say.","pos":[7428,7474]},{"pos":[7479,7533],"content":"Specify whether the <bpt id=\"p1\">**</bpt>Heard you say<ept id=\"p1\">**</ept> screen is shown.","source":"Specify whether the **Heard you say** screen is shown."},{"pos":[7538,7614],"content":"Read the recognized string back to the user on the <bpt id=\"p1\">**</bpt>Heard you say<ept id=\"p1\">**</ept> screen.","source":"Read the recognized string back to the user on the **Heard you say** screen."},{"content":"Here is an example of the built-in recognition flow for a speech recognizer that uses a SRGS-defined constraint.","pos":[7616,7728]},{"content":"In this example, speech recognition is successful.","pos":[7729,7779]},{"content":"initial recognition screen for a constraint based on a sgrs grammar file","pos":[7783,7855]},{"content":"intermediate recognition screen for a constraint based on a sgrs grammar file","pos":[7904,7981]},{"content":"final recognition screen for a constraint based on a sgrs grammar file","pos":[8035,8105]},{"pos":[8261,8277],"content":"Always listening"},{"content":"Your app can listen for and recognize speech input as soon as the app is launched, without user intervention.","pos":[8280,8389]},{"content":"You should customize the grammar constraints based on the app context.","pos":[8391,8461]},{"content":"This keeps the speech recognition experience very targeted and relevant to the current task, and minimizes errors.","pos":[8462,8576]},{"pos":[8683,8700],"content":"\"What can I say?\""},{"content":"When speech input is enabled, it's important to help users discover what exactly can be understood and what actions can be performed.","pos":[8703,8836]},{"content":"If speech recognition is user enabled, consider using the command bar or a menu command to show all words and phrases supported in the current context.","pos":[8838,8989]},{"content":"If speech recognition is always on, consider adding the phrase \"What can I say?\"","pos":[8991,9071]},{"content":"to every page.","pos":[9072,9086]},{"content":"When the user says this phrase, display all words and phrases supported in the current context.","pos":[9087,9182]},{"content":"Using this phrase provides a consistent way for users to discover speech capabilities across the system.","pos":[9183,9287]},{"pos":[9409,9429],"content":"Recognition failures"},{"content":"Speech recognition will fail.","pos":[9432,9461]},{"content":"Failures happen when audio quality is poor, when only part of a phrase is recognized, or when no input is detected at all.","pos":[9462,9584]},{"content":"Handle failure gracefully, help a user understand why recognition failed, and recover.","pos":[9586,9672]},{"content":"Your app should inform the user that they weren't understood and that they need to try again.","pos":[9674,9767]},{"content":"Consider providing examples of one or more supported phrases.","pos":[9769,9830]},{"content":"The user is likely to repeat a suggested phrase, which increases recognition success.","pos":[9831,9916]},{"content":"You should display a list of potential matches for a user to select from.","pos":[9918,9991]},{"content":"This can be far more efficient than going through the recognition process again.","pos":[9992,10072]},{"content":"You should always support alternative input types, which is especially helpful for handling repeated recognition failures.","pos":[10074,10196]},{"content":"For example, you could suggest that the user try to use a keyboard, or use touch or a mouse to select from a list of potential matches.","pos":[10197,10332]},{"content":"Use the built-in speech recognition experience as it includes screens that inform the user that recognition was not successful and lets the user make another recognition attempt.","pos":[10334,10512]},{"content":"Listen for and try to correct issues in the audio input.","pos":[10514,10570]},{"content":"The speech recognizer can detect issues with the audio quality that might adversely affect speech recognition accuracy.","pos":[10571,10690]},{"content":"You can use the information provided by the speech recognizer to inform the user of the issue and let them take corrective action, if possible.","pos":[10691,10834]},{"content":"For example, if the volume setting on the microphone is too low, you can prompt the user to speak louder or turn the volume up.","pos":[10835,10962]},{"pos":[11057,11068],"content":"Constraints"},{"content":"Constraints, or grammars, define the spoken words and phrases that can be matched by the speech recognizer.","pos":[11071,11178]},{"content":"You can specify one of the pre-defined web service grammars or you can create a custom grammar that is installed with your app.","pos":[11179,11306]},{"pos":[11426,11445],"content":"Predefined grammars"},{"content":"Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar.","pos":[11447,11574]},{"content":"When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device","pos":[11575,11700]},{"content":"The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases.","pos":[11706,11873]},{"content":"Free-text dictation is useful when you don't want to limit the kinds of things a user can say.","pos":[11874,11968]},{"content":"Typical uses include creating notes or dictating the content for a message.","pos":[11969,12044]},{"content":"The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say.","pos":[12049,12166]},{"content":"However, it is optimized to recognize terms that people typically use when searching the web.","pos":[12167,12260]},{"pos":[12262,12470],"content":"<bpt id=\"p1\">**</bpt>Note<ept id=\"p1\">**</ept>  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.","source":"**Note**  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device."},{"content":"These predefined grammars can be used to recognize up to 10 seconds of speech input and require no authoring effort on your part.","pos":[12475,12604]},{"content":"However, they do require connection to a network.","pos":[12605,12654]},{"pos":[12762,12777],"content":"Custom grammars"},{"content":"A custom grammar is designed and authored by you and is installed with your app.","pos":[12779,12859]},{"content":"Speech recognition using a custom constraint is performed on the device.","pos":[12860,12932]},{"content":"Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases.","pos":[12938,13060]},{"content":"A list constraint works well for recognizing short, distinct phrases.","pos":[13061,13130]},{"content":"Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match.","pos":[13131,13289]},{"content":"The list can also be programmatically updated.","pos":[13290,13336]},{"content":"An SRGS grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the <bpt id=\"p1\">[</bpt>SRGS Version 1.0<ept id=\"p1\">](http://go.microsoft.com/fwlink/p/?LinkID=262302)</ept>.","pos":[13341,13526],"source":"An SRGS grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the [SRGS Version 1.0](http://go.microsoft.com/fwlink/p/?LinkID=262302)."},{"content":"An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.","pos":[13527,13686]},{"content":"Here are some tips for authoring SRGS grammars:","pos":[13692,13739]},{"content":"Keep each grammar small.","pos":[13749,13773]},{"content":"Grammars that contain fewer phrases tend to provide more accurate recognition than larger grammars that contain many phrases.","pos":[13774,13899]},{"content":"It's better to have several smaller grammars for specific scenarios than to have a single grammar for your entire app.","pos":[13900,14018]},{"content":"Let users know what to say for each app context and enable and disable grammars as needed.","pos":[14027,14117]},{"content":"Design each grammar so users can speak a command in a variety of ways.","pos":[14126,14196]},{"content":"For example, you can use the <bpt id=\"p1\">**</bpt>GARBAGE<ept id=\"p1\">**</ept> rule to match speech input that your grammar does not define.","pos":[14197,14299],"source":" For example, you can use the **GARBAGE** rule to match speech input that your grammar does not define."},{"content":"This lets users speak additional words that have no meaning to your app.","pos":[14300,14372]},{"content":"For example, \"give me\", \"and\", \"uh\", \"maybe\", and so on.","pos":[14373,14429]},{"content":"Use the <bpt id=\"p1\">[</bpt>sapi:subset<ept id=\"p1\">](http://msdn.microsoft.com/library/windowsphone/design/jj572474.aspx)</ept> element to help match speech input.","pos":[14438,14564],"source":"Use the [sapi:subset](http://msdn.microsoft.com/library/windowsphone/design/jj572474.aspx) element to help match speech input."},{"content":"This is a Microsoft extension to the SRGS specification to help match partial phrases.","pos":[14565,14651]},{"content":"Try to avoid defining phrases in your grammar that contain only one syllable.","pos":[14660,14737]},{"content":"Recognition tends to be more accurate for phrases containing two or more syllables.","pos":[14738,14821]},{"content":"Avoid using phrases that sound similar.","pos":[14830,14869]},{"content":"For example, phrases such as \"hello\", \"bellow\", and \"fellow\" can confuse the recognition engine and result in poor recognition accuracy.","pos":[14870,15006]},{"content":"<bpt id=\"p1\">**</bpt>Note<ept id=\"p1\">**</ept>  Which type of constraint type you use depends on the complexity of the recognition experience you want to create.","pos":[15008,15131],"source":"**Note**  Which type of constraint type you use depends on the complexity of the recognition experience you want to create."},{"content":"Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.","pos":[15132,15259]},{"pos":[15388,15409],"content":"Custom pronunciations"},{"content":"If your app contains specialized vocabulary with unusual or fictional words, or words with uncommon pronunciations, you might be able to improve recognition performance for those words by defining custom pronunciations.","pos":[15411,15630]},{"content":"For a small list of words and phrases, or a list of infrequently used words and phrases, you can create custom pronunciations in a SRGS grammar.","pos":[15632,15776]},{"content":"See <bpt id=\"p1\">[</bpt>token Element<ept id=\"p1\">](http://msdn.microsoft.com/library/windowsphone/design/hh361600.aspx)</ept> for more info.","pos":[15777,15880],"source":" See [token Element](http://msdn.microsoft.com/library/windowsphone/design/hh361600.aspx) for more info."},{"content":"For larger lists of words and phrases, or frequently used words and phrases, you can create separate pronunciation lexicon documents.","pos":[15882,16015]},{"content":"See <bpt id=\"p1\">[</bpt>About Lexicons and Phonetic Alphabets<ept id=\"p1\">](http://msdn.microsoft.com/library/windowsphone/design/hh361646.aspx)</ept> for more info.","pos":[16016,16143],"source":" See [About Lexicons and Phonetic Alphabets](http://msdn.microsoft.com/library/windowsphone/design/hh361646.aspx) for more info."},{"pos":[16226,16233],"content":"Testing"},{"content":"Test speech recognition accuracy and any supporting UI with your app's target audience.","pos":[16236,16323]},{"content":"This is the best way to determine the effectiveness of the speech interaction experience in your app.","pos":[16324,16425]},{"content":"For example, are users getting poor recognition results because your app isn't listening for a common phrase?","pos":[16426,16535]},{"content":"Either modify the grammar to support this phrase or provide users with a list of supported phrases.","pos":[16537,16636]},{"content":"If you already provide the list of supported phrases, ensure it is easily discoverable.","pos":[16637,16724]},{"pos":[16846,16866],"content":"Text-to-speech (TTS)"},{"content":"TTS generates speech output from plain text or SSML.","pos":[16869,16921]},{"content":"Try to design prompts that are polite and encouraging.","pos":[16923,16977]},{"content":"Consider whether you should read long strings of text.","pos":[16979,17033]},{"content":"It's one thing to listen to a text message, but quite another to listen to a long list of search results that are difficult to remember.","pos":[17034,17170]},{"content":"You should provide media controls to let users pause, or stop, TTS.","pos":[17172,17239]},{"content":"You should listen to all TTS strings to ensure they are intelligible and sound natural.","pos":[17241,17328]},{"content":"Stringing together an unusual sequence of words or speaking part numbers or punctuation might cause a phrase to become unintelligible.","pos":[17334,17468]},{"content":"Speech can sound unnatural when the prosody or cadence is different from how a native speaker would say a phrase.","pos":[17473,17586]},{"content":"Both issues can be addressed bu using SSML instead of plain text as input to the speech synthesizer.","pos":[17588,17688]},{"content":"For more info about SSML, see <bpt id=\"p1\">[</bpt>Use SSML to Control Synthesized Speech<ept id=\"p1\">](http://msdn.microsoft.com/library/windowsphone/design/hh378454.aspx)</ept> and <bpt id=\"p2\">[</bpt>Speech Synthesis Markup Language Reference<ept id=\"p2\">](http://msdn.microsoft.com/library/windowsphone/design/hh378377.aspx)</ept>.","pos":[17689,17947],"source":" For more info about SSML, see [Use SSML to Control Synthesized Speech](http://msdn.microsoft.com/library/windowsphone/design/hh378454.aspx) and [Speech Synthesis Markup Language Reference](http://msdn.microsoft.com/library/windowsphone/design/hh378377.aspx)."},{"content":"Other articles in this section","pos":[17952,17982]},{"content":"Topic","pos":[18100,18105]},{"content":"Description","pos":[18128,18139]},{"content":"Speech recognition","pos":[18206,18224]},{"content":"Use speech recognition to provide input, specify an action or command, and accomplish tasks.","pos":[18278,18370]},{"content":"Specify the speech recognizer language","pos":[18425,18463]},{"content":"Learn how to select an installed language to use for speech recognition.","pos":[18537,18609]},{"content":"Define custom recognition constraints","pos":[18663,18700]},{"content":"Learn how to define and use custom constraints for speech recognition.","pos":[18773,18843]},{"content":"Enable continuous dictation","pos":[18898,18925]},{"content":"Learn how to capture and recognize long-form, continuous dictation speech input.","pos":[18988,19068]},{"content":"Manage issues with audio input","pos":[19122,19152]},{"content":"Learn how to manage issues with speech-recognition accuracy caused by audio-input quality.","pos":[19218,19308]},{"content":"Set speech recognition timeouts","pos":[19363,19394]},{"content":"Set how long a speech recognizer ignores silence or unrecognizable sounds (babble) and continues listening for speech input.","pos":[19461,19585]},{"pos":[19660,19676],"content":"Related articles"},{"content":"Speech interactions","pos":[19682,19701]},{"pos":[19763,19856],"content":"<bpt id=\"p1\">[</bpt>Cortana interactions<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/mt185598)</ept><ph id=\"ph1\">\n</ph><bpt id=\"p2\">**</bpt>Samples<ept id=\"p2\">**</ept>","leadings":[""," "],"source":"[Cortana interactions](https://msdn.microsoft.com/library/windows/apps/mt185598)\n**Samples**"},{"content":"Speech recognition and speech synthesis sample","pos":[19860,19906]}],"content":"---\nDescription: Incorporate speech into your apps using Cortana voice commands, speech recognition, and speech synthesis.\ntitle: Speech interactions\nms.assetid: 646DB3CE-FA81-4727-8C21-936C81079439\nlabel: Speech interactions\ntemplate: detail.hbs\n---\n\n# Speech interactions\n\\[ Updated for UWP apps on Windows 10. For Windows 8.x articles, see the [archive](http://go.microsoft.com/fwlink/p/?linkid=619132) \\]\n\nIntegrate speech recognition and text-to-speech (also known as TTS, or speech synthesis) directly into the user experience of your app.\n\n\n**Other speech components**\n\n-   See the [Cortana design guidelines](cortana-interactions.md) if you are exposing app functionality in the **Cortana** UI.\n\n\n**Speech recognition:  **converts words spoken by the user into text for form input, for text dictation, to specify an action or command, and to accomplish tasks. Both pre-defined grammars for free-text dictation and web search, and custom grammars authored using Speech Recognition Grammar Specification (SRGS) Version 1.0 are supported.\n\n**TTS:  **uses a speech synthesis engine (voice) to convert a text string into spoken words. The input string can be either basic, unadorned text or more complex Speech Synthesis Markup Language (SSML). SSML provides a standard way to control characteristics of speech output, such as pronunciation, volume, pitch, rate or speed, and emphasis.\n\n**Note**  Using **Cortana** and customized voice commands, your app can be launched in the foreground (the app takes focus, just as if it was launched from the Start menu) or activated as a background service (**Cortana** retains focus but provides results from the app). Commands that require additional context or user input (such as sending a message to a specific contact) are best handled in a foreground app, while basic commands can be handled in **Cortana** through a background app.\nIf you are exposing functionality as a background service through voice commands in the **Cortana** UI, see the [Cortana design guidelines](cortana-design-guidelines.md).\n\n \n\nDesigned and implemented thoughtfully, speech can be a robust and enjoyable way for people to interact with your app, complementing, or even replacing, keyboard, mouse, touch, and gestures.\n\n## <span id=\"Speech_interaction_design\"></span><span id=\"speech_interaction_design\"></span><span id=\"SPEECH_INTERACTION_DESIGN\"></span>Speech interaction design\n\n\nThese guidelines and recommendations describe how to best integrate both speech recognition and TTS into the interaction experience of your app.\n\nIf you are considering supporting speech interactions in your app:\n\n-   What actions can be taken through speech? Can a user navigate between pages, invoke commands, or enter data as text fields, brief notes, or long messages?\n-   Is speech input a good option for completing a task?\n-   How does a user know when speech input is available?\n-   Is the app always listening, or does the user need to take an action for the app to enter listening mode?\n-   What phrases initiate an action or behavior? Do the phrases and actions need to be enumerated on screen?\n-   Are prompt, confirmation, and disambiguation screens or TTS required?\n-   What is the interaction dialog between app and user?\n-   Is a custom or constrained vocabulary required (such as medicine, science, or locale) for the context of your app?\n-   Is network connectivity required?\n\n## <span id=\"Text_input\"></span><span id=\"text_input\"></span><span id=\"TEXT_INPUT\"></span>Text input\n\n\nSpeech for text input can range from short form (single word or phrase) to long form (continuous dictation). Short form input must be less than 10 seconds in length, while long form input session can be up to two minutes in length. (Long form input can be restarted without user intervention to give the impression of continuous dictation.)\n\nYou should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on. For example, a command bar button with a microphone glyph (see [Command bars](../controls-and-patterns/app-bars.md)) can be used to show both availability and state.\n\nProvide ongoing recognition feedback to minimize any apparent lack of response while recognition is being performed.\n\nLet users revise recognition text using keyboard input, disambiguation prompts, suggestions, or additional speech recognition.\n\nStop recognition if input is detected from a device other than speech recognition, such as touch or keyboard. This probably indicates that the user has moved onto another task, such as correcting the recognition text or interacting with other form fields.\n\nSpecify the length of time for which no speech input indicates that recognition is over. Do not automatically restart recognition after this period of time as it typically indicates the user has stopped engaging with your app.\n\nDisable all continuous recognition UI and terminate the recognition session if a network connection is not available. Continuous recogntion requires a network connection.\n\n## <span id=\"Commanding\"></span><span id=\"commanding\"></span><span id=\"COMMANDING\"></span>Commanding\n\n\nSpeech input can initiate actions, invoke commands, and accomplish tasks.\n\nIf space permits, consider displaying the supported responses for the current app context, with examples of valid input. This reduces the potential responses your app has to process and also eliminates confusion for the user.\n\nTry to frame your questions such that they elicit as specific a response as possible. For example, \"What do you want to do today?\" is very open ended and would require a very large grammar definition due to how varied the responses could be. Alternatively, \"Would you like to play a game or listen to music?\" constrains the response to one of two valid answers with a correspondingly small grammar definition. A small grammar is much easier to author and results in much more accurate recognition results.\n\nRequest confirmation from the user when speech recognition confidence is low. If the user's intent is unclear, it's better to get clarification than to initiate an unintended action.\n\nYou should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on. For example, a command bar button with a microphone glyph (see [Guidelines for command bars](../controls-and-patterns/app-bars.md)) can be used to show both availability and state.\n\nIf the speech recognition switch is typically out of view, consider displaying a state indicator in the content area of the app.\n\nIf recognition is initiated by the user, consider using the built-in recognition experience for consistency. The built-in experience includes customizable screens with prompts, examples, disambiguations, confirmations, and errors.\n\nThe screens vary depending on the specified constraints:\n\n-   Pre-defined grammar (dictation or web search)\n\n    -   The **Listening** screen.\n    -   The **Thinking** screen.\n    -   The **Heard you say** screen or the error screen.\n-   List of words or phrases, or a SRGS grammar file\n\n    -   The **Listening** screen.\n    -   The **Did you say** screen, if what the user said could be interpreted as more than one potential result.\n    -   The **Heard you say** screen or the error screen.\n\nOn the **Listening** screen you can:\n\n-   Customize the heading text.\n-   Provide example text of what the user can say.\n-   Specify whether the **Heard you say** screen is shown.\n-   Read the recognized string back to the user on the **Heard you say** screen.\n\nHere is an example of the built-in recognition flow for a speech recognizer that uses a SRGS-defined constraint. In this example, speech recognition is successful.\n\n![initial recognition screen for a constraint based on a sgrs grammar file](images/speech/speech-listening-initial.png)\n\n![intermediate recognition screen for a constraint based on a sgrs grammar file](images/speech/speech-listening-intermediate.png)\n\n![final recognition screen for a constraint based on a sgrs grammar file](images/speech/speech-listening-complete.png)\n\n## <span id=\"Always_listening\"></span><span id=\"always_listening\"></span><span id=\"ALWAYS_LISTENING\"></span>Always listening\n\n\nYour app can listen for and recognize speech input as soon as the app is launched, without user intervention.\n\nYou should customize the grammar constraints based on the app context. This keeps the speech recognition experience very targeted and relevant to the current task, and minimizes errors.\n\n## <span id=\"What_can_I_say_\"></span><span id=\"what_can_i_say_\"></span><span id=\"WHAT_CAN_I_SAY_\"></span>\"What can I say?\"\n\n\nWhen speech input is enabled, it's important to help users discover what exactly can be understood and what actions can be performed.\n\nIf speech recognition is user enabled, consider using the command bar or a menu command to show all words and phrases supported in the current context.\n\nIf speech recognition is always on, consider adding the phrase \"What can I say?\" to every page. When the user says this phrase, display all words and phrases supported in the current context. Using this phrase provides a consistent way for users to discover speech capabilities across the system.\n\n## <span id=\"Recognition_failures\"></span><span id=\"recognition_failures\"></span><span id=\"RECOGNITION_FAILURES\"></span>Recognition failures\n\n\nSpeech recognition will fail. Failures happen when audio quality is poor, when only part of a phrase is recognized, or when no input is detected at all.\n\nHandle failure gracefully, help a user understand why recognition failed, and recover.\n\nYour app should inform the user that they weren't understood and that they need to try again.\n\nConsider providing examples of one or more supported phrases. The user is likely to repeat a suggested phrase, which increases recognition success.\n\nYou should display a list of potential matches for a user to select from. This can be far more efficient than going through the recognition process again.\n\nYou should always support alternative input types, which is especially helpful for handling repeated recognition failures. For example, you could suggest that the user try to use a keyboard, or use touch or a mouse to select from a list of potential matches.\n\nUse the built-in speech recognition experience as it includes screens that inform the user that recognition was not successful and lets the user make another recognition attempt.\n\nListen for and try to correct issues in the audio input. The speech recognizer can detect issues with the audio quality that might adversely affect speech recognition accuracy. You can use the information provided by the speech recognizer to inform the user of the issue and let them take corrective action, if possible. For example, if the volume setting on the microphone is too low, you can prompt the user to speak louder or turn the volume up.\n\n## <span id=\"Constraints\"></span><span id=\"constraints\"></span><span id=\"CONSTRAINTS\"></span>Constraints\n\n\nConstraints, or grammars, define the spoken words and phrases that can be matched by the speech recognizer. You can specify one of the pre-defined web service grammars or you can create a custom grammar that is installed with your app.\n\n### <span id=\"Predefined_grammars\"></span><span id=\"predefined_grammars\"></span><span id=\"PREDEFINED_GRAMMARS\"></span>Predefined grammars\n\nPredefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar. When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device\n\n-   The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases. Free-text dictation is useful when you don't want to limit the kinds of things a user can say. Typical uses include creating notes or dictating the content for a message.\n-   The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say. However, it is optimized to recognize terms that people typically use when searching the web.\n\n**Note**  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.\n\n \n\nThese predefined grammars can be used to recognize up to 10 seconds of speech input and require no authoring effort on your part. However, they do require connection to a network.\n\n### <span id=\"Custom_grammars\"></span><span id=\"custom_grammars\"></span><span id=\"CUSTOM_GRAMMARS\"></span>Custom grammars\n\nA custom grammar is designed and authored by you and is installed with your app. Speech recognition using a custom constraint is performed on the device.\n\n-   Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases. A list constraint works well for recognizing short, distinct phrases. Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match. The list can also be programmatically updated.\n-   An SRGS grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the [SRGS Version 1.0](http://go.microsoft.com/fwlink/p/?LinkID=262302). An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.\n\n    Here are some tips for authoring SRGS grammars:\n\n    -   Keep each grammar small. Grammars that contain fewer phrases tend to provide more accurate recognition than larger grammars that contain many phrases. It's better to have several smaller grammars for specific scenarios than to have a single grammar for your entire app.\n    -   Let users know what to say for each app context and enable and disable grammars as needed.\n    -   Design each grammar so users can speak a command in a variety of ways. For example, you can use the **GARBAGE** rule to match speech input that your grammar does not define. This lets users speak additional words that have no meaning to your app. For example, \"give me\", \"and\", \"uh\", \"maybe\", and so on.\n    -   Use the [sapi:subset](http://msdn.microsoft.com/library/windowsphone/design/jj572474.aspx) element to help match speech input. This is a Microsoft extension to the SRGS specification to help match partial phrases.\n    -   Try to avoid defining phrases in your grammar that contain only one syllable. Recognition tends to be more accurate for phrases containing two or more syllables.\n    -   Avoid using phrases that sound similar. For example, phrases such as \"hello\", \"bellow\", and \"fellow\" can confuse the recognition engine and result in poor recognition accuracy.\n\n**Note**  Which type of constraint type you use depends on the complexity of the recognition experience you want to create. Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.\n\n \n\n### <span id=\"Custom_pronunciations\"></span><span id=\"custom_pronunciations\"></span><span id=\"CUSTOM_PRONUNCIATIONS\"></span>Custom pronunciations\n\nIf your app contains specialized vocabulary with unusual or fictional words, or words with uncommon pronunciations, you might be able to improve recognition performance for those words by defining custom pronunciations.\n\nFor a small list of words and phrases, or a list of infrequently used words and phrases, you can create custom pronunciations in a SRGS grammar. See [token Element](http://msdn.microsoft.com/library/windowsphone/design/hh361600.aspx) for more info.\n\nFor larger lists of words and phrases, or frequently used words and phrases, you can create separate pronunciation lexicon documents. See [About Lexicons and Phonetic Alphabets](http://msdn.microsoft.com/library/windowsphone/design/hh361646.aspx) for more info.\n\n## <span id=\"Testing\"></span><span id=\"testing\"></span><span id=\"TESTING\"></span>Testing\n\n\nTest speech recognition accuracy and any supporting UI with your app's target audience. This is the best way to determine the effectiveness of the speech interaction experience in your app. For example, are users getting poor recognition results because your app isn't listening for a common phrase?\n\nEither modify the grammar to support this phrase or provide users with a list of supported phrases. If you already provide the list of supported phrases, ensure it is easily discoverable.\n\n## <span id=\"Text-to-speech__TTS_\"></span><span id=\"text-to-speech__tts_\"></span><span id=\"TEXT-TO-SPEECH__TTS_\"></span>Text-to-speech (TTS)\n\n\nTTS generates speech output from plain text or SSML.\n\nTry to design prompts that are polite and encouraging.\n\nConsider whether you should read long strings of text. It's one thing to listen to a text message, but quite another to listen to a long list of search results that are difficult to remember.\n\nYou should provide media controls to let users pause, or stop, TTS.\n\nYou should listen to all TTS strings to ensure they are intelligible and sound natural.\n\n-   Stringing together an unusual sequence of words or speaking part numbers or punctuation might cause a phrase to become unintelligible.\n-   Speech can sound unnatural when the prosody or cadence is different from how a native speaker would say a phrase.\n\nBoth issues can be addressed bu using SSML instead of plain text as input to the speech synthesizer. For more info about SSML, see [Use SSML to Control Synthesized Speech](http://msdn.microsoft.com/library/windowsphone/design/hh378454.aspx) and [Speech Synthesis Markup Language Reference](http://msdn.microsoft.com/library/windowsphone/design/hh378377.aspx).\n\n## Other articles in this section \n<table>\n<colgroup>\n<col width=\"50%\" />\n<col width=\"50%\" />\n</colgroup>\n<thead>\n<tr class=\"header\">\n<th align=\"left\">Topic</th>\n<th align=\"left\">Description</th>\n</tr>\n</thead>\n<tbody>\n<tr class=\"odd\">\n<td align=\"left\"><p>[Speech recognition](speech-recognition.md)</p></td>\n<td align=\"left\"><p>Use speech recognition to provide input, specify an action or command, and accomplish tasks.</p></td>\n</tr>\n<tr class=\"even\">\n<td align=\"left\"><p>[Specify the speech recognizer language](specify-the-speech-recognizer-language.md)</p></td>\n<td align=\"left\"><p>Learn how to select an installed language to use for speech recognition.</p></td>\n</tr>\n<tr class=\"odd\">\n<td align=\"left\"><p>[Define custom recognition constraints](define-custom-recognition-constraints.md)</p></td>\n<td align=\"left\"><p>Learn how to define and use custom constraints for speech recognition.</p></td>\n</tr>\n<tr class=\"even\">\n<td align=\"left\"><p>[Enable continuous dictation](enable-continuous-dictation.md)</p></td>\n<td align=\"left\"><p>Learn how to capture and recognize long-form, continuous dictation speech input.</p></td>\n</tr>\n<tr class=\"odd\">\n<td align=\"left\"><p>[Manage issues with audio input](manage-issues-with-audio-input.md)</p></td>\n<td align=\"left\"><p>Learn how to manage issues with speech-recognition accuracy caused by audio-input quality.</p></td>\n</tr>\n<tr class=\"even\">\n<td align=\"left\"><p>[Set speech recognition timeouts](set-speech-recognition-timeouts.md)</p></td>\n<td align=\"left\"><p>Set how long a speech recognizer ignores silence or unrecognizable sounds (babble) and continues listening for speech input.</p></td>\n</tr>\n</tbody>\n</table>\n\n \n\n\n## <span id=\"related_topics\"></span>Related articles\n\n\n* [Speech interactions](https://msdn.microsoft.com/library/windows/apps/mt185614)\n* [Cortana interactions](https://msdn.microsoft.com/library/windows/apps/mt185598)\n **Samples**\n* [Speech recognition and speech synthesis sample](http://go.microsoft.com/fwlink/p/?LinkID=619897)\n \n\n \n\n\n\n"}
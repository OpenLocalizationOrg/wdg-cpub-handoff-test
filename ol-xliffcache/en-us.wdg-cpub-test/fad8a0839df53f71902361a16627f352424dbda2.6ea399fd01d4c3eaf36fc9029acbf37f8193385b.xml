{"nodes":[{"pos":[17,87],"content":"Learn how to define and use custom constraints for speech recognition.","needQuote":true,"needEscape":true,"nodes":[{"content":"Learn how to define and use custom constraints for speech recognition.","pos":[0,70]}]},{"pos":[95,132],"content":"Define custom recognition constraints","needQuote":true,"needEscape":true,"nodes":[{"content":"Define custom recognition constraints","pos":[0,37]}]},{"content":"Define custom recognition constraints","pos":[255,292]},{"content":"\\[ Updated for UWP apps on Windows 10.","pos":[295,333]},{"content":"For Windows 8.x articles, see the <bpt id=\"p1\">[</bpt>archive<ept id=\"p1\">](http://go.microsoft.com/fwlink/p/?linkid=619132)</ept> \\]","pos":[334,429],"source":" For Windows 8.x articles, see the [archive](http://go.microsoft.com/fwlink/p/?linkid=619132) \\]"},{"content":"Learn how to define and use custom constraints for speech recognition.","pos":[431,501]},{"content":"Important APIs","pos":[505,519]},{"content":"SpeechRecognitionTopicConstraint","pos":[530,562]},{"content":"SpeechRecognitionListConstraint","pos":[631,662]},{"content":"SpeechRecognitionGrammarFileConstraint","pos":[731,769]},{"content":"Speech recognition requires at least one constraint to define a recognizable vocabulary.","pos":[833,921]},{"content":"If no constraint is specified, the predefined dictation grammar of Universal Windows apps is used.","pos":[922,1020]},{"content":"See <bpt id=\"p1\">[</bpt>Speech recognition<ept id=\"p1\">](speech-recognition.md)</ept>.","pos":[1021,1069],"source":" See [Speech recognition](speech-recognition.md)."},{"pos":[1177,1192],"content":"Add constraints"},{"pos":[1195,1347],"content":"Use the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SpeechRecognizer.Constraints<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn653241)</ept> property to add constraints to a speech recognizer.","source":"Use the [**SpeechRecognizer.Constraints**](https://msdn.microsoft.com/library/windows/apps/dn653241) property to add constraints to a speech recognizer."},{"content":"Here, we cover the three kinds of speech recognition constraints used from within an app.","pos":[1349,1438]},{"content":"(For voice command constraints, see <bpt id=\"p1\">[</bpt>Launch a foreground app with voice commands in Cortana<ept id=\"p1\">](launch-a-foreground-app-with-voice-commands-in-cortana.md)</ept>.)","pos":[1439,1592],"source":" (For voice command constraints, see [Launch a foreground app with voice commands in Cortana](launch-a-foreground-app-with-voice-commands-in-cortana.md).)"},{"pos":[1598,1764],"content":"<bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SpeechRecognitionTopicConstraint<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn631446)</ept>—A constraint based on a predefined grammar (dictation or web search).","source":"[**SpeechRecognitionTopicConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631446)—A constraint based on a predefined grammar (dictation or web search)."},{"pos":[1769,1914],"content":"<bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SpeechRecognitionListConstraint<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn631421)</ept>—A constraint based on a list of words or phrases.","source":"[**SpeechRecognitionListConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631421)—A constraint based on a list of words or phrases."},{"pos":[1919,2101],"content":"<bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SpeechRecognitionGrammarFileConstraint<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn631412)</ept>—A constraint defined in a Speech Recognition Grammar Specification (SRGS) file.","source":"[**SpeechRecognitionGrammarFileConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631412)—A constraint defined in a Speech Recognition Grammar Specification (SRGS) file."},{"content":"Each speech recognizer can have one constraint collection.","pos":[2103,2161]},{"content":"Only these combinations of constraints are valid:","pos":[2162,2211]},{"content":"A single-topic constraint, or predefined grammar (dictation or web search).","pos":[2217,2292]},{"content":"No other constraints are allowed.","pos":[2293,2326]},{"content":"A combination of list constraints and/or grammar-file constraints.","pos":[2331,2397]},{"pos":[2399,2602],"content":"<bpt id=\"p1\">**</bpt>Remember:  <ept id=\"p1\">**</ept>Call the <bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>SpeechRecognizer.CompileConstraintsAsync<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/dn653240)</ept> method to compile the constraints before starting the recognition process.","source":"**Remember:  **Call the [**SpeechRecognizer.CompileConstraintsAsync**](https://msdn.microsoft.com/library/windows/apps/dn653240) method to compile the constraints before starting the recognition process."},{"pos":[2853,2916],"content":"Specify a web-search grammar (SpeechRecognitionTopicConstraint)"},{"content":"Topic constraints (dictation or web-search grammar) must be added to the constraints collection of a speech recognizer.","pos":[2919,3038]},{"content":"Here, we add a web-search grammar to the constraints collection.","pos":[3040,3104]},{"pos":[4689,4761],"content":"Specify a programmatic list constraint (SpeechRecognitionListConstraint)"},{"content":"List constraints must be added to the constraints collection of a speech recognizer.","pos":[4764,4848]},{"content":"Keep the following points in mind:","pos":[4850,4884]},{"content":"You can add multiple list constraints to a constraints collection.","pos":[4890,4956]},{"pos":[4961,5054],"content":"You can use any collection that implements <bpt id=\"p1\">**</bpt>IIterable<ph id=\"ph1\">&amp;lt;</ph>String<ph id=\"ph2\">&amp;gt;</ph><ept id=\"p1\">**</ept> for the string values.","source":"You can use any collection that implements **IIterable&lt;String&gt;** for the string values."},{"content":"Here, we programmatically specify an array of words as a list constraint and add it to the constraints collection of a speech recognizer.","pos":[5056,5193]},{"pos":[6528,6603],"content":"Specify an SRGS grammar constraint (SpeechRecognitionGrammarFileConstraint)"},{"content":"SRGS grammar files must be added to the constraints collection of a speech recognizer.","pos":[6606,6692]},{"content":"The SRGS Version 1.0 is the industry-standard markup language for creating XML-format grammars for speech recognition.","pos":[6694,6812]},{"content":"Although Universal Windows apps provide alternatives to using SRGS for creating speech-recognition grammars, you might find that using SRGS to create grammars produces the best results, particularly for more involved speech recognition scenarios.","pos":[6813,7059]},{"content":"SRGS grammars provide a full set of features to help you architect complex voice interaction for your apps.","pos":[7061,7168]},{"content":"For example, with SRGS grammars you can:","pos":[7169,7209]},{"content":"Specify the order in which words and phrases must be spoken to be recognized.","pos":[7215,7292]},{"content":"Combine words from multiple lists and phrases to be recognized.","pos":[7297,7360]},{"content":"Link to other grammars.","pos":[7365,7388]},{"content":"Assign a weight to an alternative word or phrase to increase or decrease the likelihood that it will be used to match speech input.","pos":[7393,7524]},{"content":"Include optional words or phrases.","pos":[7529,7563]},{"content":"Use special rules that help filter out unspecified or unanticipated input, such as random speech that doesn't match the grammar, or background noise.","pos":[7568,7717]},{"content":"Use semantics to define what speech recognition means to your app.","pos":[7722,7788]},{"content":"Specify pronunciations, either inline in a grammar or via a link to a lexicon.","pos":[7793,7871]},{"content":"For more info about SRGS elements and attributes, see the <bpt id=\"p1\">[</bpt>SRGS Grammar XML Reference<ept id=\"p1\">](http://go.microsoft.com/fwlink/p/?LinkID=269886)</ept> .","pos":[7873,8010],"source":"For more info about SRGS elements and attributes, see the [SRGS Grammar XML Reference](http://go.microsoft.com/fwlink/p/?LinkID=269886) ."},{"content":"To get started creating an SRGS grammar, see <bpt id=\"p1\">[</bpt>How to Create a Basic XML Grammar<ept id=\"p1\">](http://go.microsoft.com/fwlink/p/?LinkID=269887)</ept>.","pos":[8011,8141],"source":" To get started creating an SRGS grammar, see [How to Create a Basic XML Grammar](http://go.microsoft.com/fwlink/p/?LinkID=269887)."},{"content":"Keep the following points in mind:","pos":[8143,8177]},{"content":"You can add multiple grammar-file constraints to a constraints collection.","pos":[8183,8257]},{"content":"Use the .grxml file extension for XML-based grammar documents that conform to SRGS rules.","pos":[8262,8351]},{"content":"This example uses an SRGS grammar defined in a file named srgs.grxml (described later).","pos":[8353,8440]},{"content":"In the file properties, the <bpt id=\"p1\">**</bpt>Package Action<ept id=\"p1\">**</ept> is set to <bpt id=\"p2\">**</bpt>Content<ept id=\"p2\">**</ept> with <bpt id=\"p3\">**</bpt>Copy to Output Directory<ept id=\"p3\">**</ept> set to <bpt id=\"p4\">**</bpt>Copy always<ept id=\"p4\">**</ept>:","pos":[8441,8567],"source":" In the file properties, the **Package Action** is set to **Content** with **Copy to Output Directory** set to **Copy always**:"},{"content":"This SRGS file (srgs.grxml) includes semantic interpretation tags.","pos":[9698,9764]},{"content":"These tags provide a mechanism for returning grammar match data to your app.","pos":[9765,9841]},{"content":"Grammars must conform to the World Wide Web Consortium (W3C) <bpt id=\"p1\">[</bpt>Semantic Interpretation for Speech Recognition (SISR) 1.0<ept id=\"p1\">](http://go.microsoft.com/fwlink/p/?LinkID=201765)</ept> specification.","pos":[9842,10026],"source":" Grammars must conform to the World Wide Web Consortium (W3C) [Semantic Interpretation for Speech Recognition (SISR) 1.0](http://go.microsoft.com/fwlink/p/?LinkID=201765) specification."},{"content":"Here, we listen for variants of \"yes\" and \"no\".","pos":[10028,10075]},{"pos":[11120,11138],"content":"Manage constraints"},{"content":"After a constraint collection is loaded for recognition, your app can manage which constraints are enabled for recognition operations by setting the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>IsEnabled<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn631402)</ept> property of a constraint to <bpt id=\"p3\">**</bpt>true<ept id=\"p3\">**</ept> or <bpt id=\"p4\">**</bpt>false<ept id=\"p4\">**</ept>.","pos":[11141,11414],"source":"After a constraint collection is loaded for recognition, your app can manage which constraints are enabled for recognition operations by setting the [**IsEnabled**](https://msdn.microsoft.com/library/windows/apps/dn631402) property of a constraint to **true** or **false**."},{"content":"The default setting is <bpt id=\"p1\">**</bpt>true<ept id=\"p1\">**</ept>.","pos":[11415,11447],"source":" The default setting is **true**."},{"content":"It's usually more efficient to load constraints once, enabling and disabling them as needed, rather than to load, unload, and compile constraints for each recognition operation.","pos":[11449,11626]},{"content":"Use the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>IsEnabled<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn631402)</ept> property, as required.","pos":[11627,11731],"source":" Use the [**IsEnabled**](https://msdn.microsoft.com/library/windows/apps/dn631402) property, as required."},{"content":"Restricting the number of constraints serves to limit the amount of data that the speech recognizer needs to search and match against the speech input.","pos":[11733,11884]},{"content":"This can improve both the performance and the accuracy of speech recognition.","pos":[11885,11962]},{"content":"Decide which constraints are enabled based on the phrases that your app can expect in the context of the current recognition operation.","pos":[11964,12099]},{"content":"For example, if the current app context is to display a color, you probably don't need to enable a constraint that recognizes the names of animals.","pos":[12100,12247]},{"content":"To prompt the user for what can be spoken, use the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SpeechRecognizerUIOptions.AudiblePrompt<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn653235)</ept> and <bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>SpeechRecognizerUIOptions.ExampleText<ept id=\"p4\">**</ept><ept id=\"p3\">](https://msdn.microsoft.com/library/windows/apps/dn653236)</ept> properties, which are set by means of the <bpt id=\"p5\">[</bpt><bpt id=\"p6\">**</bpt>SpeechRecognizer.UIOptions<ept id=\"p6\">**</ept><ept id=\"p5\">](https://msdn.microsoft.com/library/windows/apps/dn653254)</ept> property.","pos":[12249,12652],"source":"To prompt the user for what can be spoken, use the [**SpeechRecognizerUIOptions.AudiblePrompt**](https://msdn.microsoft.com/library/windows/apps/dn653235) and [**SpeechRecognizerUIOptions.ExampleText**](https://msdn.microsoft.com/library/windows/apps/dn653236) properties, which are set by means of the [**SpeechRecognizer.UIOptions**](https://msdn.microsoft.com/library/windows/apps/dn653254) property."},{"content":"Preparing users for what they can say during the recognition operation increases the likelihood that they will speak a phrase that can be matched to an active constraint.","pos":[12653,12823]},{"pos":[12861,12877],"content":"Related articles"},{"content":"Speech interactions","pos":[12883,12902]},{"content":"Samples","pos":[12931,12938]},{"content":"Speech recognition and speech synthesis sample","pos":[12944,12990]}],"content":"---\nDescription: Learn how to define and use custom constraints for speech recognition.\ntitle: Define custom recognition constraints\nms.assetid: 26289DE5-6AC9-42C3-A160-E522AE62D2FC\nlabel: Define custom recognition constraints\ntemplate: detail.hbs\n---\n\n# Define custom recognition constraints\n\n\n\\[ Updated for UWP apps on Windows 10. For Windows 8.x articles, see the [archive](http://go.microsoft.com/fwlink/p/?linkid=619132) \\]\n\nLearn how to define and use custom constraints for speech recognition.\n\n**Important APIs**\n\n-   [**SpeechRecognitionTopicConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631446)\n-   [**SpeechRecognitionListConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631421)\n-   [**SpeechRecognitionGrammarFileConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631412)\n\n\nSpeech recognition requires at least one constraint to define a recognizable vocabulary. If no constraint is specified, the predefined dictation grammar of Universal Windows apps is used. See [Speech recognition](speech-recognition.md).\n\n\n## <span id=\"Add_constraints\"></span><span id=\"add_constraints\"></span><span id=\"ADD_CONSTRAINTS\"></span>Add constraints\n\n\nUse the [**SpeechRecognizer.Constraints**](https://msdn.microsoft.com/library/windows/apps/dn653241) property to add constraints to a speech recognizer.\n\nHere, we cover the three kinds of speech recognition constraints used from within an app. (For voice command constraints, see [Launch a foreground app with voice commands in Cortana](launch-a-foreground-app-with-voice-commands-in-cortana.md).)\n\n-   [**SpeechRecognitionTopicConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631446)—A constraint based on a predefined grammar (dictation or web search).\n-   [**SpeechRecognitionListConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631421)—A constraint based on a list of words or phrases.\n-   [**SpeechRecognitionGrammarFileConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631412)—A constraint defined in a Speech Recognition Grammar Specification (SRGS) file.\n\nEach speech recognizer can have one constraint collection. Only these combinations of constraints are valid:\n\n-   A single-topic constraint, or predefined grammar (dictation or web search). No other constraints are allowed.\n-   A combination of list constraints and/or grammar-file constraints.\n\n**Remember:  **Call the [**SpeechRecognizer.CompileConstraintsAsync**](https://msdn.microsoft.com/library/windows/apps/dn653240) method to compile the constraints before starting the recognition process.\n\n## <span id=\"Specify_a_web-search_grammar__SpeechRecognitionTopicConstraint_\"></span><span id=\"specify_a_web-search_grammar__speechrecognitiontopicconstraint_\"></span><span id=\"SPECIFY_A_WEB-SEARCH_GRAMMAR__SPEECHRECOGNITIONTOPICCONSTRAINT_\"></span>Specify a web-search grammar (SpeechRecognitionTopicConstraint)\n\n\nTopic constraints (dictation or web-search grammar) must be added to the constraints collection of a speech recognizer.\n\nHere, we add a web-search grammar to the constraints collection.\n\n```CSharp\nprivate async void WeatherSearch_Click(object sender, RoutedEventArgs e)\n{\n    // Create an instance of SpeechRecognizer.\n    var speechRecognizer = new Windows.Media.SpeechRecognition.SpeechRecognizer();\n\n    // Listen for audio input issues.\n    speechRecognizer.RecognitionQualityDegrading += speechRecognizer_RecognitionQualityDegrading;\n\n    // Add a web search grammar to the recognizer.\n    var webSearchGrammar = new Windows.Media.SpeechRecognition.SpeechRecognitionTopicConstraint(Windows.Media.SpeechRecognition.SpeechRecognitionScenario.WebSearch, \"webSearch\");\n\n\n    speechRecognizer.UIOptions.AudiblePrompt = \"Say what you want to search for...\";\n    speechRecognizer.UIOptions.ExampleText = @\"Ex. &#39;weather for London&#39;\";\n    speechRecognizer.Constraints.Add(webSearchGrammar);\n\n    // Compile the constraint.\n    await speechRecognizer.CompileConstraintsAsync();\n\n    // Start recognition.\n    Windows.Media.SpeechRecognition.SpeechRecognitionResult speechRecognitionResult = await speechRecognizer.RecognizeWithUIAsync();\n    //await speechRecognizer.RecognizeWithUIAsync();\n\n    // Do something with the recognition result.\n    var messageDialog = new Windows.UI.Popups.MessageDialog(speechRecognitionResult.Text, \"Text spoken\");\n    await messageDialog.ShowAsync();\n}\n```\n\n## <span id=\"Specify_a_programmatic_list_constraint__SpeechRecognitionListConstraint_\"></span><span id=\"specify_a_programmatic_list_constraint__speechrecognitionlistconstraint_\"></span><span id=\"SPECIFY_A_PROGRAMMATIC_LIST_CONSTRAINT__SPEECHRECOGNITIONLISTCONSTRAINT_\"></span>Specify a programmatic list constraint (SpeechRecognitionListConstraint)\n\n\nList constraints must be added to the constraints collection of a speech recognizer.\n\nKeep the following points in mind:\n\n-   You can add multiple list constraints to a constraints collection.\n-   You can use any collection that implements **IIterable&lt;String&gt;** for the string values.\n\nHere, we programmatically specify an array of words as a list constraint and add it to the constraints collection of a speech recognizer.\n\n```CSharp\nprivate async void YesOrNo_Click(object sender, RoutedEventArgs e)\n{\n    // Create an instance of SpeechRecognizer.\n    var speechRecognizer = new Windows.Media.SpeechRecognition.SpeechRecognizer();\n\n    // You could create this array dynamically.\n    string[] responses = { \"Yes\", \"No\" };\n\n\n    // Add a list constraint to the recognizer.\n    var listConstraint = new Windows.Media.SpeechRecognition.SpeechRecognitionListConstraint(responses, \"yesOrNo\");\n\n    speechRecognizer.UIOptions.ExampleText = @\"Ex. &#39;yes&#39;, &#39;no&#39;\";\n    speechRecognizer.Constraints.Add(listConstraint);\n\n    // Compile the constraint.\n    await speechRecognizer.CompileConstraintsAsync();\n\n    // Start recognition.\n    Windows.Media.SpeechRecognition.SpeechRecognitionResult speechRecognitionResult = await speechRecognizer.RecognizeWithUIAsync();\n\n    // Do something with the recognition result.\n    var messageDialog = new Windows.UI.Popups.MessageDialog(speechRecognitionResult.Text, \"Text spoken\");\n    await messageDialog.ShowAsync();\n}\n```\n\n## <span id=\"Specify_an_SRGS_grammar_constraint__SpeechRecognitionGrammarFileConstraint_\"></span><span id=\"specify_an_srgs_grammar_constraint__speechrecognitiongrammarfileconstraint_\"></span><span id=\"SPECIFY_AN_SRGS_GRAMMAR_CONSTRAINT__SPEECHRECOGNITIONGRAMMARFILECONSTRAINT_\"></span>Specify an SRGS grammar constraint (SpeechRecognitionGrammarFileConstraint)\n\n\nSRGS grammar files must be added to the constraints collection of a speech recognizer.\n\nThe SRGS Version 1.0 is the industry-standard markup language for creating XML-format grammars for speech recognition. Although Universal Windows apps provide alternatives to using SRGS for creating speech-recognition grammars, you might find that using SRGS to create grammars produces the best results, particularly for more involved speech recognition scenarios.\n\nSRGS grammars provide a full set of features to help you architect complex voice interaction for your apps. For example, with SRGS grammars you can:\n\n-   Specify the order in which words and phrases must be spoken to be recognized.\n-   Combine words from multiple lists and phrases to be recognized.\n-   Link to other grammars.\n-   Assign a weight to an alternative word or phrase to increase or decrease the likelihood that it will be used to match speech input.\n-   Include optional words or phrases.\n-   Use special rules that help filter out unspecified or unanticipated input, such as random speech that doesn't match the grammar, or background noise.\n-   Use semantics to define what speech recognition means to your app.\n-   Specify pronunciations, either inline in a grammar or via a link to a lexicon.\n\nFor more info about SRGS elements and attributes, see the [SRGS Grammar XML Reference](http://go.microsoft.com/fwlink/p/?LinkID=269886) . To get started creating an SRGS grammar, see [How to Create a Basic XML Grammar](http://go.microsoft.com/fwlink/p/?LinkID=269887).\n\nKeep the following points in mind:\n\n-   You can add multiple grammar-file constraints to a constraints collection.\n-   Use the .grxml file extension for XML-based grammar documents that conform to SRGS rules.\n\nThis example uses an SRGS grammar defined in a file named srgs.grxml (described later). In the file properties, the **Package Action** is set to **Content** with **Copy to Output Directory** set to **Copy always**:\n\n```CSharp\nprivate async void Colors_Click(object sender, RoutedEventArgs e)\n{\n    // Create an instance of SpeechRecognizer.\n    var speechRecognizer = new Windows.Media.SpeechRecognition.SpeechRecognizer();\n\n    // Add a grammar file constraint to the recognizer.\n    var storageFile = await Windows.Storage.StorageFile.GetFileFromApplicationUriAsync(new Uri(\"ms-appx:///Colors.grxml\"));\n    var grammarFileConstraint = new Windows.Media.SpeechRecognition.SpeechRecognitionGrammarFileConstraint(storageFile, \"colors\");\n\n    speechRecognizer.UIOptions.ExampleText = @\"Ex. &#39;blue background&#39;, &#39;green text&#39;\";\n    speechRecognizer.Constraints.Add(grammarFileConstraint);\n\n    // Compile the constraint.\n    await speechRecognizer.CompileConstraintsAsync();\n\n    // Start recognition.\n    Windows.Media.SpeechRecognition.SpeechRecognitionResult speechRecognitionResult = await speechRecognizer.RecognizeWithUIAsync();\n\n    // Do something with the recognition result.\n    var messageDialog = new Windows.UI.Popups.MessageDialog(speechRecognitionResult.Text, \"Text spoken\");\n    await messageDialog.ShowAsync();\n}\n```\n\nThis SRGS file (srgs.grxml) includes semantic interpretation tags. These tags provide a mechanism for returning grammar match data to your app. Grammars must conform to the World Wide Web Consortium (W3C) [Semantic Interpretation for Speech Recognition (SISR) 1.0](http://go.microsoft.com/fwlink/p/?LinkID=201765) specification.\n\nHere, we listen for variants of \"yes\" and \"no\".\n\n```CSharp\n<grammar xml:lang=\"en-US\" \n         root=\"yesOrNo\"\n         version=\"1.0\" \n         tag-format=\"semantics/1.0\"\n         xmlns=\"http://www.w3.org/2001/06/grammar\">\n\n    <!-- The following rules recognize variants of yes and no. -->\n      <rule id=\"yesOrNo\">\n         <one-of>\n            <item>\n              <one-of>\n                 <item>yes</item>\n                 <item>yeah</item>\n                 <item>yep</item>\n                 <item>yup</item>\n                 <item>un huh</item>\n                 <item>yay yus</item>\n              </one-of>\n              <tag>out=\"yes\";</tag>\n            </item>\n            <item>\n              <one-of>\n                 <item>no</item>\n                 <item>nope</item>\n                 <item>nah</item>\n                 <item>uh uh</item>\n               </one-of>\n               <tag>out=\"no\";</tag>\n            </item>\n         </one-of>\n      </rule>\n</grammar>\n```\n\n## <span id=\"Manage_constraints\"></span><span id=\"manage_constraints\"></span><span id=\"MANAGE_CONSTRAINTS\"></span>Manage constraints\n\n\nAfter a constraint collection is loaded for recognition, your app can manage which constraints are enabled for recognition operations by setting the [**IsEnabled**](https://msdn.microsoft.com/library/windows/apps/dn631402) property of a constraint to **true** or **false**. The default setting is **true**.\n\nIt's usually more efficient to load constraints once, enabling and disabling them as needed, rather than to load, unload, and compile constraints for each recognition operation. Use the [**IsEnabled**](https://msdn.microsoft.com/library/windows/apps/dn631402) property, as required.\n\nRestricting the number of constraints serves to limit the amount of data that the speech recognizer needs to search and match against the speech input. This can improve both the performance and the accuracy of speech recognition.\n\nDecide which constraints are enabled based on the phrases that your app can expect in the context of the current recognition operation. For example, if the current app context is to display a color, you probably don't need to enable a constraint that recognizes the names of animals.\n\nTo prompt the user for what can be spoken, use the [**SpeechRecognizerUIOptions.AudiblePrompt**](https://msdn.microsoft.com/library/windows/apps/dn653235) and [**SpeechRecognizerUIOptions.ExampleText**](https://msdn.microsoft.com/library/windows/apps/dn653236) properties, which are set by means of the [**SpeechRecognizer.UIOptions**](https://msdn.microsoft.com/library/windows/apps/dn653254) property. Preparing users for what they can say during the recognition operation increases the likelihood that they will speak a phrase that can be matched to an active constraint.\n\n## <span id=\"related_topics\"></span>Related articles\n\n\n* [Speech interactions](speech-interactions.md)\n\n**Samples**\n* [Speech recognition and speech synthesis sample](http://go.microsoft.com/fwlink/p/?LinkID=619897)\n \n\n \n\n\n\n\n"}
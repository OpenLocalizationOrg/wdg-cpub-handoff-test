<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns:mda="urn:oasis:names:tc:xliff:metadata:2.0" srcLang="en-US" trgLang="en-us" version="2.0" xml:space="preserve" xmlns="urn:oasis:names:tc:xliff:document:2.0">
	<file id="1">
		<mda:metadata>
			<mda:metaGroup>
				<mda:meta type="tool-id">mdxliff</mda:meta>
				<mda:meta type="tool-name">mdxliff</mda:meta>
				<mda:meta type="tool-version">1.0-c5d768e</mda:meta>
				<mda:meta type="tool-company">Microsoft</mda:meta>
			</mda:metaGroup>
		<mda:metaGroup><mda:meta type="olfilehash">fad8a0839df53f71902361a16627f352424dbda2</mda:meta><mda:meta type="olfilepath">wdg-cpub-test\ndolci2\input-and-devices\define-custom-recognition-constraints.md</mda:meta><mda:meta type="oltranslationpriority"></mda:meta><mda:meta type="oltranslationtype">Human Translation</mda:meta><mda:meta type="olskeletonhash">6ea399fd01d4c3eaf36fc9029acbf37f8193385b</mda:meta><mda:meta type="olxliffhash">99da5f91ea06305c6b258bc1629f704269b3ff82</mda:meta></mda:metaGroup></mda:metadata>
		<group id="content">
			<unit id="101" translate="yes">
				<segment state="initial">
					<source>Learn how to define and use custom constraints for speech recognition.</source>
					<target>Learn how to define and use custom constraints for speech recognition.</target>
				</segment>
			</unit>
			<unit id="102" translate="yes">
				<segment state="initial">
					<source>Define custom recognition constraints</source>
					<target>Define custom recognition constraints</target>
				</segment>
			</unit>
			<unit id="103" translate="yes">
				<segment state="initial">
					<source>Define custom recognition constraints</source>
					<target>Define custom recognition constraints</target>
				</segment>
			</unit>
			<unit id="104" translate="yes">
				<segment state="initial">
					<source>\[ Updated for UWP apps on Windows 10.</source>
					<target>\[ Updated for UWP apps on Windows 10.</target>
				</segment>
			</unit>
			<unit id="105" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?linkid=619132)</data>
				</originalData>
				<segment state="initial">
					<source>For Windows 8.x articles, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">archive</pc> \]</source>
					<target>For Windows 8.x articles, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">archive</pc> \]</target>
				</segment>
			</unit>
			<unit id="106" translate="yes">
				<segment state="initial">
					<source>Learn how to define and use custom constraints for speech recognition.</source>
					<target>Learn how to define and use custom constraints for speech recognition.</target>
				</segment>
			</unit>
			<unit id="107" translate="yes">
				<segment state="initial">
					<source>Important APIs</source>
					<target>Important APIs</target>
				</segment>
			</unit>
			<unit id="108" translate="yes">
				<segment state="initial">
					<source>SpeechRecognitionTopicConstraint</source>
					<target>SpeechRecognitionTopicConstraint</target>
				</segment>
			</unit>
			<unit id="109" translate="yes">
				<segment state="initial">
					<source>SpeechRecognitionListConstraint</source>
					<target>SpeechRecognitionListConstraint</target>
				</segment>
			</unit>
			<unit id="110" translate="yes">
				<segment state="initial">
					<source>SpeechRecognitionGrammarFileConstraint</source>
					<target>SpeechRecognitionGrammarFileConstraint</target>
				</segment>
			</unit>
			<unit id="111" translate="yes">
				<segment state="initial">
					<source>Speech recognition requires at least one constraint to define a recognizable vocabulary.</source>
					<target>Speech recognition requires at least one constraint to define a recognizable vocabulary.</target>
				</segment>
			</unit>
			<unit id="112" translate="yes">
				<segment state="initial">
					<source>If no constraint is specified, the predefined dictation grammar of Universal Windows apps is used.</source>
					<target>If no constraint is specified, the predefined dictation grammar of Universal Windows apps is used.</target>
				</segment>
			</unit>
			<unit id="113" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](speech-recognition.md)</data>
				</originalData>
				<segment state="initial">
					<source>See <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Speech recognition</pc>.</source>
					<target>See <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Speech recognition</pc>.</target>
				</segment>
			</unit>
			<unit id="114" translate="yes">
				<segment state="initial">
					<source>Add constraints</source>
					<target>Add constraints</target>
				</segment>
			</unit>
			<unit id="115" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn653241)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizer.Constraints</pc></pc> property to add constraints to a speech recognizer.</source>
					<target>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizer.Constraints</pc></pc> property to add constraints to a speech recognizer.</target>
				</segment>
			</unit>
			<unit id="116" translate="yes">
				<segment state="initial">
					<source>Here, we cover the three kinds of speech recognition constraints used from within an app.</source>
					<target>Here, we cover the three kinds of speech recognition constraints used from within an app.</target>
				</segment>
			</unit>
			<unit id="117" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](launch-a-foreground-app-with-voice-commands-in-cortana.md)</data>
				</originalData>
				<segment state="initial">
					<source>(For voice command constraints, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Launch a foreground app with voice commands in Cortana</pc>.)</source>
					<target>(For voice command constraints, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Launch a foreground app with voice commands in Cortana</pc>.)</target>
				</segment>
			</unit>
			<unit id="118" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn631446)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognitionTopicConstraint</pc></pc>—A constraint based on a predefined grammar (dictation or web search).</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognitionTopicConstraint</pc></pc>—A constraint based on a predefined grammar (dictation or web search).</target>
				</segment>
			</unit>
			<unit id="119" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn631421)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognitionListConstraint</pc></pc>—A constraint based on a list of words or phrases.</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognitionListConstraint</pc></pc>—A constraint based on a list of words or phrases.</target>
				</segment>
			</unit>
			<unit id="120" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn631412)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognitionGrammarFileConstraint</pc></pc>—A constraint defined in a Speech Recognition Grammar Specification (SRGS) file.</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognitionGrammarFileConstraint</pc></pc>—A constraint defined in a Speech Recognition Grammar Specification (SRGS) file.</target>
				</segment>
			</unit>
			<unit id="121" translate="yes">
				<segment state="initial">
					<source>Each speech recognizer can have one constraint collection.</source>
					<target>Each speech recognizer can have one constraint collection.</target>
				</segment>
			</unit>
			<unit id="122" translate="yes">
				<segment state="initial">
					<source>Only these combinations of constraints are valid:</source>
					<target>Only these combinations of constraints are valid:</target>
				</segment>
			</unit>
			<unit id="123" translate="yes">
				<segment state="initial">
					<source>A single-topic constraint, or predefined grammar (dictation or web search).</source>
					<target>A single-topic constraint, or predefined grammar (dictation or web search).</target>
				</segment>
			</unit>
			<unit id="124" translate="yes">
				<segment state="initial">
					<source>No other constraints are allowed.</source>
					<target>No other constraints are allowed.</target>
				</segment>
			</unit>
			<unit id="125" translate="yes">
				<segment state="initial">
					<source>A combination of list constraints and/or grammar-file constraints.</source>
					<target>A combination of list constraints and/or grammar-file constraints.</target>
				</segment>
			</unit>
			<unit id="126" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn653240)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Remember:  </pc>Call the <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognizer.CompileConstraintsAsync</pc></pc> method to compile the constraints before starting the recognition process.</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Remember:  </pc>Call the <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SpeechRecognizer.CompileConstraintsAsync</pc></pc> method to compile the constraints before starting the recognition process.</target>
				</segment>
			</unit>
			<unit id="127" translate="yes">
				<segment state="initial">
					<source>Specify a web-search grammar (SpeechRecognitionTopicConstraint)</source>
					<target>Specify a web-search grammar (SpeechRecognitionTopicConstraint)</target>
				</segment>
			</unit>
			<unit id="128" translate="yes">
				<segment state="initial">
					<source>Topic constraints (dictation or web-search grammar) must be added to the constraints collection of a speech recognizer.</source>
					<target>Topic constraints (dictation or web-search grammar) must be added to the constraints collection of a speech recognizer.</target>
				</segment>
			</unit>
			<unit id="129" translate="yes">
				<segment state="initial">
					<source>Here, we add a web-search grammar to the constraints collection.</source>
					<target>Here, we add a web-search grammar to the constraints collection.</target>
				</segment>
			</unit>
			<unit id="130" translate="yes">
				<segment state="initial">
					<source>Specify a programmatic list constraint (SpeechRecognitionListConstraint)</source>
					<target>Specify a programmatic list constraint (SpeechRecognitionListConstraint)</target>
				</segment>
			</unit>
			<unit id="131" translate="yes">
				<segment state="initial">
					<source>List constraints must be added to the constraints collection of a speech recognizer.</source>
					<target>List constraints must be added to the constraints collection of a speech recognizer.</target>
				</segment>
			</unit>
			<unit id="132" translate="yes">
				<segment state="initial">
					<source>Keep the following points in mind:</source>
					<target>Keep the following points in mind:</target>
				</segment>
			</unit>
			<unit id="133" translate="yes">
				<segment state="initial">
					<source>You can add multiple list constraints to a constraints collection.</source>
					<target>You can add multiple list constraints to a constraints collection.</target>
				</segment>
			</unit>
			<unit id="134" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">&amp;lt;</data>
					<data id="id4">&amp;gt;</data>
				</originalData>
				<segment state="initial">
					<source>You can use any collection that implements <pc dataRefEnd="id2" dataRefStart="id1" id="p1">IIterable<ph dataRef="id3" id="ph1" />String<ph dataRef="id4" id="ph2" /></pc> for the string values.</source>
					<target>You can use any collection that implements <pc dataRefEnd="id2" dataRefStart="id1" id="p1">IIterable<ph dataRef="id3" id="ph1" />String<ph dataRef="id4" id="ph2" /></pc> for the string values.</target>
				</segment>
			</unit>
			<unit id="135" translate="yes">
				<segment state="initial">
					<source>Here, we programmatically specify an array of words as a list constraint and add it to the constraints collection of a speech recognizer.</source>
					<target>Here, we programmatically specify an array of words as a list constraint and add it to the constraints collection of a speech recognizer.</target>
				</segment>
			</unit>
			<unit id="136" translate="yes">
				<segment state="initial">
					<source>Specify an SRGS grammar constraint (SpeechRecognitionGrammarFileConstraint)</source>
					<target>Specify an SRGS grammar constraint (SpeechRecognitionGrammarFileConstraint)</target>
				</segment>
			</unit>
			<unit id="137" translate="yes">
				<segment state="initial">
					<source>SRGS grammar files must be added to the constraints collection of a speech recognizer.</source>
					<target>SRGS grammar files must be added to the constraints collection of a speech recognizer.</target>
				</segment>
			</unit>
			<unit id="138" translate="yes">
				<segment state="initial">
					<source>The SRGS Version 1.0 is the industry-standard markup language for creating XML-format grammars for speech recognition.</source>
					<target>The SRGS Version 1.0 is the industry-standard markup language for creating XML-format grammars for speech recognition.</target>
				</segment>
			</unit>
			<unit id="139" translate="yes">
				<segment state="initial">
					<source>Although Universal Windows apps provide alternatives to using SRGS for creating speech-recognition grammars, you might find that using SRGS to create grammars produces the best results, particularly for more involved speech recognition scenarios.</source>
					<target>Although Universal Windows apps provide alternatives to using SRGS for creating speech-recognition grammars, you might find that using SRGS to create grammars produces the best results, particularly for more involved speech recognition scenarios.</target>
				</segment>
			</unit>
			<unit id="140" translate="yes">
				<segment state="initial">
					<source>SRGS grammars provide a full set of features to help you architect complex voice interaction for your apps.</source>
					<target>SRGS grammars provide a full set of features to help you architect complex voice interaction for your apps.</target>
				</segment>
			</unit>
			<unit id="141" translate="yes">
				<segment state="initial">
					<source>For example, with SRGS grammars you can:</source>
					<target>For example, with SRGS grammars you can:</target>
				</segment>
			</unit>
			<unit id="142" translate="yes">
				<segment state="initial">
					<source>Specify the order in which words and phrases must be spoken to be recognized.</source>
					<target>Specify the order in which words and phrases must be spoken to be recognized.</target>
				</segment>
			</unit>
			<unit id="143" translate="yes">
				<segment state="initial">
					<source>Combine words from multiple lists and phrases to be recognized.</source>
					<target>Combine words from multiple lists and phrases to be recognized.</target>
				</segment>
			</unit>
			<unit id="144" translate="yes">
				<segment state="initial">
					<source>Link to other grammars.</source>
					<target>Link to other grammars.</target>
				</segment>
			</unit>
			<unit id="145" translate="yes">
				<segment state="initial">
					<source>Assign a weight to an alternative word or phrase to increase or decrease the likelihood that it will be used to match speech input.</source>
					<target>Assign a weight to an alternative word or phrase to increase or decrease the likelihood that it will be used to match speech input.</target>
				</segment>
			</unit>
			<unit id="146" translate="yes">
				<segment state="initial">
					<source>Include optional words or phrases.</source>
					<target>Include optional words or phrases.</target>
				</segment>
			</unit>
			<unit id="147" translate="yes">
				<segment state="initial">
					<source>Use special rules that help filter out unspecified or unanticipated input, such as random speech that doesn't match the grammar, or background noise.</source>
					<target>Use special rules that help filter out unspecified or unanticipated input, such as random speech that doesn't match the grammar, or background noise.</target>
				</segment>
			</unit>
			<unit id="148" translate="yes">
				<segment state="initial">
					<source>Use semantics to define what speech recognition means to your app.</source>
					<target>Use semantics to define what speech recognition means to your app.</target>
				</segment>
			</unit>
			<unit id="149" translate="yes">
				<segment state="initial">
					<source>Specify pronunciations, either inline in a grammar or via a link to a lexicon.</source>
					<target>Specify pronunciations, either inline in a grammar or via a link to a lexicon.</target>
				</segment>
			</unit>
			<unit id="150" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?LinkID=269886)</data>
				</originalData>
				<segment state="initial">
					<source>For more info about SRGS elements and attributes, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SRGS Grammar XML Reference</pc> .</source>
					<target>For more info about SRGS elements and attributes, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SRGS Grammar XML Reference</pc> .</target>
				</segment>
			</unit>
			<unit id="151" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?LinkID=269887)</data>
				</originalData>
				<segment state="initial">
					<source>To get started creating an SRGS grammar, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">How to Create a Basic XML Grammar</pc>.</source>
					<target>To get started creating an SRGS grammar, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">How to Create a Basic XML Grammar</pc>.</target>
				</segment>
			</unit>
			<unit id="152" translate="yes">
				<segment state="initial">
					<source>Keep the following points in mind:</source>
					<target>Keep the following points in mind:</target>
				</segment>
			</unit>
			<unit id="153" translate="yes">
				<segment state="initial">
					<source>You can add multiple grammar-file constraints to a constraints collection.</source>
					<target>You can add multiple grammar-file constraints to a constraints collection.</target>
				</segment>
			</unit>
			<unit id="154" translate="yes">
				<segment state="initial">
					<source>Use the .grxml file extension for XML-based grammar documents that conform to SRGS rules.</source>
					<target>Use the .grxml file extension for XML-based grammar documents that conform to SRGS rules.</target>
				</segment>
			</unit>
			<unit id="155" translate="yes">
				<segment state="initial">
					<source>This example uses an SRGS grammar defined in a file named srgs.grxml (described later).</source>
					<target>This example uses an SRGS grammar defined in a file named srgs.grxml (described later).</target>
				</segment>
			</unit>
			<unit id="156" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>In the file properties, the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Package Action</pc> is set to <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Content</pc> with <pc dataRefEnd="id6" dataRefStart="id5" id="p3">Copy to Output Directory</pc> set to <pc dataRefEnd="id8" dataRefStart="id7" id="p4">Copy always</pc>:</source>
					<target>In the file properties, the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Package Action</pc> is set to <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Content</pc> with <pc dataRefEnd="id6" dataRefStart="id5" id="p3">Copy to Output Directory</pc> set to <pc dataRefEnd="id8" dataRefStart="id7" id="p4">Copy always</pc>:</target>
				</segment>
			</unit>
			<unit id="157" translate="yes">
				<segment state="initial">
					<source>This SRGS file (srgs.grxml) includes semantic interpretation tags.</source>
					<target>This SRGS file (srgs.grxml) includes semantic interpretation tags.</target>
				</segment>
			</unit>
			<unit id="158" translate="yes">
				<segment state="initial">
					<source>These tags provide a mechanism for returning grammar match data to your app.</source>
					<target>These tags provide a mechanism for returning grammar match data to your app.</target>
				</segment>
			</unit>
			<unit id="159" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?LinkID=201765)</data>
				</originalData>
				<segment state="initial">
					<source>Grammars must conform to the World Wide Web Consortium (W3C) <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Semantic Interpretation for Speech Recognition (SISR) 1.0</pc> specification.</source>
					<target>Grammars must conform to the World Wide Web Consortium (W3C) <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Semantic Interpretation for Speech Recognition (SISR) 1.0</pc> specification.</target>
				</segment>
			</unit>
			<unit id="160" translate="yes">
				<segment state="initial">
					<source>Here, we listen for variants of "yes" and "no".</source>
					<target>Here, we listen for variants of "yes" and "no".</target>
				</segment>
			</unit>
			<unit id="161" translate="yes">
				<segment state="initial">
					<source>Manage constraints</source>
					<target>Manage constraints</target>
				</segment>
			</unit>
			<unit id="162" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn631402)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>After a constraint collection is loaded for recognition, your app can manage which constraints are enabled for recognition operations by setting the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">IsEnabled</pc></pc> property of a constraint to <pc dataRefEnd="id6" dataRefStart="id5" id="p3">true</pc> or <pc dataRefEnd="id8" dataRefStart="id7" id="p4">false</pc>.</source>
					<target>After a constraint collection is loaded for recognition, your app can manage which constraints are enabled for recognition operations by setting the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">IsEnabled</pc></pc> property of a constraint to <pc dataRefEnd="id6" dataRefStart="id5" id="p3">true</pc> or <pc dataRefEnd="id8" dataRefStart="id7" id="p4">false</pc>.</target>
				</segment>
			</unit>
			<unit id="163" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The default setting is <pc dataRefEnd="id2" dataRefStart="id1" id="p1">true</pc>.</source>
					<target>The default setting is <pc dataRefEnd="id2" dataRefStart="id1" id="p1">true</pc>.</target>
				</segment>
			</unit>
			<unit id="164" translate="yes">
				<segment state="initial">
					<source>It's usually more efficient to load constraints once, enabling and disabling them as needed, rather than to load, unload, and compile constraints for each recognition operation.</source>
					<target>It's usually more efficient to load constraints once, enabling and disabling them as needed, rather than to load, unload, and compile constraints for each recognition operation.</target>
				</segment>
			</unit>
			<unit id="165" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn631402)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">IsEnabled</pc></pc> property, as required.</source>
					<target>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">IsEnabled</pc></pc> property, as required.</target>
				</segment>
			</unit>
			<unit id="166" translate="yes">
				<segment state="initial">
					<source>Restricting the number of constraints serves to limit the amount of data that the speech recognizer needs to search and match against the speech input.</source>
					<target>Restricting the number of constraints serves to limit the amount of data that the speech recognizer needs to search and match against the speech input.</target>
				</segment>
			</unit>
			<unit id="167" translate="yes">
				<segment state="initial">
					<source>This can improve both the performance and the accuracy of speech recognition.</source>
					<target>This can improve both the performance and the accuracy of speech recognition.</target>
				</segment>
			</unit>
			<unit id="168" translate="yes">
				<segment state="initial">
					<source>Decide which constraints are enabled based on the phrases that your app can expect in the context of the current recognition operation.</source>
					<target>Decide which constraints are enabled based on the phrases that your app can expect in the context of the current recognition operation.</target>
				</segment>
			</unit>
			<unit id="169" translate="yes">
				<segment state="initial">
					<source>For example, if the current app context is to display a color, you probably don't need to enable a constraint that recognizes the names of animals.</source>
					<target>For example, if the current app context is to display a color, you probably don't need to enable a constraint that recognizes the names of animals.</target>
				</segment>
			</unit>
			<unit id="170" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn653235)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn653236)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
					<data id="id9">[</data>
					<data id="id10">](https://msdn.microsoft.com/library/windows/apps/dn653254)</data>
					<data id="id11">**</data>
					<data id="id12">**</data>
				</originalData>
				<segment state="initial">
					<source>To prompt the user for what can be spoken, use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizerUIOptions.AudiblePrompt</pc></pc> and <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">SpeechRecognizerUIOptions.ExampleText</pc></pc> properties, which are set by means of the <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">SpeechRecognizer.UIOptions</pc></pc> property.</source>
					<target>To prompt the user for what can be spoken, use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SpeechRecognizerUIOptions.AudiblePrompt</pc></pc> and <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">SpeechRecognizerUIOptions.ExampleText</pc></pc> properties, which are set by means of the <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">SpeechRecognizer.UIOptions</pc></pc> property.</target>
				</segment>
			</unit>
			<unit id="171" translate="yes">
				<segment state="initial">
					<source>Preparing users for what they can say during the recognition operation increases the likelihood that they will speak a phrase that can be matched to an active constraint.</source>
					<target>Preparing users for what they can say during the recognition operation increases the likelihood that they will speak a phrase that can be matched to an active constraint.</target>
				</segment>
			</unit>
			<unit id="172" translate="yes">
				<segment state="initial">
					<source>Related articles</source>
					<target>Related articles</target>
				</segment>
			</unit>
			<unit id="173" translate="yes">
				<segment state="initial">
					<source>Speech interactions</source>
					<target>Speech interactions</target>
				</segment>
			</unit>
			<unit id="174" translate="yes">
				<segment state="initial">
					<source>Samples</source>
					<target>Samples</target>
				</segment>
			</unit>
			<unit id="175" translate="yes">
				<segment state="initial">
					<source>Speech recognition and speech synthesis sample</source>
					<target>Speech recognition and speech synthesis sample</target>
				</segment>
			</unit>
		</group>
	</file>
</xliff>
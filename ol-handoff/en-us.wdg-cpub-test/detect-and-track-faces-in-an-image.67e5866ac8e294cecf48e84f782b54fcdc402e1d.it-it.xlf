<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns:mda="urn:oasis:names:tc:xliff:metadata:2.0" srcLang="en-US" trgLang="it-it" version="2.0" xml:space="preserve" xmlns="urn:oasis:names:tc:xliff:document:2.0">
	<file id="1">
		<mda:metadata>
			<mda:metaGroup>
				<mda:meta type="tool-id">mdxliff</mda:meta>
				<mda:meta type="tool-name">mdxliff</mda:meta>
				<mda:meta type="tool-version">1.0-2eb3c86</mda:meta>
				<mda:meta type="tool-company">Microsoft</mda:meta>
			</mda:metaGroup>
		<mda:metaGroup><mda:meta type="olfilehash">df58c3033a77f6763ef86f7d3ea13320ac805335</mda:meta><mda:meta type="olfilepath">wdg-cpub-test\ndolci1\audio-video-camera\detect-and-track-faces-in-an-image.md</mda:meta><mda:meta type="oltranslationpriority"></mda:meta><mda:meta type="oltranslationtype">Human Translation</mda:meta><mda:meta type="olskeletonhash">24a8281282a148066db6463f09689829c8e84042</mda:meta><mda:meta type="olxliffhash">97e62fb1dc9ad646c6d9b24f9ed103e0b27f0e8f</mda:meta></mda:metaGroup></mda:metadata>
		<group id="content">
			<unit id="101" translate="yes">
				<segment state="initial">
					<source>This topic shows how to use the FaceDetector to detect faces in an image.</source>
					<target>This topic shows how to use the FaceDetector to detect faces in an image.</target>
				</segment>
			</unit>
			<unit id="102" translate="yes">
				<segment state="initial">
					<source>The FaceTracker is optimized for tracking faces over time in a sequence of video frames.</source>
					<target>The FaceTracker is optimized for tracking faces over time in a sequence of video frames.</target>
				</segment>
			</unit>
			<unit id="103" translate="yes">
				<segment state="initial">
					<source>Detect faces in images or videos</source>
					<target>Detect faces in images or videos</target>
				</segment>
			</unit>
			<unit id="104" translate="yes">
				<segment state="initial">
					<source>Detect faces in images or videos</source>
					<target>Detect faces in images or videos</target>
				</segment>
			</unit>
			<unit id="105" translate="yes">
				<segment state="initial">
					<source>\[ Updated for UWP apps on Windows 10.</source>
					<target>\[ Updated for UWP apps on Windows 10.</target>
				</segment>
			</unit>
			<unit id="106" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?linkid=619132)</data>
				</originalData>
				<segment state="initial">
					<source>For Windows 8.x articles, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">archive</pc> \]</source>
					<target>For Windows 8.x articles, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">archive</pc> \]</target>
				</segment>
			</unit>
			<unit id="107" translate="yes">
				<segment state="initial">
					<source>\[Some information relates to pre-released product which may be substantially modified before it's commercially released.</source>
					<target>\[Some information relates to pre-released product which may be substantially modified before it's commercially released.</target>
				</segment>
			</unit>
			<unit id="108" translate="yes">
				<segment state="initial">
					<source>Microsoft makes no warranties, express or implied, with respect to the information provided here.\]</source>
					<target>Microsoft makes no warranties, express or implied, with respect to the information provided here.\]</target>
				</segment>
			</unit>
			<unit id="109" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974129)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>This topic shows how to use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetector</pc></pc> to detect faces in an image.</source>
					<target>This topic shows how to use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetector</pc></pc> to detect faces in an image.</target>
				</segment>
			</unit>
			<unit id="110" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974150)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceTracker</pc></pc> is optimized for tracking faces over time in a sequence of video frames.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceTracker</pc></pc> is optimized for tracking faces over time in a sequence of video frames.</target>
				</segment>
			</unit>
			<unit id="111" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948776)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](scene-analysis-for-media-capture.md)</data>
				</originalData>
				<segment state="initial">
					<source>For an alternative method of tracking faces using the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect</pc></pc>, see <pc dataRefEnd="id6" dataRefStart="id5" id="p3">Scene analysis for media capture</pc>.</source>
					<target>For an alternative method of tracking faces using the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect</pc></pc>, see <pc dataRefEnd="id6" dataRefStart="id5" id="p3">Scene analysis for media capture</pc>.</target>
				</segment>
			</unit>
			<unit id="112" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?LinkId=620512&amp;clcid=0x409)</data>
					<data id="id3">[</data>
					<data id="id4">](http://go.microsoft.com/fwlink/p/?LinkId=620513&amp;clcid=0x409)</data>
				</originalData>
				<segment state="initial">
					<source>The code in this article was adapted from the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Basic Face Detection</pc> and <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Basic Face Tracking</pc> samples.</source>
					<target>The code in this article was adapted from the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Basic Face Detection</pc> and <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Basic Face Tracking</pc> samples.</target>
				</segment>
			</unit>
			<unit id="113" translate="yes">
				<segment state="initial">
					<source>You can download these samples to see the code used in context or to use the sample as a starting point for your own app.</source>
					<target>You can download these samples to see the code used in context or to use the sample as a starting point for your own app.</target>
				</segment>
			</unit>
			<unit id="114" translate="yes">
				<segment state="initial">
					<source>Detect faces in a single image</source>
					<target>Detect faces in a single image</target>
				</segment>
			</unit>
			<unit id="115" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974129)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetector</pc></pc> class allows you to detect one or more faces in a still image.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetector</pc></pc> class allows you to detect one or more faces in a still image.</target>
				</segment>
			</unit>
			<unit id="116" translate="yes">
				<segment state="initial">
					<source>This example uses APIs from the following namespaces.</source>
					<target>This example uses APIs from the following namespaces.</target>
				</segment>
			</unit>
			<unit id="117" translate="yes">
				<segment state="initial">
					<source>FaceDetectionUsing</source>
					<target>FaceDetectionUsing</target>
				</segment>
			</unit>
			<unit id="118" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974129)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn974123)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>Declare a class member variable for the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetector</pc></pc> object and for the list of <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">DetectedFace</pc></pc> objects that will be detected in the image.</source>
					<target>Declare a class member variable for the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetector</pc></pc> object and for the list of <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">DetectedFace</pc></pc> objects that will be detected in the image.</target>
				</segment>
			</unit>
			<unit id="119" translate="yes">
				<segment state="initial">
					<source>ClassVariables1</source>
					<target>ClassVariables1</target>
				</segment>
			</unit>
			<unit id="120" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn887358)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Face detection operates on a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SoftwareBitmap</pc></pc> object which can be created in a variety of ways.</source>
					<target>Face detection operates on a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SoftwareBitmap</pc></pc> object which can be created in a variety of ways.</target>
				</segment>
			</unit>
			<unit id="121" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/br207847)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>In this example a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FileOpenPicker</pc></pc> is used to allow the user to pick an image file in which faces will be detected.</source>
					<target>In this example a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FileOpenPicker</pc></pc> is used to allow the user to pick an image file in which faces will be detected.</target>
				</segment>
			</unit>
			<unit id="122" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](imaging.md)</data>
				</originalData>
				<segment state="initial">
					<source>For more information on working with software bitmaps, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Imaging</pc>.</source>
					<target>For more information on working with software bitmaps, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Imaging</pc>.</target>
				</segment>
			</unit>
			<unit id="123" translate="yes">
				<segment state="initial">
					<source>Picker</source>
					<target>Picker</target>
				</segment>
			</unit>
			<unit id="124" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/br226176)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">BitmapDecoder</pc></pc> class to decode the image file into a <pc dataRefEnd="id6" dataRefStart="id5" id="p3">SoftwareBitmap</pc>.</source>
					<target>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">BitmapDecoder</pc></pc> class to decode the image file into a <pc dataRefEnd="id6" dataRefStart="id5" id="p3">SoftwareBitmap</pc>.</target>
				</segment>
			</unit>
			<unit id="125" translate="yes">
				<segment state="initial">
					<source>The face detection process is quicker with a smaller image and so you may want to scale the source image down to a smaller size.</source>
					<target>The face detection process is quicker with a smaller image and so you may want to scale the source image down to a smaller size.</target>
				</segment>
			</unit>
			<unit id="126" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/br226254)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/br226261)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
					<data id="id9">[</data>
					<data id="id10">](https://msdn.microsoft.com/library/windows/apps/br226260)</data>
					<data id="id11">**</data>
					<data id="id12">**</data>
					<data id="id13">[</data>
					<data id="id14">](https://msdn.microsoft.com/library/windows/apps/dn887332)</data>
					<data id="id15">**</data>
					<data id="id16">**</data>
					<data id="id17">**</data>
					<data id="id18">**</data>
				</originalData>
				<segment state="initial">
					<source>This can be performed during decoding by creating a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">BitmapTransform</pc></pc> object, setting the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">ScaledWidth</pc></pc> and <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">ScaledHeight</pc></pc> properties and passing it into the call to <pc dataRefEnd="id14" dataRefStart="id13" id="p7"><pc dataRefEnd="id16" dataRefStart="id15" id="p8">GetSoftwareBitmapAsync</pc></pc>, which returns the decoded and scaled <pc dataRefEnd="id18" dataRefStart="id17" id="p9">SoftwareBitmap</pc>.</source>
					<target>This can be performed during decoding by creating a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">BitmapTransform</pc></pc> object, setting the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">ScaledWidth</pc></pc> and <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">ScaledHeight</pc></pc> properties and passing it into the call to <pc dataRefEnd="id14" dataRefStart="id13" id="p7"><pc dataRefEnd="id16" dataRefStart="id15" id="p8">GetSoftwareBitmapAsync</pc></pc>, which returns the decoded and scaled <pc dataRefEnd="id18" dataRefStart="id17" id="p9">SoftwareBitmap</pc>.</target>
				</segment>
			</unit>
			<unit id="127" translate="yes">
				<segment state="initial">
					<source>Decode</source>
					<target>Decode</target>
				</segment>
			</unit>
			<unit id="128" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>In the current version, the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceDetector</pc> class only supports images in Gray8 or Nv12.</source>
					<target>In the current version, the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceDetector</pc> class only supports images in Gray8 or Nv12.</target>
				</segment>
			</unit>
			<unit id="129" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn887362)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SoftwareBitmap</pc> class provides the <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">Convert</pc></pc> method, which converts a bitmap from one format to another.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SoftwareBitmap</pc> class provides the <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">Convert</pc></pc> method, which converts a bitmap from one format to another.</target>
				</segment>
			</unit>
			<unit id="130" translate="yes">
				<segment state="initial">
					<source>This example converts the source image into the Gray8 pixel format if it is not already in that format.</source>
					<target>This example converts the source image into the Gray8 pixel format if it is not already in that format.</target>
				</segment>
			</unit>
			<unit id="131" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974140)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn974142)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>If you want, you can use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">GetSupportedBitmapPixelFormats</pc></pc> and <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">IsBitmapPixelFormatSupported</pc></pc> methods to determine at runtime if a pixel format is supported, in case the set of supported formats is expanded in future versions.</source>
					<target>If you want, you can use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">GetSupportedBitmapPixelFormats</pc></pc> and <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">IsBitmapPixelFormatSupported</pc></pc> methods to determine at runtime if a pixel format is supported, in case the set of supported formats is expanded in future versions.</target>
				</segment>
			</unit>
			<unit id="132" translate="yes">
				<segment state="initial">
					<source>Format</source>
					<target>Format</target>
				</segment>
			</unit>
			<unit id="133" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn974132)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
					<data id="id7">[</data>
					<data id="id8">](https://msdn.microsoft.com/library/windows/apps/dn974134)</data>
					<data id="id9">**</data>
					<data id="id10">**</data>
				</originalData>
				<segment state="initial">
					<source>Instantiate the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceDetector</pc> object by calling <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">CreateAsync</pc></pc> and then call <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">DetectFacesAsync</pc></pc>, passing in the bitmap that has been scaled to a reasonable size and converted to a supported pixel format.</source>
					<target>Instantiate the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceDetector</pc> object by calling <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">CreateAsync</pc></pc> and then call <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">DetectFacesAsync</pc></pc>, passing in the bitmap that has been scaled to a reasonable size and converted to a supported pixel format.</target>
				</segment>
			</unit>
			<unit id="134" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974123)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>This method returns a list of <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">DetectedFace</pc></pc> objects.</source>
					<target>This method returns a list of <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">DetectedFace</pc></pc> objects.</target>
				</segment>
			</unit>
			<unit id="135" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">ShowDetectedFaces</pc> is a helper method, shown below, that draws squares around the faces in the image.</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">ShowDetectedFaces</pc> is a helper method, shown below, that draws squares around the faces in the image.</target>
				</segment>
			</unit>
			<unit id="136" translate="yes">
				<segment state="initial">
					<source>Detect</source>
					<target>Detect</target>
				</segment>
			</unit>
			<unit id="137" translate="yes">
				<segment state="initial">
					<source>Be sure to dispose of the objects that were created during the face detection process.</source>
					<target>Be sure to dispose of the objects that were created during the face detection process.</target>
				</segment>
			</unit>
			<unit id="138" translate="yes">
				<segment state="initial">
					<source>Dispose</source>
					<target>Dispose</target>
				</segment>
			</unit>
			<unit id="139" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/br209267)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>To display the image and draw boxes around the detected faces, add a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">Canvas</pc></pc> element to your XAML page.</source>
					<target>To display the image and draw boxes around the detected faces, add a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">Canvas</pc></pc> element to your XAML page.</target>
				</segment>
			</unit>
			<unit id="140" translate="yes">
				<segment state="initial">
					<source>Canvas</source>
					<target>Canvas</target>
				</segment>
			</unit>
			<unit id="141" translate="yes">
				<segment state="initial">
					<source>Define some member variables to style the squares that will be drawn.</source>
					<target>Define some member variables to style the squares that will be drawn.</target>
				</segment>
			</unit>
			<unit id="142" translate="yes">
				<segment state="initial">
					<source>ClassVariables2</source>
					<target>ClassVariables2</target>
				</segment>
			</unit>
			<unit id="143" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/br210101)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
					<data id="id7">[</data>
					<data id="id8">](https://msdn.microsoft.com/library/windows/apps/dn997854)</data>
					<data id="id9">**</data>
					<data id="id10">**</data>
					<data id="id11">**</data>
					<data id="id12">**</data>
				</originalData>
				<segment state="initial">
					<source>In the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">ShowDetectedFaces</pc> helper method, a new <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">ImageBrush</pc></pc> is created and the source is set to a <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">SoftwareBitmapSource</pc></pc> created from the <pc dataRefEnd="id12" dataRefStart="id11" id="p6">SoftwareBitmap</pc> representing the source image.</source>
					<target>In the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">ShowDetectedFaces</pc> helper method, a new <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">ImageBrush</pc></pc> is created and the source is set to a <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">SoftwareBitmapSource</pc></pc> created from the <pc dataRefEnd="id12" dataRefStart="id11" id="p6">SoftwareBitmap</pc> representing the source image.</target>
				</segment>
			</unit>
			<unit id="144" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The background of the XAML <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Canvas</pc> control is set to the image brush.</source>
					<target>The background of the XAML <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Canvas</pc> control is set to the image brush.</target>
				</segment>
			</unit>
			<unit id="145" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974126)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn974123)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>If the list of faces passed into the helper method isn't empty, loop through each face in the list and use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceBox</pc></pc> property of the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">DetectedFace</pc></pc> class to determine the position and size of the rectangle within the image that contains the face.</source>
					<target>If the list of faces passed into the helper method isn't empty, loop through each face in the list and use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceBox</pc></pc> property of the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">DetectedFace</pc></pc> class to determine the position and size of the rectangle within the image that contains the face.</target>
				</segment>
			</unit>
			<unit id="146" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source>Because the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Canvas</pc> control is very likely to be a different size than the source image, you should multiply both the X and Y coordinates and the width and height of the <pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceBox</pc> by a scaling value which is the ratio of the source image size to the actual size of the <pc dataRefEnd="id6" dataRefStart="id5" id="p3">Canvas</pc> control.</source>
					<target>Because the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Canvas</pc> control is very likely to be a different size than the source image, you should multiply both the X and Y coordinates and the width and height of the <pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceBox</pc> by a scaling value which is the ratio of the source image size to the actual size of the <pc dataRefEnd="id6" dataRefStart="id5" id="p3">Canvas</pc> control.</target>
				</segment>
			</unit>
			<unit id="147" translate="yes">
				<segment state="initial">
					<source>ShowDetectedFaces</source>
					<target>ShowDetectedFaces</target>
				</segment>
			</unit>
			<unit id="148" translate="yes">
				<segment state="initial">
					<source>Track faces in a sequence of frames</source>
					<target>Track faces in a sequence of frames</target>
				</segment>
			</unit>
			<unit id="149" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974150)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn974129)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>If you want to detect faces in video, it is more efficient to use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceTracker</pc></pc> class rather than the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">FaceDetector</pc></pc> class, although the implementation steps are very similar.</source>
					<target>If you want to detect faces in video, it is more efficient to use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceTracker</pc></pc> class rather than the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">FaceDetector</pc></pc> class, although the implementation steps are very similar.</target>
				</segment>
			</unit>
			<unit id="150" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceTracker</pc> uses information about previously processed frames to optimize the detection process.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceTracker</pc> uses information about previously processed frames to optimize the detection process.</target>
				</segment>
			</unit>
			<unit id="151" translate="yes">
				<segment state="initial">
					<source>FaceTrackingUsing</source>
					<target>FaceTrackingUsing</target>
				</segment>
			</unit>
			<unit id="152" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>Declare a class variable for the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceTracker</pc> object.</source>
					<target>Declare a class variable for the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceTracker</pc> object.</target>
				</segment>
			</unit>
			<unit id="153" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/br230587)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>This example uses a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">ThreadPoolTimer</pc></pc> to initiate face tracking on a defined interval.</source>
					<target>This example uses a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">ThreadPoolTimer</pc></pc> to initiate face tracking on a defined interval.</target>
				</segment>
			</unit>
			<unit id="154" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/system.threading.semaphoreslim.aspx)</data>
				</originalData>
				<segment state="initial">
					<source>A <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SemaphoreSlim</pc> is used to make sure that only one face tracking operation is running at a time.</source>
					<target>A <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SemaphoreSlim</pc> is used to make sure that only one face tracking operation is running at a time.</target>
				</segment>
			</unit>
			<unit id="155" translate="yes">
				<segment state="initial">
					<source>ClassVariables3</source>
					<target>ClassVariables3</target>
				</segment>
			</unit>
			<unit id="156" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn974151)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source>To initialize the face tracking operation, create a new <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceTracker</pc> object by calling <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">CreateAsync</pc></pc>.</source>
					<target>To initialize the face tracking operation, create a new <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceTracker</pc> object by calling <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">CreateAsync</pc></pc>.</target>
				</segment>
			</unit>
			<unit id="157" translate="yes">
				<segment state="initial">
					<source>Initialize the desired timer interval and then create the timer.</source>
					<target>Initialize the desired timer interval and then create the timer.</target>
				</segment>
			</unit>
			<unit id="158" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">ProcessCurrentVideoFrame</pc> helper method will be called every time the specified interval elapses.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">ProcessCurrentVideoFrame</pc> helper method will be called every time the specified interval elapses.</target>
				</segment>
			</unit>
			<unit id="159" translate="yes">
				<segment state="initial">
					<source>TrackingInit</source>
					<target>TrackingInit</target>
				</segment>
			</unit>
			<unit id="160" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">ProcessCurrentVideoFrame</pc> helper is called asynchronously by the timer, so the method first calls the semaphore's <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Wait</pc> method to see if a tracking operation is ongoing, and if it is the method returns without trying to detect faces.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">ProcessCurrentVideoFrame</pc> helper is called asynchronously by the timer, so the method first calls the semaphore's <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Wait</pc> method to see if a tracking operation is ongoing, and if it is the method returns without trying to detect faces.</target>
				</segment>
			</unit>
			<unit id="161" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>At the end of this method, the semaphore's <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Release</pc> method is called, which allows the subsequent call to <pc dataRefEnd="id4" dataRefStart="id3" id="p2">ProcessCurrentVideoFrame</pc> to continue.</source>
					<target>At the end of this method, the semaphore's <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Release</pc> method is called, which allows the subsequent call to <pc dataRefEnd="id4" dataRefStart="id3" id="p2">ProcessCurrentVideoFrame</pc> to continue.</target>
				</segment>
			</unit>
			<unit id="162" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974150)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn930917)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceTracker</pc></pc> class operates on <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">VideoFrame</pc></pc> objects.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceTracker</pc></pc> class operates on <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">VideoFrame</pc></pc> objects.</target>
				</segment>
			</unit>
			<unit id="163" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](capture-photos-and-video-with-mediacapture.md)</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn764784)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
					<data id="id9">[</data>
					<data id="id10">](https://msdn.microsoft.com/library/windows/apps/dn764788)</data>
					<data id="id11">**</data>
					<data id="id12">**</data>
				</originalData>
				<segment state="initial">
					<source>There are multiple ways you can obtain a <pc dataRefEnd="id2" dataRefStart="id1" id="p1">VideoFrame</pc> including capturing a preview frame from a running <pc dataRefEnd="id4" dataRefStart="id3" id="p2">MediaCapture</pc> object or by implementing the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">ProcessFrame</pc></pc> method of the <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">IBasicVideoEffect</pc></pc>.</source>
					<target>There are multiple ways you can obtain a <pc dataRefEnd="id2" dataRefStart="id1" id="p1">VideoFrame</pc> including capturing a preview frame from a running <pc dataRefEnd="id4" dataRefStart="id3" id="p2">MediaCapture</pc> object or by implementing the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">ProcessFrame</pc></pc> method of the <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">IBasicVideoEffect</pc></pc>.</target>
				</segment>
			</unit>
			<unit id="164" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>This example uses an undefined helper method that returns a video frame, <pc dataRefEnd="id2" dataRefStart="id1" id="p1">GetLatestFrame</pc>, as a placeholder for this operation.</source>
					<target>This example uses an undefined helper method that returns a video frame, <pc dataRefEnd="id2" dataRefStart="id1" id="p1">GetLatestFrame</pc>, as a placeholder for this operation.</target>
				</segment>
			</unit>
			<unit id="165" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](get-a-preview-frame.md)</data>
				</originalData>
				<segment state="initial">
					<source>For information on getting video frames from the preview stream of a running media capture device, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Get a preview frame</pc>.</source>
					<target>For information on getting video frames from the preview stream of a running media capture device, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Get a preview frame</pc>.</target>
				</segment>
			</unit>
			<unit id="166" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>As with <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceDetector</pc>, the <pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceTracker</pc> supports a limited set of pixel formats.</source>
					<target>As with <pc dataRefEnd="id2" dataRefStart="id1" id="p1">FaceDetector</pc>, the <pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceTracker</pc> supports a limited set of pixel formats.</target>
				</segment>
			</unit>
			<unit id="167" translate="yes">
				<segment state="initial">
					<source>This example abandons face detection if the supplied frame is not in the Nv12 format.</source>
					<target>This example abandons face detection if the supplied frame is not in the Nv12 format.</target>
				</segment>
			</unit>
			<unit id="168" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974157)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn974123)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>Call <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">ProcessNextFrameAsync</pc></pc> to retrieve a list of <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">DetectedFace</pc></pc> objects representing the faces in the frame.</source>
					<target>Call <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">ProcessNextFrameAsync</pc></pc> to retrieve a list of <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">DetectedFace</pc></pc> objects representing the faces in the frame.</target>
				</segment>
			</unit>
			<unit id="169" translate="yes">
				<segment state="initial">
					<source>Once you have the list of faces, you can display them in the same manner described above for face detection.</source>
					<target>Once you have the list of faces, you can display them in the same manner described above for face detection.</target>
				</segment>
			</unit>
			<unit id="170" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/hh750317)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Note that, because the face tracking helper method is not called on the UI thread, you must make any UI updates in within a call <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">CoredDispatcher.RunAsync</pc></pc>.</source>
					<target>Note that, because the face tracking helper method is not called on the UI thread, you must make any UI updates in within a call <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">CoredDispatcher.RunAsync</pc></pc>.</target>
				</segment>
			</unit>
			<unit id="171" translate="yes">
				<segment state="initial">
					<source>ProcessCurrentVideoFrame</source>
					<target>ProcessCurrentVideoFrame</target>
				</segment>
			</unit>
			<unit id="172" translate="yes">
				<segment state="initial">
					<source>Related topics</source>
					<target>Related topics</target>
				</segment>
			</unit>
			<unit id="173" translate="yes">
				<segment state="initial">
					<source>Scene analysis for media capture</source>
					<target>Scene analysis for media capture</target>
				</segment>
			</unit>
			<unit id="174" translate="yes">
				<segment state="initial">
					<source>Basic Face Detection sample</source>
					<target>Basic Face Detection sample</target>
				</segment>
			</unit>
			<unit id="175" translate="yes">
				<segment state="initial">
					<source>Basic Face Tracking sample</source>
					<target>Basic Face Tracking sample</target>
				</segment>
			</unit>
			<unit id="176" translate="yes">
				<segment state="initial">
					<source>Capture photos and video with MediaCapture</source>
					<target>Capture photos and video with MediaCapture</target>
				</segment>
			</unit>
		</group>
	</file>
</xliff>
<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns:mda="urn:oasis:names:tc:xliff:metadata:2.0" srcLang="en-US" trgLang="ja-jp" version="2.0" xml:space="preserve" xmlns="urn:oasis:names:tc:xliff:document:2.0">
	<file id="1">
		<mda:metadata>
			<mda:metaGroup>
				<mda:meta type="tool-id">mdxliff</mda:meta>
				<mda:meta type="tool-name">mdxliff</mda:meta>
				<mda:meta type="tool-version">1.0-2eb3c86</mda:meta>
				<mda:meta type="tool-company">Microsoft</mda:meta>
			</mda:metaGroup>
		<mda:metaGroup><mda:meta type="olfilehash">891c0d274c2d3fb82f855011158ecd3ccdcd87b3</mda:meta><mda:meta type="olfilepath">wdg-cpub-test\ndolciTestTDReach\audio-video-camera\scene-analysis-for-media-capture.md</mda:meta><mda:meta type="oltranslationpriority"></mda:meta><mda:meta type="oltranslationtype">Human Translation</mda:meta><mda:meta type="olskeletonhash">3059f22129a30559b2b7a81874749ca11485157f</mda:meta><mda:meta type="olxliffhash">bc87d6cc61eea0001d656a3e8e6f7877b84937fe</mda:meta></mda:metaGroup></mda:metadata>
		<group id="content">
			<unit id="101" translate="yes">
				<segment state="initial">
					<source>This article describes how to use the SceneAnalysisEffect and the FaceDetectionEffect to analyze the content of the media capture preview stream.</source>
					<target>This article describes how to use the SceneAnalysisEffect and the FaceDetectionEffect to analyze the content of the media capture preview stream.</target>
				</segment>
			</unit>
			<unit id="102" translate="yes">
				<segment state="initial">
					<source>Scene analysis for media capture</source>
					<target>Scene analysis for media capture</target>
				</segment>
			</unit>
			<unit id="103" translate="yes">
				<segment state="initial">
					<source>Scene analysis for media capture</source>
					<target>Scene analysis for media capture</target>
				</segment>
			</unit>
			<unit id="104" translate="yes">
				<segment state="initial">
					<source>\[ Updated for UWP apps on Windows 10.</source>
					<target>\[ Updated for UWP apps on Windows 10.</target>
				</segment>
			</unit>
			<unit id="105" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?linkid=619132)</data>
				</originalData>
				<segment state="initial">
					<source>For Windows 8.x articles, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">archive</pc> \]</source>
					<target>For Windows 8.x articles, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">archive</pc> \]</target>
				</segment>
			</unit>
			<unit id="106" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948902)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn948776)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>This article describes how to use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalysisEffect</pc></pc> and the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">FaceDetectionEffect</pc></pc> to analyze the content of the media capture preview stream.</source>
					<target>This article describes how to use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalysisEffect</pc></pc> and the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">FaceDetectionEffect</pc></pc> to analyze the content of the media capture preview stream.</target>
				</segment>
			</unit>
			<unit id="107" translate="yes">
				<segment state="initial">
					<source>Scene analysis effect</source>
					<target>Scene analysis effect</target>
				</segment>
			</unit>
			<unit id="108" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948902)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalysisEffect</pc></pc> analyzes the video frames in the media capture preview stream and recommends processing options to improve the capture result.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalysisEffect</pc></pc> analyzes the video frames in the media capture preview stream and recommends processing options to improve the capture result.</target>
				</segment>
			</unit>
			<unit id="109" translate="yes">
				<segment state="initial">
					<source>Currently, the effect supports detecting whether the capture would be improved by using High Dynamic Range (HDR) processing.</source>
					<target>Currently, the effect supports detecting whether the capture would be improved by using High Dynamic Range (HDR) processing.</target>
				</segment>
			</unit>
			<unit id="110" translate="yes">
				<segment state="initial">
					<source>If the effect recommends using HDR, you can do this in the following ways:</source>
					<target>If the effect recommends using HDR, you can do this in the following ways:</target>
				</segment>
			</unit>
			<unit id="111" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/mt181386)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">AdvancedPhotoCapture</pc></pc> class to capture photos using the Windows built-in HDR processing algorithm.</source>
					<target>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">AdvancedPhotoCapture</pc></pc> class to capture photos using the Windows built-in HDR processing algorithm.</target>
				</segment>
			</unit>
			<unit id="112" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](high-dynamic-range-hdr-photo-capture.md)</data>
				</originalData>
				<segment state="initial">
					<source>For more information, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">High Dynamic Range (HDR) photo capture</pc>.</source>
					<target>For more information, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">High Dynamic Range (HDR) photo capture</pc>.</target>
				</segment>
			</unit>
			<unit id="113" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn926680)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">HdrVideoControl</pc></pc> to capture video using the Windows built-in HDR processing algorithm.</source>
					<target>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">HdrVideoControl</pc></pc> to capture video using the Windows built-in HDR processing algorithm.</target>
				</segment>
			</unit>
			<unit id="114" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](capture-device-controls-for-video-capture.md)</data>
				</originalData>
				<segment state="initial">
					<source>For more information, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Capture device controls for video capture</pc>.</source>
					<target>For more information, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Capture device controls for video capture</pc>.</target>
				</segment>
			</unit>
			<unit id="115" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn640573)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">VariablePhotoSequenceControl</pc></pc> to capture a sequence of frames that you can then composite using a custom HDR implementation.</source>
					<target>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">VariablePhotoSequenceControl</pc></pc> to capture a sequence of frames that you can then composite using a custom HDR implementation.</target>
				</segment>
			</unit>
			<unit id="116" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](variable-photo-sequence.md)</data>
				</originalData>
				<segment state="initial">
					<source>For more information, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Variable photo sequence</pc>.</source>
					<target>For more information, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Variable photo sequence</pc>.</target>
				</segment>
			</unit>
			<unit id="117" translate="yes">
				<segment state="initial">
					<source>Scene analysis namespaces</source>
					<target>Scene analysis namespaces</target>
				</segment>
			</unit>
			<unit id="118" translate="yes">
				<segment state="initial">
					<source>To use scene analysis, your app must include the following namespaces in addition to the namespaces required for basic media capture.</source>
					<target>To use scene analysis, your app must include the following namespaces in addition to the namespaces required for basic media capture.</target>
				</segment>
			</unit>
			<unit id="119" translate="yes">
				<segment state="initial">
					<source>SceneAnalysisUsing</source>
					<target>SceneAnalysisUsing</target>
				</segment>
			</unit>
			<unit id="120" translate="yes">
				<segment state="initial">
					<source>Initialize the scene analysis effect and add it to the preview stream</source>
					<target>Initialize the scene analysis effect and add it to the preview stream</target>
				</segment>
			</unit>
			<unit id="121" translate="yes">
				<segment state="initial">
					<source>Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.</source>
					<target>Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.</target>
				</segment>
			</unit>
			<unit id="122" translate="yes">
				<segment state="initial">
					<source>Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.</source>
					<target>Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.</target>
				</segment>
			</unit>
			<unit id="123" translate="yes">
				<segment state="initial">
					<source>DeclareSceneAnalysisEffect</source>
					<target>DeclareSceneAnalysisEffect</target>
				</segment>
			</unit>
			<unit id="124" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn948903)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source>In your app, after you have initialized the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">MediaCapture</pc> object, create a new instance of <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SceneAnalysisEffectDefinition</pc></pc>.</source>
					<target>In your app, after you have initialized the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">MediaCapture</pc> object, create a new instance of <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">SceneAnalysisEffectDefinition</pc></pc>.</target>
				</segment>
			</unit>
			<unit id="125" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn878035)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
					<data id="id9">[</data>
					<data id="id10">](https://msdn.microsoft.com/library/windows/apps/br226640)</data>
					<data id="id11">**</data>
					<data id="id12">**</data>
				</originalData>
				<segment state="initial">
					<source>Register the effect with the capture device by calling <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">AddVideoEffectAsync</pc></pc> on your <pc dataRefEnd="id6" dataRefStart="id5" id="p3">MediaCapture</pc> object, providing the <pc dataRefEnd="id8" dataRefStart="id7" id="p4">SceneAnalysisEffectDefinition</pc> and specifying <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">MediaStreamType.VideoPreview</pc></pc> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.</source>
					<target>Register the effect with the capture device by calling <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">AddVideoEffectAsync</pc></pc> on your <pc dataRefEnd="id6" dataRefStart="id5" id="p3">MediaCapture</pc> object, providing the <pc dataRefEnd="id8" dataRefStart="id7" id="p4">SceneAnalysisEffectDefinition</pc> and specifying <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">MediaStreamType.VideoPreview</pc></pc> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.</target>
				</segment>
			</unit>
			<unit id="126" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">AddVideoEffectAsync</pc> returns an instance of the added effect.</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">AddVideoEffectAsync</pc> returns an instance of the added effect.</target>
				</segment>
			</unit>
			<unit id="127" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948902)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Because this method can be used with multiple effect types, you must cast the returned instance to a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalysisEffect</pc></pc> object.</source>
					<target>Because this method can be used with multiple effect types, you must cast the returned instance to a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalysisEffect</pc></pc> object.</target>
				</segment>
			</unit>
			<unit id="128" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948920)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>To receive the results of the scene analysis, you must register a handler for the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalyzed</pc></pc> event.</source>
					<target>To receive the results of the scene analysis, you must register a handler for the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalyzed</pc></pc> event.</target>
				</segment>
			</unit>
			<unit id="129" translate="yes">
				<segment state="initial">
					<source>Currently, the scene analysis effect only includes the high dynamic range analyzer.</source>
					<target>Currently, the scene analysis effect only includes the high dynamic range analyzer.</target>
				</segment>
			</unit>
			<unit id="130" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948827)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Enable HDR analysis by setting the effect's <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">HighDynamicRangeControl.Enabled</pc></pc> to true.</source>
					<target>Enable HDR analysis by setting the effect's <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">HighDynamicRangeControl.Enabled</pc></pc> to true.</target>
				</segment>
			</unit>
			<unit id="131" translate="yes">
				<segment state="initial">
					<source>CreateSceneAnalysisEffectAsync</source>
					<target>CreateSceneAnalysisEffectAsync</target>
				</segment>
			</unit>
			<unit id="132" translate="yes">
				<segment state="initial">
					<source>Implement the SceneAnalyzed event handler</source>
					<target>Implement the SceneAnalyzed event handler</target>
				</segment>
			</unit>
			<unit id="133" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The results of the scene analysis are returned in the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SceneAnalyzed</pc> event handler.</source>
					<target>The results of the scene analysis are returned in the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SceneAnalyzed</pc> event handler.</target>
				</segment>
			</unit>
			<unit id="134" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948922)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn948907)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
					<data id="id9">[</data>
					<data id="id10">](https://msdn.microsoft.com/library/windows/apps/dn948830)</data>
					<data id="id11">**</data>
					<data id="id12">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalyzedEventArgs</pc></pc> object passed into the handler has a <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">SceneAnalysisEffectFrame</pc></pc> object which has a <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">HighDynamicRangeOutput</pc></pc> object.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SceneAnalyzedEventArgs</pc></pc> object passed into the handler has a <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">SceneAnalysisEffectFrame</pc></pc> object which has a <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">HighDynamicRangeOutput</pc></pc> object.</target>
				</segment>
			</unit>
			<unit id="135" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948833)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">Certainty</pc></pc> property of the high dynamic range output provides a value between 0 and 1.0 where 0 indicates that HDR processing would not help improve the capture result and 1.0 indicates that HDR processing would help.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">Certainty</pc></pc> property of the high dynamic range output provides a value between 0 and 1.0 where 0 indicates that HDR processing would not help improve the capture result and 1.0 indicates that HDR processing would help.</target>
				</segment>
			</unit>
			<unit id="136" translate="yes">
				<segment state="initial">
					<source>Your can decide the threshold point at which you want to use HDR or show the results to the user and let the user decide.</source>
					<target>Your can decide the threshold point at which you want to use HDR or show the results to the user and let the user decide.</target>
				</segment>
			</unit>
			<unit id="137" translate="yes">
				<segment state="initial">
					<source>SceneAnalyzed</source>
					<target>SceneAnalyzed</target>
				</segment>
			</unit>
			<unit id="138" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948830)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn948834)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">HighDynamicRangeOutput</pc></pc> object passed into the handler also has a <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">FrameControllers</pc></pc> property which contains suggested frame controllers for capturing a variable photo sequence for HDR processing.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">HighDynamicRangeOutput</pc></pc> object passed into the handler also has a <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">FrameControllers</pc></pc> property which contains suggested frame controllers for capturing a variable photo sequence for HDR processing.</target>
				</segment>
			</unit>
			<unit id="139" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](variable-photo-sequence.md)</data>
				</originalData>
				<segment state="initial">
					<source>For more information, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Variable photo sequence</pc>.</source>
					<target>For more information, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Variable photo sequence</pc>.</target>
				</segment>
			</unit>
			<unit id="140" translate="yes">
				<segment state="initial">
					<source>Clean up the scene analysis effect</source>
					<target>Clean up the scene analysis effect</target>
				</segment>
			</unit>
			<unit id="141" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn948827)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
					<data id="id7">[</data>
					<data id="id8">](https://msdn.microsoft.com/library/windows/apps/dn948920)</data>
					<data id="id9">**</data>
					<data id="id10">**</data>
				</originalData>
				<segment state="initial">
					<source>When your app is done capturing, before disposing of the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">MediaCapture</pc> object, you should disable the scene analysis effect by setting the effect's <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">HighDynamicRangeAnalyzer.Enabled</pc></pc> property to false and unregister your <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">SceneAnalyzed</pc></pc> event handler.</source>
					<target>When your app is done capturing, before disposing of the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">MediaCapture</pc> object, you should disable the scene analysis effect by setting the effect's <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">HighDynamicRangeAnalyzer.Enabled</pc></pc> property to false and unregister your <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">SceneAnalyzed</pc></pc> event handler.</target>
				</segment>
			</unit>
			<unit id="142" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/br226592)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Call <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">MediaCapture.ClearEffectsAsync</pc></pc>, specifying the video preview stream since that was the stream to which the effect was added.</source>
					<target>Call <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">MediaCapture.ClearEffectsAsync</pc></pc>, specifying the video preview stream since that was the stream to which the effect was added.</target>
				</segment>
			</unit>
			<unit id="143" translate="yes">
				<segment state="initial">
					<source>Finally, set your member variable to null.</source>
					<target>Finally, set your member variable to null.</target>
				</segment>
			</unit>
			<unit id="144" translate="yes">
				<segment state="initial">
					<source>CleanUpSceneAnalysisEffectAsync</source>
					<target>CleanUpSceneAnalysisEffectAsync</target>
				</segment>
			</unit>
			<unit id="145" translate="yes">
				<segment state="initial">
					<source>Face detection effect</source>
					<target>Face detection effect</target>
				</segment>
			</unit>
			<unit id="146" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948776)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect</pc></pc> identifies the location of faces within the media capture preview stream.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect</pc></pc> identifies the location of faces within the media capture preview stream.</target>
				</segment>
			</unit>
			<unit id="147" translate="yes">
				<segment state="initial">
					<source>The effect allows you to receive a notification whenever a face is detected in the preview stream and provides the bounding box for each detected face within the preview frame.</source>
					<target>The effect allows you to receive a notification whenever a face is detected in the preview stream and provides the bounding box for each detected face within the preview frame.</target>
				</segment>
			</unit>
			<unit id="148" translate="yes">
				<segment state="initial">
					<source>On supported devices, the face detection effect also provides enhanced exposure and focus on the most important face in the scene.</source>
					<target>On supported devices, the face detection effect also provides enhanced exposure and focus on the most important face in the scene.</target>
				</segment>
			</unit>
			<unit id="149" translate="yes">
				<segment state="initial">
					<source>Face detection namespaces</source>
					<target>Face detection namespaces</target>
				</segment>
			</unit>
			<unit id="150" translate="yes">
				<segment state="initial">
					<source>To use face detection, your app must include the following namespaces in addition to the namespaces required for basic media capture.</source>
					<target>To use face detection, your app must include the following namespaces in addition to the namespaces required for basic media capture.</target>
				</segment>
			</unit>
			<unit id="151" translate="yes">
				<segment state="initial">
					<source>FaceDetectionUsing</source>
					<target>FaceDetectionUsing</target>
				</segment>
			</unit>
			<unit id="152" translate="yes">
				<segment state="initial">
					<source>Initialize the face detection effect and add it to the preview stream</source>
					<target>Initialize the face detection effect and add it to the preview stream</target>
				</segment>
			</unit>
			<unit id="153" translate="yes">
				<segment state="initial">
					<source>Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.</source>
					<target>Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.</target>
				</segment>
			</unit>
			<unit id="154" translate="yes">
				<segment state="initial">
					<source>Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.</source>
					<target>Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.</target>
				</segment>
			</unit>
			<unit id="155" translate="yes">
				<segment state="initial">
					<source>DeclareFaceDetectionEffect</source>
					<target>DeclareFaceDetectionEffect</target>
				</segment>
			</unit>
			<unit id="156" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn948778)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source>In your app, after you have initialized the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">MediaCapture</pc> object, create a new instance of <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">FaceDetectionEffectDefinition</pc></pc>.</source>
					<target>In your app, after you have initialized the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">MediaCapture</pc> object, create a new instance of <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">FaceDetectionEffectDefinition</pc></pc>.</target>
				</segment>
			</unit>
			<unit id="157" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948781)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Set the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">DetectionMode</pc></pc> property to prioritize faster face detection or more accurate face detection.</source>
					<target>Set the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">DetectionMode</pc></pc> property to prioritize faster face detection or more accurate face detection.</target>
				</segment>
			</unit>
			<unit id="158" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948786)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Set <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SynchronousDetectionEnabled</pc></pc> to specify that incoming frames are not delayed waiting for face detection to complete as this can result in a choppy preview experience.</source>
					<target>Set <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">SynchronousDetectionEnabled</pc></pc> to specify that incoming frames are not delayed waiting for face detection to complete as this can result in a choppy preview experience.</target>
				</segment>
			</unit>
			<unit id="159" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn878035)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
					<data id="id9">[</data>
					<data id="id10">](https://msdn.microsoft.com/library/windows/apps/br226640)</data>
					<data id="id11">**</data>
					<data id="id12">**</data>
				</originalData>
				<segment state="initial">
					<source>Register the effect with the capture device by calling <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">AddVideoEffectAsync</pc></pc> on your <pc dataRefEnd="id6" dataRefStart="id5" id="p3">MediaCapture</pc> object, providing the <pc dataRefEnd="id8" dataRefStart="id7" id="p4">FaceDetectionEffectDefinition</pc> and specifying <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">MediaStreamType.VideoPreview</pc></pc> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.</source>
					<target>Register the effect with the capture device by calling <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">AddVideoEffectAsync</pc></pc> on your <pc dataRefEnd="id6" dataRefStart="id5" id="p3">MediaCapture</pc> object, providing the <pc dataRefEnd="id8" dataRefStart="id7" id="p4">FaceDetectionEffectDefinition</pc> and specifying <pc dataRefEnd="id10" dataRefStart="id9" id="p5"><pc dataRefEnd="id12" dataRefStart="id11" id="p6">MediaStreamType.VideoPreview</pc></pc> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.</target>
				</segment>
			</unit>
			<unit id="160" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">AddVideoEffectAsync</pc> returns an instance of the added effect.</source>
					<target><pc dataRefEnd="id2" dataRefStart="id1" id="p1">AddVideoEffectAsync</pc> returns an instance of the added effect.</target>
				</segment>
			</unit>
			<unit id="161" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948776)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Because this method can be used with multiple effect types, you must cast the returned instance to a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect</pc></pc> object.</source>
					<target>Because this method can be used with multiple effect types, you must cast the returned instance to a <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect</pc></pc> object.</target>
				</segment>
			</unit>
			<unit id="162" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948818)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Enable or disable the effect by setting the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect.Enabled</pc></pc> property.</source>
					<target>Enable or disable the effect by setting the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect.Enabled</pc></pc> property.</target>
				</segment>
			</unit>
			<unit id="163" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948814)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Adjust how often the effect analyzes frames by setting the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect.DesiredDetectionInterval</pc></pc> property.</source>
					<target>Adjust how often the effect analyzes frames by setting the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffect.DesiredDetectionInterval</pc></pc> property.</target>
				</segment>
			</unit>
			<unit id="164" translate="yes">
				<segment state="initial">
					<source>Both of these properties can be adjusted while media capture is ongoing.</source>
					<target>Both of these properties can be adjusted while media capture is ongoing.</target>
				</segment>
			</unit>
			<unit id="165" translate="yes">
				<segment state="initial">
					<source>CreateFaceDetectionEffectAsync</source>
					<target>CreateFaceDetectionEffectAsync</target>
				</segment>
			</unit>
			<unit id="166" translate="yes">
				<segment state="initial">
					<source>Receive notifications when faces are detected</source>
					<target>Receive notifications when faces are detected</target>
				</segment>
			</unit>
			<unit id="167" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948820)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>If you want to perform some action when faces are detected, such as drawing a box around detected faces in the video preview, you can register for the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetected</pc></pc> event.</source>
					<target>If you want to perform some action when faces are detected, such as drawing a box around detected faces in the video preview, you can register for the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetected</pc></pc> event.</target>
				</segment>
			</unit>
			<unit id="168" translate="yes">
				<segment state="initial">
					<source>RegisterFaceDetectionHandler</source>
					<target>RegisterFaceDetectionHandler</target>
				</segment>
			</unit>
			<unit id="169" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn948792)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn948774)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>In the handler for the event, you can get a list of all faces detected in a frame by accessing the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffectFrame.DetectedFaces</pc></pc> property of the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">FaceDetectedEventArgs</pc></pc>.</source>
					<target>In the handler for the event, you can get a list of all faces detected in a frame by accessing the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceDetectionEffectFrame.DetectedFaces</pc></pc> property of the <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">FaceDetectedEventArgs</pc></pc>.</target>
				</segment>
			</unit>
			<unit id="170" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn974126)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/br226169)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceBox</pc></pc> property is a <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">BitmapBounds</pc></pc> structure that describes the rectangle containing the detected face in units relative to the preview stream dimensions.</source>
					<target>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">FaceBox</pc></pc> property is a <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">BitmapBounds</pc></pc> structure that describes the rectangle containing the detected face in units relative to the preview stream dimensions.</target>
				</segment>
			</unit>
			<unit id="171" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/?LinkId=619486)</data>
				</originalData>
				<segment state="initial">
					<source>To view sample code that transforms the preview stream coordinates into screen coordinates, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">face detection UWP sample</pc>.</source>
					<target>To view sample code that transforms the preview stream coordinates into screen coordinates, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">face detection UWP sample</pc>.</target>
				</segment>
			</unit>
			<unit id="172" translate="yes">
				<segment state="initial">
					<source>FaceDetected</source>
					<target>FaceDetected</target>
				</segment>
			</unit>
			<unit id="173" translate="yes">
				<segment state="initial">
					<source>Clean up the face detection effect</source>
					<target>Clean up the face detection effect</target>
				</segment>
			</unit>
			<unit id="174" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](https://msdn.microsoft.com/library/windows/apps/dn948818)</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
					<data id="id7">[</data>
					<data id="id8">](https://msdn.microsoft.com/library/windows/apps/dn948820)</data>
					<data id="id9">**</data>
					<data id="id10">**</data>
				</originalData>
				<segment state="initial">
					<source>When your app is done capturing, before disposing of the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">MediaCapture</pc> object, you should disable the face detection effect with <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">FaceDetectionEffect.Enabled</pc></pc> and unregister your <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">FaceDetected</pc></pc> event handler if you previously registered one.</source>
					<target>When your app is done capturing, before disposing of the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">MediaCapture</pc> object, you should disable the face detection effect with <pc dataRefEnd="id4" dataRefStart="id3" id="p2"><pc dataRefEnd="id6" dataRefStart="id5" id="p3">FaceDetectionEffect.Enabled</pc></pc> and unregister your <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">FaceDetected</pc></pc> event handler if you previously registered one.</target>
				</segment>
			</unit>
			<unit id="175" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/br226592)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Call <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">MediaCapture.ClearEffectsAsync</pc></pc>, specifying the video preview stream since that was the stream to which the effect was added.</source>
					<target>Call <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">MediaCapture.ClearEffectsAsync</pc></pc>, specifying the video preview stream since that was the stream to which the effect was added.</target>
				</segment>
			</unit>
			<unit id="176" translate="yes">
				<segment state="initial">
					<source>Finally, set your member variable to null.</source>
					<target>Finally, set your member variable to null.</target>
				</segment>
			</unit>
			<unit id="177" translate="yes">
				<segment state="initial">
					<source>CleanUpFaceDetectionEffectAsync</source>
					<target>CleanUpFaceDetectionEffectAsync</target>
				</segment>
			</unit>
			<unit id="178" translate="yes">
				<segment state="initial">
					<source>Check for focus and exposure support for detected faces</source>
					<target>Check for focus and exposure support for detected faces</target>
				</segment>
			</unit>
			<unit id="179" translate="yes">
				<segment state="initial">
					<source>Not all devices have a capture device that can adjust its focus and exposure based on detected faces.</source>
					<target>Not all devices have a capture device that can adjust its focus and exposure based on detected faces.</target>
				</segment>
			</unit>
			<unit id="180" translate="yes">
				<segment state="initial">
					<source>Because face detection consumes device resources, you may only want to enable face detection on devices that can use the feature to enhance capture.</source>
					<target>Because face detection consumes device resources, you may only want to enable face detection on devices that can use the feature to enhance capture.</target>
				</segment>
			</unit>
			<unit id="181" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/br226825)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](capture-photos-and-video-with-mediacapture.md)</data>
					<data id="id7">[</data>
					<data id="id8">](https://msdn.microsoft.com/library/windows/apps/dn279064)</data>
					<data id="id9">**</data>
					<data id="id10">**</data>
				</originalData>
				<segment state="initial">
					<source>To see if face-based capture optimization is available, get the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">VideoDeviceController</pc></pc> for your initialized <pc dataRefEnd="id6" dataRefStart="id5" id="p3">MediaCapture</pc> and then get the video device controller's <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">RegionsOfInterestControl</pc></pc>.</source>
					<target>To see if face-based capture optimization is available, get the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">VideoDeviceController</pc></pc> for your initialized <pc dataRefEnd="id6" dataRefStart="id5" id="p3">MediaCapture</pc> and then get the video device controller's <pc dataRefEnd="id8" dataRefStart="id7" id="p4"><pc dataRefEnd="id10" dataRefStart="id9" id="p5">RegionsOfInterestControl</pc></pc>.</target>
				</segment>
			</unit>
			<unit id="182" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn279069)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>Check to see if the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">MaxRegions</pc></pc> supports at least one region.</source>
					<target>Check to see if the <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">MaxRegions</pc></pc> supports at least one region.</target>
				</segment>
			</unit>
			<unit id="183" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/dn279065)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">[</data>
					<data id="id6">](https://msdn.microsoft.com/library/windows/apps/dn279066)</data>
					<data id="id7">**</data>
					<data id="id8">**</data>
				</originalData>
				<segment state="initial">
					<source>Then check to see if either <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">AutoExposureSupported</pc></pc> or <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">AutoFocusSupported</pc></pc> are true.</source>
					<target>Then check to see if either <pc dataRefEnd="id2" dataRefStart="id1" id="p1"><pc dataRefEnd="id4" dataRefStart="id3" id="p2">AutoExposureSupported</pc></pc> or <pc dataRefEnd="id6" dataRefStart="id5" id="p3"><pc dataRefEnd="id8" dataRefStart="id7" id="p4">AutoFocusSupported</pc></pc> are true.</target>
				</segment>
			</unit>
			<unit id="184" translate="yes">
				<segment state="initial">
					<source>If these conditions are met, then the device can take advantage of face detection to enhance capture.</source>
					<target>If these conditions are met, then the device can take advantage of face detection to enhance capture.</target>
				</segment>
			</unit>
			<unit id="185" translate="yes">
				<segment state="initial">
					<source>AreFaceFocusAndExposureSupported</source>
					<target>AreFaceFocusAndExposureSupported</target>
				</segment>
			</unit>
			<unit id="186" translate="yes">
				<segment state="initial">
					<source>Related topics</source>
					<target>Related topics</target>
				</segment>
			</unit>
			<unit id="187" translate="yes">
				<segment state="initial">
					<source>Capture photos and video with MediaCapture</source>
					<target>Capture photos and video with MediaCapture</target>
				</segment>
			</unit>
		</group>
	</file>
</xliff>
<?xml version="1.0" encoding="utf-8"?>
<xliff xmlns:mda="urn:oasis:names:tc:xliff:metadata:2.0" srcLang="en-US" trgLang="de-de" version="2.0" xml:space="preserve" xmlns="urn:oasis:names:tc:xliff:document:2.0">
	<file id="1">
		<mda:metadata>
			<mda:metaGroup>
				<mda:meta type="tool-id">mdxliff</mda:meta>
				<mda:meta type="tool-name">mdxliff</mda:meta>
				<mda:meta type="tool-version">1.0-10daaac</mda:meta>
				<mda:meta type="tool-company">Microsoft</mda:meta>
			</mda:metaGroup>
		<mda:metaGroup><mda:meta type="olfilehash">defebf0926bfdc9a7eb84c8c1cd6997124ad7288</mda:meta><mda:meta type="olfilepath">wdg-cpub-test\ndolci2\input-and-devices\speech-interactions.md</mda:meta><mda:meta type="oltranslationpriority"></mda:meta><mda:meta type="oltranslationtype">Human Translation</mda:meta><mda:meta type="olskeletonhash">18136edc016050bcdff8af9259aca6c9ee4c637d</mda:meta><mda:meta type="olxliffhash">294051bec4c4e741d5db4b48eb28214706d65b54</mda:meta></mda:metaGroup></mda:metadata>
		<group id="content">
			<unit id="101" translate="yes">
				<segment state="initial">
					<source>Incorporate speech into your apps using Cortana voice commands, speech recognition, and speech synthesis.</source>
				</segment>
			</unit>
			<unit id="102" translate="yes">
				<segment state="initial">
					<source>Speech interactions</source>
				</segment>
			</unit>
			<unit id="103" translate="yes">
				<segment state="initial">
					<source>Speech interactions</source>
				</segment>
			</unit>
			<unit id="104" translate="yes">
				<segment state="initial">
					<source>\[ Updated for UWP apps on Windows 10.</source>
				</segment>
			</unit>
			<unit id="105" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?linkid=619132)</data>
				</originalData>
				<segment state="initial">
					<source>For Windows 8.x articles, see the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">archive</pc> \]</source>
				</segment>
			</unit>
			<unit id="106" translate="yes">
				<segment state="initial">
					<source>Integrate speech recognition and text-to-speech (also known as TTS, or speech synthesis) directly into the user experience of your app.</source>
				</segment>
			</unit>
			<unit id="107" translate="yes">
				<segment state="initial">
					<source>Other speech components</source>
				</segment>
			</unit>
			<unit id="108" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](cortana-interactions.md)</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
				</originalData>
				<segment state="initial">
					<source>See the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Cortana design guidelines</pc> if you are exposing app functionality in the <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Cortana</pc> UI.</source>
				</segment>
			</unit>
			<unit id="109" translate="yes">
				<segment state="initial">
					<source>**Speech recognition:  **converts words spoken by the user into text for form input, for text dictation, to specify an action or command, and to accomplish tasks.</source>
				</segment>
			</unit>
			<unit id="110" translate="yes">
				<segment state="initial">
					<source>Both pre-defined grammars for free-text dictation and web search, and custom grammars authored using Speech Recognition Grammar Specification (SRGS) Version 1.0 are supported.</source>
				</segment>
			</unit>
			<unit id="111" translate="yes">
				<segment state="initial">
					<source>**TTS:  **uses a speech synthesis engine (voice) to convert a text string into spoken words.</source>
				</segment>
			</unit>
			<unit id="112" translate="yes">
				<segment state="initial">
					<source>The input string can be either basic, unadorned text or more complex Speech Synthesis Markup Language (SSML).</source>
				</segment>
			</unit>
			<unit id="113" translate="yes">
				<segment state="initial">
					<source>SSML provides a standard way to control characteristics of speech output, such as pronunciation, volume, pitch, rate or speed, and emphasis.</source>
				</segment>
			</unit>
			<unit id="114" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">**</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
					<data id="id6">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Note</pc>  Using <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Cortana</pc> and customized voice commands, your app can be launched in the foreground (the app takes focus, just as if it was launched from the Start menu) or activated as a background service (<pc dataRefEnd="id6" dataRefStart="id5" id="p3">Cortana</pc> retains focus but provides results from the app).</source>
				</segment>
			</unit>
			<unit id="115" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>Commands that require additional context or user input (such as sending a message to a specific contact) are best handled in a foreground app, while basic commands can be handled in <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Cortana</pc> through a background app.</source>
				</segment>
			</unit>
			<unit id="116" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
					<data id="id3">[</data>
					<data id="id4">](cortana-design-guidelines.md)</data>
				</originalData>
				<segment state="initial">
					<source>If you are exposing functionality as a background service through voice commands in the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Cortana</pc> UI, see the <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Cortana design guidelines</pc>.</source>
				</segment>
			</unit>
			<unit id="117" translate="yes">
				<segment state="initial">
					<source>Designed and implemented thoughtfully, speech can be a robust and enjoyable way for people to interact with your app, complementing, or even replacing, keyboard, mouse, touch, and gestures.</source>
				</segment>
			</unit>
			<unit id="118" translate="yes">
				<segment state="initial">
					<source>Speech interaction design</source>
				</segment>
			</unit>
			<unit id="119" translate="yes">
				<segment state="initial">
					<source>These guidelines and recommendations describe how to best integrate both speech recognition and TTS into the interaction experience of your app.</source>
				</segment>
			</unit>
			<unit id="120" translate="yes">
				<segment state="initial">
					<source>If you are considering supporting speech interactions in your app:</source>
				</segment>
			</unit>
			<unit id="121" translate="yes">
				<segment state="initial">
					<source>What actions can be taken through speech?</source>
				</segment>
			</unit>
			<unit id="122" translate="yes">
				<segment state="initial">
					<source>Can a user navigate between pages, invoke commands, or enter data as text fields, brief notes, or long messages?</source>
				</segment>
			</unit>
			<unit id="123" translate="yes">
				<segment state="initial">
					<source>Is speech input a good option for completing a task?</source>
				</segment>
			</unit>
			<unit id="124" translate="yes">
				<segment state="initial">
					<source>How does a user know when speech input is available?</source>
				</segment>
			</unit>
			<unit id="125" translate="yes">
				<segment state="initial">
					<source>Is the app always listening, or does the user need to take an action for the app to enter listening mode?</source>
				</segment>
			</unit>
			<unit id="126" translate="yes">
				<segment state="initial">
					<source>What phrases initiate an action or behavior?</source>
				</segment>
			</unit>
			<unit id="127" translate="yes">
				<segment state="initial">
					<source>Do the phrases and actions need to be enumerated on screen?</source>
				</segment>
			</unit>
			<unit id="128" translate="yes">
				<segment state="initial">
					<source>Are prompt, confirmation, and disambiguation screens or TTS required?</source>
				</segment>
			</unit>
			<unit id="129" translate="yes">
				<segment state="initial">
					<source>What is the interaction dialog between app and user?</source>
				</segment>
			</unit>
			<unit id="130" translate="yes">
				<segment state="initial">
					<source>Is a custom or constrained vocabulary required (such as medicine, science, or locale) for the context of your app?</source>
				</segment>
			</unit>
			<unit id="131" translate="yes">
				<segment state="initial">
					<source>Is network connectivity required?</source>
				</segment>
			</unit>
			<unit id="132" translate="yes">
				<segment state="initial">
					<source>Text input</source>
				</segment>
			</unit>
			<unit id="133" translate="yes">
				<segment state="initial">
					<source>Speech for text input can range from short form (single word or phrase) to long form (continuous dictation).</source>
				</segment>
			</unit>
			<unit id="134" translate="yes">
				<segment state="initial">
					<source>Short form input must be less than 10 seconds in length, while long form input session can be up to two minutes in length.</source>
				</segment>
			</unit>
			<unit id="135" translate="yes">
				<segment state="initial">
					<source>(Long form input can be restarted without user intervention to give the impression of continuous dictation.)</source>
				</segment>
			</unit>
			<unit id="136" translate="yes">
				<segment state="initial">
					<source>You should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on.</source>
				</segment>
			</unit>
			<unit id="137" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](../controls-and-patterns/app-bars.md)</data>
				</originalData>
				<segment state="initial">
					<source>For example, a command bar button with a microphone glyph (see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Command bars</pc>) can be used to show both availability and state.</source>
				</segment>
			</unit>
			<unit id="138" translate="yes">
				<segment state="initial">
					<source>Provide ongoing recognition feedback to minimize any apparent lack of response while recognition is being performed.</source>
				</segment>
			</unit>
			<unit id="139" translate="yes">
				<segment state="initial">
					<source>Let users revise recognition text using keyboard input, disambiguation prompts, suggestions, or additional speech recognition.</source>
				</segment>
			</unit>
			<unit id="140" translate="yes">
				<segment state="initial">
					<source>Stop recognition if input is detected from a device other than speech recognition, such as touch or keyboard.</source>
				</segment>
			</unit>
			<unit id="141" translate="yes">
				<segment state="initial">
					<source>This probably indicates that the user has moved onto another task, such as correcting the recognition text or interacting with other form fields.</source>
				</segment>
			</unit>
			<unit id="142" translate="yes">
				<segment state="initial">
					<source>Specify the length of time for which no speech input indicates that recognition is over.</source>
				</segment>
			</unit>
			<unit id="143" translate="yes">
				<segment state="initial">
					<source>Do not automatically restart recognition after this period of time as it typically indicates the user has stopped engaging with your app.</source>
				</segment>
			</unit>
			<unit id="144" translate="yes">
				<segment state="initial">
					<source>Disable all continuous recognition UI and terminate the recognition session if a network connection is not available.</source>
				</segment>
			</unit>
			<unit id="145" translate="yes">
				<segment state="initial">
					<source>Continuous recogntion requires a network connection.</source>
				</segment>
			</unit>
			<unit id="146" translate="yes">
				<segment state="initial">
					<source>Commanding</source>
				</segment>
			</unit>
			<unit id="147" translate="yes">
				<segment state="initial">
					<source>Speech input can initiate actions, invoke commands, and accomplish tasks.</source>
				</segment>
			</unit>
			<unit id="148" translate="yes">
				<segment state="initial">
					<source>If space permits, consider displaying the supported responses for the current app context, with examples of valid input.</source>
				</segment>
			</unit>
			<unit id="149" translate="yes">
				<segment state="initial">
					<source>This reduces the potential responses your app has to process and also eliminates confusion for the user.</source>
				</segment>
			</unit>
			<unit id="150" translate="yes">
				<segment state="initial">
					<source>Try to frame your questions such that they elicit as specific a response as possible.</source>
				</segment>
			</unit>
			<unit id="151" translate="yes">
				<segment state="initial">
					<source>For example, "What do you want to do today?"</source>
				</segment>
			</unit>
			<unit id="152" translate="yes">
				<segment state="initial">
					<source>is very open ended and would require a very large grammar definition due to how varied the responses could be.</source>
				</segment>
			</unit>
			<unit id="153" translate="yes">
				<segment state="initial">
					<source>Alternatively, "Would you like to play a game or listen to music?"</source>
				</segment>
			</unit>
			<unit id="154" translate="yes">
				<segment state="initial">
					<source>constrains the response to one of two valid answers with a correspondingly small grammar definition.</source>
				</segment>
			</unit>
			<unit id="155" translate="yes">
				<segment state="initial">
					<source>A small grammar is much easier to author and results in much more accurate recognition results.</source>
				</segment>
			</unit>
			<unit id="156" translate="yes">
				<segment state="initial">
					<source>Request confirmation from the user when speech recognition confidence is low.</source>
				</segment>
			</unit>
			<unit id="157" translate="yes">
				<segment state="initial">
					<source>If the user's intent is unclear, it's better to get clarification than to initiate an unintended action.</source>
				</segment>
			</unit>
			<unit id="158" translate="yes">
				<segment state="initial">
					<source>You should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on.</source>
				</segment>
			</unit>
			<unit id="159" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](../controls-and-patterns/app-bars.md)</data>
				</originalData>
				<segment state="initial">
					<source>For example, a command bar button with a microphone glyph (see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Guidelines for command bars</pc>) can be used to show both availability and state.</source>
				</segment>
			</unit>
			<unit id="160" translate="yes">
				<segment state="initial">
					<source>If the speech recognition switch is typically out of view, consider displaying a state indicator in the content area of the app.</source>
				</segment>
			</unit>
			<unit id="161" translate="yes">
				<segment state="initial">
					<source>If recognition is initiated by the user, consider using the built-in recognition experience for consistency.</source>
				</segment>
			</unit>
			<unit id="162" translate="yes">
				<segment state="initial">
					<source>The built-in experience includes customizable screens with prompts, examples, disambiguations, confirmations, and errors.</source>
				</segment>
			</unit>
			<unit id="163" translate="yes">
				<segment state="initial">
					<source>The screens vary depending on the specified constraints:</source>
				</segment>
			</unit>
			<unit id="164" translate="yes">
				<segment state="initial">
					<source>Pre-defined grammar (dictation or web search)</source>
				</segment>
			</unit>
			<unit id="165" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Listening</pc> screen.</source>
				</segment>
			</unit>
			<unit id="166" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Thinking</pc> screen.</source>
				</segment>
			</unit>
			<unit id="167" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Heard you say</pc> screen or the error screen.</source>
				</segment>
			</unit>
			<unit id="168" translate="yes">
				<segment state="initial">
					<source>List of words or phrases, or a SRGS grammar file</source>
				</segment>
			</unit>
			<unit id="169" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Listening</pc> screen.</source>
				</segment>
			</unit>
			<unit id="170" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Did you say</pc> screen, if what the user said could be interpreted as more than one potential result.</source>
				</segment>
			</unit>
			<unit id="171" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>The <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Heard you say</pc> screen or the error screen.</source>
				</segment>
			</unit>
			<unit id="172" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>On the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Listening</pc> screen you can:</source>
				</segment>
			</unit>
			<unit id="173" translate="yes">
				<segment state="initial">
					<source>Customize the heading text.</source>
				</segment>
			</unit>
			<unit id="174" translate="yes">
				<segment state="initial">
					<source>Provide example text of what the user can say.</source>
				</segment>
			</unit>
			<unit id="175" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>Specify whether the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Heard you say</pc> screen is shown.</source>
				</segment>
			</unit>
			<unit id="176" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>Read the recognized string back to the user on the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Heard you say</pc> screen.</source>
				</segment>
			</unit>
			<unit id="177" translate="yes">
				<segment state="initial">
					<source>Here is an example of the built-in recognition flow for a speech recognizer that uses a SRGS-defined constraint.</source>
				</segment>
			</unit>
			<unit id="178" translate="yes">
				<segment state="initial">
					<source>In this example, speech recognition is successful.</source>
				</segment>
			</unit>
			<unit id="179" translate="yes">
				<segment state="initial">
					<source>initial recognition screen for a constraint based on a sgrs grammar file</source>
				</segment>
			</unit>
			<unit id="180" translate="yes">
				<segment state="initial">
					<source>intermediate recognition screen for a constraint based on a sgrs grammar file</source>
				</segment>
			</unit>
			<unit id="181" translate="yes">
				<segment state="initial">
					<source>final recognition screen for a constraint based on a sgrs grammar file</source>
				</segment>
			</unit>
			<unit id="182" translate="yes">
				<segment state="initial">
					<source>Always listening</source>
				</segment>
			</unit>
			<unit id="183" translate="yes">
				<segment state="initial">
					<source>Your app can listen for and recognize speech input as soon as the app is launched, without user intervention.</source>
				</segment>
			</unit>
			<unit id="184" translate="yes">
				<segment state="initial">
					<source>You should customize the grammar constraints based on the app context.</source>
				</segment>
			</unit>
			<unit id="185" translate="yes">
				<segment state="initial">
					<source>This keeps the speech recognition experience very targeted and relevant to the current task, and minimizes errors.</source>
				</segment>
			</unit>
			<unit id="186" translate="yes">
				<segment state="initial">
					<source>"What can I say?"</source>
				</segment>
			</unit>
			<unit id="187" translate="yes">
				<segment state="initial">
					<source>When speech input is enabled, it's important to help users discover what exactly can be understood and what actions can be performed.</source>
				</segment>
			</unit>
			<unit id="188" translate="yes">
				<segment state="initial">
					<source>If speech recognition is user enabled, consider using the command bar or a menu command to show all words and phrases supported in the current context.</source>
				</segment>
			</unit>
			<unit id="189" translate="yes">
				<segment state="initial">
					<source>If speech recognition is always on, consider adding the phrase "What can I say?"</source>
				</segment>
			</unit>
			<unit id="190" translate="yes">
				<segment state="initial">
					<source>to every page.</source>
				</segment>
			</unit>
			<unit id="191" translate="yes">
				<segment state="initial">
					<source>When the user says this phrase, display all words and phrases supported in the current context.</source>
				</segment>
			</unit>
			<unit id="192" translate="yes">
				<segment state="initial">
					<source>Using this phrase provides a consistent way for users to discover speech capabilities across the system.</source>
				</segment>
			</unit>
			<unit id="193" translate="yes">
				<segment state="initial">
					<source>Recognition failures</source>
				</segment>
			</unit>
			<unit id="194" translate="yes">
				<segment state="initial">
					<source>Speech recognition will fail.</source>
				</segment>
			</unit>
			<unit id="195" translate="yes">
				<segment state="initial">
					<source>Failures happen when audio quality is poor, when only part of a phrase is recognized, or when no input is detected at all.</source>
				</segment>
			</unit>
			<unit id="196" translate="yes">
				<segment state="initial">
					<source>Handle failure gracefully, help a user understand why recognition failed, and recover.</source>
				</segment>
			</unit>
			<unit id="197" translate="yes">
				<segment state="initial">
					<source>Your app should inform the user that they weren't understood and that they need to try again.</source>
				</segment>
			</unit>
			<unit id="198" translate="yes">
				<segment state="initial">
					<source>Consider providing examples of one or more supported phrases.</source>
				</segment>
			</unit>
			<unit id="199" translate="yes">
				<segment state="initial">
					<source>The user is likely to repeat a suggested phrase, which increases recognition success.</source>
				</segment>
			</unit>
			<unit id="200" translate="yes">
				<segment state="initial">
					<source>You should display a list of potential matches for a user to select from.</source>
				</segment>
			</unit>
			<unit id="201" translate="yes">
				<segment state="initial">
					<source>This can be far more efficient than going through the recognition process again.</source>
				</segment>
			</unit>
			<unit id="202" translate="yes">
				<segment state="initial">
					<source>You should always support alternative input types, which is especially helpful for handling repeated recognition failures.</source>
				</segment>
			</unit>
			<unit id="203" translate="yes">
				<segment state="initial">
					<source>For example, you could suggest that the user try to use a keyboard, or use touch or a mouse to select from a list of potential matches.</source>
				</segment>
			</unit>
			<unit id="204" translate="yes">
				<segment state="initial">
					<source>Use the built-in speech recognition experience as it includes screens that inform the user that recognition was not successful and lets the user make another recognition attempt.</source>
				</segment>
			</unit>
			<unit id="205" translate="yes">
				<segment state="initial">
					<source>Listen for and try to correct issues in the audio input.</source>
				</segment>
			</unit>
			<unit id="206" translate="yes">
				<segment state="initial">
					<source>The speech recognizer can detect issues with the audio quality that might adversely affect speech recognition accuracy.</source>
				</segment>
			</unit>
			<unit id="207" translate="yes">
				<segment state="initial">
					<source>You can use the information provided by the speech recognizer to inform the user of the issue and let them take corrective action, if possible.</source>
				</segment>
			</unit>
			<unit id="208" translate="yes">
				<segment state="initial">
					<source>For example, if the volume setting on the microphone is too low, you can prompt the user to speak louder or turn the volume up.</source>
				</segment>
			</unit>
			<unit id="209" translate="yes">
				<segment state="initial">
					<source>Constraints</source>
				</segment>
			</unit>
			<unit id="210" translate="yes">
				<segment state="initial">
					<source>Constraints, or grammars, define the spoken words and phrases that can be matched by the speech recognizer.</source>
				</segment>
			</unit>
			<unit id="211" translate="yes">
				<segment state="initial">
					<source>You can specify one of the pre-defined web service grammars or you can create a custom grammar that is installed with your app.</source>
				</segment>
			</unit>
			<unit id="212" translate="yes">
				<segment state="initial">
					<source>Predefined grammars</source>
				</segment>
			</unit>
			<unit id="213" translate="yes">
				<segment state="initial">
					<source>Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar.</source>
				</segment>
			</unit>
			<unit id="214" translate="yes">
				<segment state="initial">
					<source>When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device</source>
				</segment>
			</unit>
			<unit id="215" translate="yes">
				<segment state="initial">
					<source>The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases.</source>
				</segment>
			</unit>
			<unit id="216" translate="yes">
				<segment state="initial">
					<source>Free-text dictation is useful when you don't want to limit the kinds of things a user can say.</source>
				</segment>
			</unit>
			<unit id="217" translate="yes">
				<segment state="initial">
					<source>Typical uses include creating notes or dictating the content for a message.</source>
				</segment>
			</unit>
			<unit id="218" translate="yes">
				<segment state="initial">
					<source>The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say.</source>
				</segment>
			</unit>
			<unit id="219" translate="yes">
				<segment state="initial">
					<source>However, it is optimized to recognize terms that people typically use when searching the web.</source>
				</segment>
			</unit>
			<unit id="220" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Note</pc>  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.</source>
				</segment>
			</unit>
			<unit id="221" translate="yes">
				<segment state="initial">
					<source>These predefined grammars can be used to recognize up to 10 seconds of speech input and require no authoring effort on your part.</source>
				</segment>
			</unit>
			<unit id="222" translate="yes">
				<segment state="initial">
					<source>However, they do require connection to a network.</source>
				</segment>
			</unit>
			<unit id="223" translate="yes">
				<segment state="initial">
					<source>Custom grammars</source>
				</segment>
			</unit>
			<unit id="224" translate="yes">
				<segment state="initial">
					<source>A custom grammar is designed and authored by you and is installed with your app.</source>
				</segment>
			</unit>
			<unit id="225" translate="yes">
				<segment state="initial">
					<source>Speech recognition using a custom constraint is performed on the device.</source>
				</segment>
			</unit>
			<unit id="226" translate="yes">
				<segment state="initial">
					<source>Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases.</source>
				</segment>
			</unit>
			<unit id="227" translate="yes">
				<segment state="initial">
					<source>A list constraint works well for recognizing short, distinct phrases.</source>
				</segment>
			</unit>
			<unit id="228" translate="yes">
				<segment state="initial">
					<source>Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match.</source>
				</segment>
			</unit>
			<unit id="229" translate="yes">
				<segment state="initial">
					<source>The list can also be programmatically updated.</source>
				</segment>
			</unit>
			<unit id="230" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://go.microsoft.com/fwlink/p/?LinkID=262302)</data>
				</originalData>
				<segment state="initial">
					<source>An SRGS grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">SRGS Version 1.0</pc>.</source>
				</segment>
			</unit>
			<unit id="231" translate="yes">
				<segment state="initial">
					<source>An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.</source>
				</segment>
			</unit>
			<unit id="232" translate="yes">
				<segment state="initial">
					<source>Here are some tips for authoring SRGS grammars:</source>
				</segment>
			</unit>
			<unit id="233" translate="yes">
				<segment state="initial">
					<source>Keep each grammar small.</source>
				</segment>
			</unit>
			<unit id="234" translate="yes">
				<segment state="initial">
					<source>Grammars that contain fewer phrases tend to provide more accurate recognition than larger grammars that contain many phrases.</source>
				</segment>
			</unit>
			<unit id="235" translate="yes">
				<segment state="initial">
					<source>It's better to have several smaller grammars for specific scenarios than to have a single grammar for your entire app.</source>
				</segment>
			</unit>
			<unit id="236" translate="yes">
				<segment state="initial">
					<source>Let users know what to say for each app context and enable and disable grammars as needed.</source>
				</segment>
			</unit>
			<unit id="237" translate="yes">
				<segment state="initial">
					<source>Design each grammar so users can speak a command in a variety of ways.</source>
				</segment>
			</unit>
			<unit id="238" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source>For example, you can use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">GARBAGE</pc> rule to match speech input that your grammar does not define.</source>
				</segment>
			</unit>
			<unit id="239" translate="yes">
				<segment state="initial">
					<source>This lets users speak additional words that have no meaning to your app.</source>
				</segment>
			</unit>
			<unit id="240" translate="yes">
				<segment state="initial">
					<source>For example, "give me", "and", "uh", "maybe", and so on.</source>
				</segment>
			</unit>
			<unit id="241" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://msdn.microsoft.com/library/windowsphone/design/jj572474.aspx)</data>
				</originalData>
				<segment state="initial">
					<source>Use the <pc dataRefEnd="id2" dataRefStart="id1" id="p1">sapi:subset</pc> element to help match speech input.</source>
				</segment>
			</unit>
			<unit id="242" translate="yes">
				<segment state="initial">
					<source>This is a Microsoft extension to the SRGS specification to help match partial phrases.</source>
				</segment>
			</unit>
			<unit id="243" translate="yes">
				<segment state="initial">
					<source>Try to avoid defining phrases in your grammar that contain only one syllable.</source>
				</segment>
			</unit>
			<unit id="244" translate="yes">
				<segment state="initial">
					<source>Recognition tends to be more accurate for phrases containing two or more syllables.</source>
				</segment>
			</unit>
			<unit id="245" translate="yes">
				<segment state="initial">
					<source>Avoid using phrases that sound similar.</source>
				</segment>
			</unit>
			<unit id="246" translate="yes">
				<segment state="initial">
					<source>For example, phrases such as "hello", "bellow", and "fellow" can confuse the recognition engine and result in poor recognition accuracy.</source>
				</segment>
			</unit>
			<unit id="247" translate="yes">
				<originalData>
					<data id="id1">**</data>
					<data id="id2">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Note</pc>  Which type of constraint type you use depends on the complexity of the recognition experience you want to create.</source>
				</segment>
			</unit>
			<unit id="248" translate="yes">
				<segment state="initial">
					<source>Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.</source>
				</segment>
			</unit>
			<unit id="249" translate="yes">
				<segment state="initial">
					<source>Custom pronunciations</source>
				</segment>
			</unit>
			<unit id="250" translate="yes">
				<segment state="initial">
					<source>If your app contains specialized vocabulary with unusual or fictional words, or words with uncommon pronunciations, you might be able to improve recognition performance for those words by defining custom pronunciations.</source>
				</segment>
			</unit>
			<unit id="251" translate="yes">
				<segment state="initial">
					<source>For a small list of words and phrases, or a list of infrequently used words and phrases, you can create custom pronunciations in a SRGS grammar.</source>
				</segment>
			</unit>
			<unit id="252" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://msdn.microsoft.com/library/windowsphone/design/hh361600.aspx)</data>
				</originalData>
				<segment state="initial">
					<source>See <pc dataRefEnd="id2" dataRefStart="id1" id="p1">token Element</pc> for more info.</source>
				</segment>
			</unit>
			<unit id="253" translate="yes">
				<segment state="initial">
					<source>For larger lists of words and phrases, or frequently used words and phrases, you can create separate pronunciation lexicon documents.</source>
				</segment>
			</unit>
			<unit id="254" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://msdn.microsoft.com/library/windowsphone/design/hh361646.aspx)</data>
				</originalData>
				<segment state="initial">
					<source>See <pc dataRefEnd="id2" dataRefStart="id1" id="p1">About Lexicons and Phonetic Alphabets</pc> for more info.</source>
				</segment>
			</unit>
			<unit id="255" translate="yes">
				<segment state="initial">
					<source>Testing</source>
				</segment>
			</unit>
			<unit id="256" translate="yes">
				<segment state="initial">
					<source>Test speech recognition accuracy and any supporting UI with your app's target audience.</source>
				</segment>
			</unit>
			<unit id="257" translate="yes">
				<segment state="initial">
					<source>This is the best way to determine the effectiveness of the speech interaction experience in your app.</source>
				</segment>
			</unit>
			<unit id="258" translate="yes">
				<segment state="initial">
					<source>For example, are users getting poor recognition results because your app isn't listening for a common phrase?</source>
				</segment>
			</unit>
			<unit id="259" translate="yes">
				<segment state="initial">
					<source>Either modify the grammar to support this phrase or provide users with a list of supported phrases.</source>
				</segment>
			</unit>
			<unit id="260" translate="yes">
				<segment state="initial">
					<source>If you already provide the list of supported phrases, ensure it is easily discoverable.</source>
				</segment>
			</unit>
			<unit id="261" translate="yes">
				<segment state="initial">
					<source>Text-to-speech (TTS)</source>
				</segment>
			</unit>
			<unit id="262" translate="yes">
				<segment state="initial">
					<source>TTS generates speech output from plain text or SSML.</source>
				</segment>
			</unit>
			<unit id="263" translate="yes">
				<segment state="initial">
					<source>Try to design prompts that are polite and encouraging.</source>
				</segment>
			</unit>
			<unit id="264" translate="yes">
				<segment state="initial">
					<source>Consider whether you should read long strings of text.</source>
				</segment>
			</unit>
			<unit id="265" translate="yes">
				<segment state="initial">
					<source>It's one thing to listen to a text message, but quite another to listen to a long list of search results that are difficult to remember.</source>
				</segment>
			</unit>
			<unit id="266" translate="yes">
				<segment state="initial">
					<source>You should provide media controls to let users pause, or stop, TTS.</source>
				</segment>
			</unit>
			<unit id="267" translate="yes">
				<segment state="initial">
					<source>You should listen to all TTS strings to ensure they are intelligible and sound natural.</source>
				</segment>
			</unit>
			<unit id="268" translate="yes">
				<segment state="initial">
					<source>Stringing together an unusual sequence of words or speaking part numbers or punctuation might cause a phrase to become unintelligible.</source>
				</segment>
			</unit>
			<unit id="269" translate="yes">
				<segment state="initial">
					<source>Speech can sound unnatural when the prosody or cadence is different from how a native speaker would say a phrase.</source>
				</segment>
			</unit>
			<unit id="270" translate="yes">
				<segment state="initial">
					<source>Both issues can be addressed bu using SSML instead of plain text as input to the speech synthesizer.</source>
				</segment>
			</unit>
			<unit id="271" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](http://msdn.microsoft.com/library/windowsphone/design/hh378454.aspx)</data>
					<data id="id3">[</data>
					<data id="id4">](http://msdn.microsoft.com/library/windowsphone/design/hh378377.aspx)</data>
				</originalData>
				<segment state="initial">
					<source>For more info about SSML, see <pc dataRefEnd="id2" dataRefStart="id1" id="p1">Use SSML to Control Synthesized Speech</pc> and <pc dataRefEnd="id4" dataRefStart="id3" id="p2">Speech Synthesis Markup Language Reference</pc>.</source>
				</segment>
			</unit>
			<unit id="272" translate="yes">
				<segment state="initial">
					<source>Other articles in this section</source>
				</segment>
			</unit>
			<unit id="273" translate="yes">
				<segment state="initial">
					<source>Topic</source>
				</segment>
			</unit>
			<unit id="274" translate="yes">
				<segment state="initial">
					<source>Description</source>
				</segment>
			</unit>
			<unit id="275" translate="yes">
				<segment state="initial">
					<source>Speech recognition</source>
				</segment>
			</unit>
			<unit id="276" translate="yes">
				<segment state="initial">
					<source>Use speech recognition to provide input, specify an action or command, and accomplish tasks.</source>
				</segment>
			</unit>
			<unit id="277" translate="yes">
				<segment state="initial">
					<source>Specify the speech recognizer language</source>
				</segment>
			</unit>
			<unit id="278" translate="yes">
				<segment state="initial">
					<source>Learn how to select an installed language to use for speech recognition.</source>
				</segment>
			</unit>
			<unit id="279" translate="yes">
				<segment state="initial">
					<source>Define custom recognition constraints</source>
				</segment>
			</unit>
			<unit id="280" translate="yes">
				<segment state="initial">
					<source>Learn how to define and use custom constraints for speech recognition.</source>
				</segment>
			</unit>
			<unit id="281" translate="yes">
				<segment state="initial">
					<source>Enable continuous dictation</source>
				</segment>
			</unit>
			<unit id="282" translate="yes">
				<segment state="initial">
					<source>Learn how to capture and recognize long-form, continuous dictation speech input.</source>
				</segment>
			</unit>
			<unit id="283" translate="yes">
				<segment state="initial">
					<source>Manage issues with audio input</source>
				</segment>
			</unit>
			<unit id="284" translate="yes">
				<segment state="initial">
					<source>Learn how to manage issues with speech-recognition accuracy caused by audio-input quality.</source>
				</segment>
			</unit>
			<unit id="285" translate="yes">
				<segment state="initial">
					<source>Set speech recognition timeouts</source>
				</segment>
			</unit>
			<unit id="286" translate="yes">
				<segment state="initial">
					<source>Set how long a speech recognizer ignores silence or unrecognizable sounds (babble) and continues listening for speech input.</source>
				</segment>
			</unit>
			<unit id="287" translate="yes">
				<segment state="initial">
					<source>Related articles</source>
				</segment>
			</unit>
			<unit id="288" translate="yes">
				<segment state="initial">
					<source>Speech interactions</source>
				</segment>
			</unit>
			<unit id="289" translate="yes">
				<originalData>
					<data id="id1">[</data>
					<data id="id2">](https://msdn.microsoft.com/library/windows/apps/mt185598)</data>
					<data id="id3">
</data>
					<data id="id4">**</data>
					<data id="id5">**</data>
				</originalData>
				<segment state="initial">
					<source><pc dataRefEnd="id2" dataRefStart="id1" id="p1">Cortana interactions</pc><ph dataRef="id3" id="ph1" /><pc dataRefEnd="id5" dataRefStart="id4" id="p2">Samples</pc></source>
				</segment>
			</unit>
			<unit id="290" translate="yes">
				<segment state="initial">
					<source>Speech recognition and speech synthesis sample</source>
				</segment>
			</unit>
		</group>
	</file>
</xliff>